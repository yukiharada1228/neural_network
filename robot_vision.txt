みなさんこんにちは 今日からロボットビジョンを始めます
さあロボットビジョンというものは まず何かという話なんですけども
ロボットが目の情報、視覚情報をもとに 何かこう動作をできるようにするために
必要となる技術となります
まずロボットビジョン自体を実現するためには
まず我々のこの世界であるシーンの画像を カメラを通して取り込みます
そしてその画像の中をですね 認識しやすいようにまず処理をします
これが画像処理といいます
そしてその画像を処理した後に 画像からものを認識したりします
例えばロボットがぶつからないように動くためには 障害物を認識しないといけます
そしてさらにロボットが動くためには 2次元的なこの認識結果だけじゃなくて
3次元のシーンがどのようになっているのか ということを理解する必要があります
そしてようやくロボットは目の情報、視覚情報から 自立して動くことができるようになるというわけです
この講義ではロボットビジョンという タイトルなんですけども
まずその最初の画像処理について 講義を始めていきます
まずこの講義ロボットビジョン 画像処理のテキストがこちらの本になります
皆さん手元にありますか まだもしかしたらない人もいるかもしれません
ちなみにこの本非常に分厚い本です
しかもなんとこの本いいところがありまして 全ページフルカラーになっています
なので非常に見やすい 実際の例が豊富に入っています
画像処理もかなりいろんな技術がたくさんありますので
そのすべての技術を非常に網羅した良い本になります
もう一つこの本のいい点がありまして ここにありますように
画像処理エンジニア検定エキスパート対応書籍 というふうに書いてあるのがわかると思います
これ何かと言いますと CGアーツ協会という財団がありまして
そこの財団がやっている検定試験に沿った対応の書籍となっています
ちなみにこの画像処理エンジニア検定には ベーシックとエキスパートという2種類あります
このエキスパートは大学院相当になります
この本全体ですから 残念ながらこの授業では全部はできません
あともう一つベーシックってなります
ベーシックは3年生の皆さんでも この画像処理のロボットビジョンの授業を受けた後に
ぜひ興味のあった人は受けてもらうと受かるぐらいだと思います
ぜひ興味のあったらこういう検定も受けるといいかなと思います
さあこの本のもう一ついい点は 実は後ろの著者のところを見ていただくとわかるように
日本の中の画像認識処理に非常に有名な著名な先生が執筆をしています
その中に一応僕も入っていまして
11章、12章、13章というのを担当して書いています
自分が書いたからこれを勧めているわけだけではなくて
この本は社会人の方に非常に人気があります
社会に出てこういった画像処理に携わっている業務の人は
必ずオフィスの本棚にこれを置いておいて
なんかちょっとあれどうだったかなという時に
この本を見て思い出したりするのにも使えたりします
非常に実用的でいい本ですのでぜひ購入をしてください
実際に本、物理的な本だけでなくて
キンドル版も出ています
キンドル版の方がちょっと安いです
キンドル版でも問題ありませんので
興味が出たら購入をしてみてください
このロボットビジョンという講義の中では
ここにあるような1から9の内容をやっていきます
今日はまず第1回目ということですから
この1にあたる画像の標本化、量子化
そして濃淡画像の処理について説明をしていきます
次回以降は2から順番にやっていきます
この講義の目的は
いろんな国画像集のアルゴリズムというもの
どういうふうにできているかという概念をちゃんと理解した上で
そして最後にここの7番にあります
シフトと呼ばれる特徴点検出記述をするためのアルゴリズムがあります
これはちょっと難しいアルゴリズムなんですけれども
このアルゴリズムが最終的に理解できれば
この画像処理全般をちゃんと理解できたというふうになると思います
そこまでにいろんな国画像処理のアルゴリズムの説明をします
そして8ではコンピュータービジョン
画像処理の次に認識をするための技術を
そうしてコンピュータービジョンもしくはロボットビジョンといいます
コンピュータービジョンとロボットビジョンは基本的には同じものなんですけれども
このコンピュータービジョンという分野でどういう研究が
どういったことがなされているかということを紹介します
ちなみに僕はこのコンピュータービジョンの研究者です
我々の研究室はここにあるように
マシンパーセプション機械知覚ですね
andロボティクスグループということでMPRGというふうに呼んでいます
我々の研究グループではこういったコンピュータービジョンだとか
AIを使った画像認識の研究に取り組んでいます
最後にこの研究紹介として
アフィン不変なキーポイントマッチングという方法について
我々の研究グループの研究についても紹介したいと思います
ではまずアナログ画像をデジタル画像にします
アナログ画像をデジタル画像にするためには標本化と量子化を行います
これがレンズです
ここにこういうあるシーンがあるわけですね
このシーンをレンズを介して
まず最初にカメラのCCDというのに取り組みます
CCDでは何をしているかというと
ここにあるように標本化を行います
まず最初にやることが標本化というわけです
標本化は空間的配置を理算的な座標に射増します
連続的な空間を細かく理算的な座標に射増するというわけです
そして標本化した後にAD変換器によって量子化を行います
量子化、クォンタイゼーションとも呼ばれます
これは何をすることかというと
明るさ、各画素と言いますけど
標本化した各画素の明るさの濃淡的を理算的な値
いわゆる整数値に変換をします
これが量子化をします
その結果画像、デジタル画像になるわけです
おさらいです
アナログ画像が入力されます
アナログ画像をまず空間的配置を標本化して
そして色明るさを量子化することによってデジタル画像になるわけです
デジタル画像になることによって
いろんな画像処理のアルゴリズムを
コンピューター上で実現することができるようになるわけです
ではまず標本化です
標本化、これはアナログ信号から
理算的な位置におけるアナログ値を取り出す処理になります
まず1次元の信号波形で確認しましょう
デジタル信号処理でもやったと思いますけども
こういった1次元のアナログ連続信号が入力され
これに対してx、これ時間だと思ってください
時間方向に理算的に区切ります
こんな感じですね
これが標本化です
その標本化点に対する値をアナログ値ですから
こちらの同じ値のアナログ値を取り出してきたものが
取り出すことを標本化という
では2次元の画像ではどうなるか見てみましょう
x方向とy方向の2次元です
上から見ると普通のこういった画像ですね
これどういう画像か
これを縦軸に画素値、明るさを表すと
xとyと明るさ画素値になります
そうするとこのような形状になって
これをまずどうするかというと
x方向とy方向に対しても標本化を行い
その各標本点に対するアナログ値を取り出してきます
これが標本化のやることです
ちなみに標本化の後に量子化を行います
量子化は何をしているのかというと
今度はこの縦方向を整数にするという感じですね
そうすると縦方向にガタガタという形になるわけですね
これが2次元で表すとこのようなパターンです
これによってアナログ画像から標本化して
標本化した後に量子化することで
デジタル画像になるというわけです
実際の例を見てみましょう
この画像はコンピューター上で表示していますので
本来であればデジタル画像です
ですがここではアナログ画像だと思ってください
下にあるのが下のバーが
明るさが連続的に変わってますよということを示して
この画像をまず標本化します
標本化感覚をこれぐらいのこの1個のですね
この1個の四角形を標本点として標本化すると
こんな画像になります
その標本点におけるアナログ値を取り出してきた画像が
こんな形です
当然標本化感覚が大きいと
元の画像の情報がかなり失われているということが分かると
ではこの標本化感覚を少し小さくしていきましょう
少し小さくしましたというぐらいですね
ちょっと元の画像の何が映っているのか
少し見えてきたかもしれませんが
でもまだまだよく分かりません
じゃあもっと細かくしますかなり小さくします
さらにもっと小さくしましょう
そうすると元のアナログ画像に含まれていた情報が
ちゃんと残っているような形で
画像が標本化できるようになっていくということは分かると思います
これをですね標本化定理と言います
この標本化定理はデジタル信号処理でも出てきたかと思います
どういうことかというと
元々の標本化するアナログ信号が
そもそも周波数Fmaxを持つときに
その2倍以上である2Fmax以上で
標本化周波数で標本化すれば
アナログ信号に含まれる情報を失わないということになります
では今から3つの例をちょっと見てみましょう
まず一番最初の例です
これは元の信号アナログ画像が
このような信号だと思っています
このような信号が入力されたときに
7.4倍の周期で標本化したときどうなるかという場合
これをですねちょっとつないでいきます
この標本点が0,1,2,3というところですね
そこの交差するところ
この黒い点が標本化した後のアナログ値になります
それと選ばれたところをですね
結んでいくとこんな波形になるわけです
どうでしょう?元の黒いアナログ波形と
標本化した後の赤色の線というものが
ほぼ一致しているのがわかります
すなわち元のアナログ信号の周波数よりも
かなり大きなこの場合7.4倍ですね
の標本化周波数で標本化しているので
元のアナログ信号の情報がほとんど失われていない
劣化していないということがわかると思う
続いてこの2つ目です
2つ目の入力アナログ信号がこんな信号です
結構先ほどに比べると高周波な信号になっているのがわかる
これを2.3倍の標本化間隔
周波数でサンプリングをします
そうしたのがこの黒の点になります
黒の点を結んでいきます
という赤色のような波形になったわけです
黒色の元のアナログ波形と
赤色の標本化した後の波形を比べてみると
高いところ低いところの位置は
ある程度表現はできているけれども
元々の正弦波の滑らかな変化が
表現できていないというのがわかります
もちろん元々の持つ周波数の2倍以上で
標本化しているのですが
十分に元の波形を表現できているとは言えない
というのがわかると思う
なので2倍以上だからといって
2倍でサンプリングするのではなくて
標本化するのではなくて
大きな2倍以上の標本化周波数で
標本化しないといけないということがわかると思います
続いて最後の例は
こんな非常に高周波な信号ですね
この信号に対して1.1倍です
そもそも一番最初の標本化定理を
満足していない状態で
標本化するとどうなるかというのが黒い点です
これも同じように結んでいきます
そうするとこのような赤色の波形が
標本化した後の波形となります
元々の黒色のアナログ信号を
全く表現できていないのがわかります
むしろ違う信号となって
見えているというのが取られているというのが
わかると思います
これが標本化定理です
2倍以上の標本化周波数で
標本化することによって
元のアナログ信号に含まれる情報が
失われないということです
続いて量子化です
量子化はそもそも標本したところの値を
有限分解量の数値に変換する処理です
ちなみに画像の素子
CCDの出力というものが
一つ一つが電圧信号に変わります
これをAD変換器によって
アナログ信号からデジタル信号に変換します
その時に電圧の信号のレベルというものを
0から255の256段階にしたとします
これを8ビット量子化
2の8乗は256ですから
8ビット量子化というふうに呼ばれます
この量子化レベルというものを
かなり増やせば増やすほど
例えば256段階から512とかですね
どんどん増やしていくことによって
量子化レベルが増えれば増えるほど
当然ながら量子化誤差も小さくなるというわけです
ではこれも同じように実際に例を見ていきましょう
まず今この画像が
元のアナログ信号の状態だと思ってください
では量子化数を32にしてみました
違い分かりますか
この動画の中ではちょっと見づらいと思いますから
PDFの方で確認しておいてください
元のアナログ信号に対して量子化数を32にすると
あんまり変化が分からないかもしれませんが
よく見るとこういうですね
グラデーションのあるところが
段階的に変わっているというのは分かると思う
続いて16です
量子化数が16になるとほとんど
こういった非常に差が分かると思います
さらに8です
8にすると顔のところにもかなり違和感がありますね
続けて4そして2となるとこんな感じになっていきます
こうやってデジタル画像を標本化と量子化によって
アナログ画像からデジタル画像に変換しました
続いてデジタル画像をどうやって表現するかということなんですが
このような画像が入力されたところ
ここの領域を見ると
こういった画素値というもので構成されているのが分かると思います
この1個1個のことをですね
この1つ1つのこと
これ1個1個のことを画素と呼びます
もしくはピクセルとも呼ばれます
この1個1個の画素の値を画素値と言います
画素値というのは明るさを表していまして
一般的に範囲は256段階
すなわち0から255の値を持つということになります
したがってこの1枚の画像のサイズというのは
横幅×高さ×1画素を8ビットとすると
それがそのままバイトという単位のファイルサイズになります
ちなみに画素値についてはですね
濃淡、濃度もしくは輝度とも呼ばれたりします
ではデジタル画像の表現について見ていきましょう
デジタル画像を表す表現する1つの例に
解像度というものがあります
解像度というものは画素の密度を示す数値です
いろんな表現があるんですけども
例えば皆さん聞いたことあるようなものに
DPIというのを聞いたことあるんじゃないかなと思います
DPIというのはドットパーインチ
1インチの長さに何画素、何ドットを存在しているかということを表します
ちなみに1インチというのは2.5センチです
皆さんの自宅にスキャナー付きのプリンターがありませんか?
そのスキャナーが何DPIだったかわかる人いますか?
ぜひこの動画を見た後調べてみてください
ちなみにここでは5DPIの画像を作ってみたいと思います
5DPIの画像を作るため、量子架数は4とします
そのためにどうすればいいかというと
まず元の画像の1インチ、すなわち2.5センチを
5DPIですから、1、2、3、4、5という形で5つに区切ります
そしてその1つ1つに対して量子化した結果がこのような形になります
今度はその1つ1つの値に対して量子架数4ですから
黒、濃いグレー、明るいグレー、白という4つの値に変換した結果
この画像が最終的に解像度5DPI、そして量子架数4のデジタル画像になるわけです
ちなみにスキャナーはいろいろあるんですよ
600DPIとかそれぐらいのものがあったりします
600DPIということはたった2.5センチの長さを600に分割して
その1個1個の値を取り出してきてくれているわけです
スキャナーは非常に我々人間にとって気の遠くなるような作業ですね
皆さんの代わりにしてくれているというのがわかると思います
ちなみにカラー画像はどうやって表現されるかということなんですが
カラー画像は可放混色という方法によって色が表現されます
R、赤の成分、G、緑の成分、B、ブルー、青の成分
光の3原色を合成することによってこのカラー画像が表現できる
ちなみに実際にファイルを取り込んだアナログ画像から
スキャナーによって取り込んだ画像をデジタル画像を
ファイルとしてセーブしておく必要があります
その時にその画像自体をどのような形式で表現するかということが重要となります
実はOSだとかコンピューター環境、アプリケーション、使用目的に応じて
いろんなフォーマットが提案されています
JPEGだとか聞いたことあるかもしれませんが
一番単純な画像の形式を今日は紹介します
それがPGMというフォーマットとPPMというフォーマットです
PGMというフォーマットはポータブルグレーマップファイルフォーマットといいまして
グレースケールの画像を扱います
PPMはポータブルピックスマップファイルフォーマットといいまして
RGBのカラー画像を扱います
じゃあまずPGMの方を見てみましょう
PGMは基本的にここのヘッダーと呼ばれるところと
画像データの羅列の2種類で表現されます
ヘッダー部分にはまずP2というマジックナンバーがあります
これは何かというとこの後ろで表現されている画像データが
ASCIIテキスト形式なのかバイナリー形式なのかということを指します
その後に4.4とあります
これは画像の横幅と縦幅を表します
今4×4の画素ですよということ
そして次に255
これは量子化した後の画素値明るさの最大値を記入しておいて
その後に4×4ですから16個の値がこうやって並んでいるわけです
このそれぞれ0、255、255、0というものが
0が黒、255が白というふうに
こういう対応した形で画像が表現できるという
ではカラー画像を見てみましょう
こちらはPGMフォーマットです
こちらも同じようにヘッダー部分と画像データの羅列部分があります
最初にマジックナンバーでp3だとテキストであるASCIIデータ
p6だとバイナリーを表します
ここでも4×4ですので4.4と書いておきます
そして最大値255を書きます
この後画像データが羅列します
まず最初のところはこの3つですね
この3つでここの最初の値を表現します
順番にRGBという値が並んでいます
RGBすべてが0ですからそこの画素は黒色になる
なのでこの3つずつで値が入ってきます
ここはすべて255ですから
ここが見てもらうとわかるように白色を表します
ここは255、0、0ですから赤色
0、255、0ですから緑色を表します
このような形でデジタル画像の形式が決まっているわけです
これテキストデータで自分で入力して
それをファイル拡張子をPPMだとかPGMという風にしてセーブすれば
そのファイルをですね
画像を表示するビューワーで見ることができるようになります
では画像を今度はコンピューターで表現するときに
プログラムをするときにどういう風に考えるかという話です
用途の方法によってデジタル画像が得られました

このデジタル画像はコンピュータ上では2次元配列として扱います。
例えば、iという配列で2次元配列にします。
2と3という要素を指定すれば、ここの画素値を参照することができます。
もちろん皆さんご存知のように、コンピュータプログラム上ではこういう2次元の配列として表現しますが、
実際のコンピュータ上は、こういったメモリ空間、1次元のメモリ空間に各要素のところが、
2次元配列の要素の値がある各メモリに配置されるということになります。
擬似的なプログラムで書くと、xとyという変数を二重ループの中で、
この場合縦が8、横も0から7の8ありますので、
8、8ということにして二重ループをして、その中にこの一つの画素に対応する処理をするというわけです。

さあではここで画質について考えてみましょう
皆さんよく画像を見た時にこの画像の画質が良い悪いといったことを言うと思います
そもそも画質が良い悪いといったのはどこから来るんでしょうか
これはもともと画像が持つ情報を考えてみましょう
画像が持つ情報は濃淡情報と濃淡の空間分布から表現情報が含まれていることになります
すなわちこの2つが画質に影響を与えるというわけです
これアナログ画像をデジタル画像に変換する時にまず空間的配置を標本化しました
その後に色明るさを量子化しています
したがってこの空間的配置から選ばれる空間分布と色明るさを量子化した時に得られる濃淡情報
この2つに画質の良し悪しが決まってくるというわけです
ここではこの下の濃淡の上の濃淡情報に着目して画質というものを見ていきましょう
これ左と右の画像があります
どちらの画像の方が画質が良いでしょう
左の方が良いと思う人や右の方が良いという人ですね
色々ですね
じゃあ後でまた見てみます
次今度はこの左の画像と右の画像どちらが良いですか
では今度はこの画像です
これも左と右の画像どちらが良いですか
こちらの画像の方がおそらく良いという人が多いと思います
さあこれちょっとよく見てみましょう
この画像だけ見て何となく感覚的に画質が良い悪いということは言えるんですけども
やっぱりコンピューター上で扱っていろんな処理をするためには
その画質の良し悪しというものを何らか数値で表現する必要があるわけです
そのためにはまず画像の情報を何らか変換をしないといけない
ここではまず濃淡変化だけに注目をします
すなわち濃淡の空間分布の情報が必要ないというわけです
なのでこういう二次元的な情報を一旦ですね
濃淡ヒストグラムと呼ばれる各画素、軌道、明るさの値を持つ画素が
画像中にいくつあるかを数えたヒストグラムのような形で表現をします
これ横が濃淡画素値を表します
例えばこの画像を順番に見ます
一番最初見ると11って値です
11のところに1画素ありました
投票します
次145のところに投票します
次は85ですから85に投票します
というふうに対応する
この各画素の画素値に対応するところに投票していくことによって
各画素値が画像中にいくつあるかというのをこうやって数えることができます
それを度数として縦軸に表現したものが濃淡ヒストグラムになります
これプログラムで二次的に書くとこのような下のようなプログラムになります
よく間違えるところとしてここですね
これは1次元の配列で表現できますので
ヒストという1次元の配列で256個の要素としておきます
これを画像の画素値を参照して
その画素値のところに比の値をインクリメントしてあげればいいわけです
これインクリメント処理をするときには
必ずその1次元配列の初期化ゼロをしておく必要があります
気をつけましょう
さあではこの画像に対する濃淡ヒストグラムを表現してみました
これ見るとわかるように暗いところの値から明るいところの値まで幅広く存在しているというのがわかります
そしてこのあたりの色明るさとこのあたりの明るさが
この画像の中に多く含まれているということもわかります
これが濃淡ヒストグラムというもの
では画質が悪いと思われる画像の濃淡ヒストグラムを見てみます
この画像の濃淡ヒストグラムを見るとほとんどが暗いところに分布しているのがわかります
一方明るいところの画像は少ないほとんどないということもわかります
なのでこの画像は画質が悪いんじゃないかということが
濃淡ヒストグラムにすることによってより客観的に確認することができるようになります
ではこの元の画像に対して今度はこんな画像です
この画像は暗いところと明るいところが全くなくて
この範囲のみに明るさが分布していることがわかります
当然はっきりしないような画像になっているんですね
これもいわゆる画質が悪いという画像になるわけです
この画質というものをですね
さらに濃淡ヒストグラムを求めた後に客観的な数値で表現するための
画像のいろんな統計量というのがあります
例えば濃淡ヒストグラムを計算した後に
一番小さな値を出したものが最小値になります
当然その濃淡ヒストグラムの最大の値を最大値と言います
濃淡ヒストグラムの平均値は画素値の合計からを画像サイズで割ってあげれば
平均値は計算できます
あと中央値というのがあります
中央値というものは画像サイズ割る2番目の画素値です
これ全部で画素が含まれるわけですけど
その半分のところを見るとここの値になりました
これを中央値と言います
再品値は一番この値が大きいところですね
これが再品値となります
あと分散も当然計算します
分散は画素値を平均で引いた後に
自乗したものを相和をとります
ちなみに分散の計算は一般的に
各値を平均で引き算してから自乗しますけども
式変形していくとこのような表現ができます
この各画素値を自乗して平均を自乗したもので
引くということでも分散は実は計算できたりします
覚えておきましょう
このような画像の統計量
最小値と最大値を濃淡ヒストグラムから
まず計算します
ここですね
計算しました
そしてこの値を使って画質を表す客観的な数値である
コントラストというのを計算することができます
例えばこのコントラストというものは何かというと
画像のヒストグラムの分布の広がりを表したものになります
要は画像の明暗を表したものになるわけです
このコントラストの式はCで表しますけども
分母がiマックスとiミニマムを足します
最大値最小値です
こちらがiマックスこちらがiミニマムになります
この値を使って分母はiマックスプラスiミニマム
そして分子はiマックスからiミニマムを引くということになるわけです
では実際にこのような濃淡ヒストグラムが得られたときの
コントラストを計算してみましょう
これはどういう風になるかというと
最大値は250ですから
250プラス5の250-5ということになります
すなわち255分の245となるわけですね
実際にこれの計算をしてみるといくつになるかというと
245割る255ですから
0.96という値になります
これコントラストが1という時はどうなるかというと
分かるようにiマックスが255iミニマムが0だとして
すると255プラス0分の255-0になりますから
255分の255すなわち1になるわけです
すなわちコントラストが1が一番大きな値となるわけです
したがってこのようなヒストグラムを得られたとき
0.96というコントラストになっていますから
コントラストが高いという状況であるということが分かると思います
一方下は計算してみましょう
191-64で191-64になりますから
分母は足し算をすればいいわけですから
191-64をするとこちらは当然255
一方191-64は127となります
なので127÷255は0.49となります
すなわちかなりコントラストが低いということが分かります
これでまず画像から濃淡ヒストグラムを計算して
濃淡ヒストグラムから得られる画像の統計量である
最小値と最大値を使うことによって
最終的にコントラストを出すことができたわけです
したがって画像が与えられたときに
この画像のコントラストは0.96ですから画質がいいですよ
だとかこの画像はコントラストが0.49ですから画質が悪いですよ
ということで客観的な数値でその画像の画質を
表すことができるようになるというわけです
では次に画質を調整してみましょう
画質の悪い画像をより良くするわけです
そのためにはいろんな方法がありますが
まず線形変換について見てみます
こちらのFが入力の濃淡ヒストグラム
こちらのGが出力画像の濃淡ヒストグラムを表します
この入力画像の画素値Fの値を入力したときに
S6画像の画素値Gをどのように計算するかというわけです
その式がこの式になります
この式は何を計算しているかというと
Imax-Imin分のDmax-Dminです
Imax Iminはまず入力画像のIminと
最大値であるImaxがIminとImaxです
ここでいうとここの値とこの値ですね
その後にS6画像の最小値と最大値を決めます
これはどれくらいコントラストを高くしたいかによって
皆さんが設定する値となります
ここではあるDminとDmax値を設定します
そうするとここの項が計算できます
これは何を表しているかというと
この傾きを表しているわけですね
この傾きによってFが入力されると
最小値から引いてその傾きに合わせて計算して
Dmin分を足すということで
幅が狭いヒストグラムを
DminからDmaxまでに広げることができるというわけです
実際にこのような入力画像に先ほどの
線形変換を施すことによって
このようなS6画像が得られます
コントラストが低い画像から
高い画像に変えることができたというのがわかると思います
この画質を調査するために
いろんな変換があります
例えば区分変換と呼ばれる方法では
ある範囲はこれぐらいの傾き
あるi2からi3の値はこのような急な傾き
i3からi4の間はまたなだらかな傾き
というものを区分ごとに傾きを変えて
変換するといったことができる
これもいい方法であるのですが
この各区分ごとにどれぐらいの傾きにするか
ということを決定するための
D1,D2,D3,D4のそれぞれ値を設定しないといけません
ちょっとめんどくさいです
そこでこのγ補正という方法があります
これはγこのような式で表現できるものなんですけど
ここのここにγがあります
このγの値を
いろいろ設定することによって
いろんな変換ができるようになるという
γの値によって
このトンカーブの形状
この形をトンカーブと言いますけども
γの値が1より大きいときは
上に凸のトンカーブになります
γの値が1より小さいときは下に凸のトンカーブになります
上に凸のトンカーブは
暗い値をかなり明るくしてあげます
一方下に凸のトンカーブ
この波線の場合は
暗いところはより暗くして
明るいところはより明るくするということになる
このγ補正の面白いところは
1つの式でこのγの値を変えることによって
いろんな効果を得ることができるようになるという
実際に見てみましょう
暗いところにたくさんの画素が分布しているのがわかります
なのでこの辺りはちょっと見づらいですね
こういったところを見やすくするために
例えば葉っぱの形状とかは非常に見づらい状態です
このようなγ2というトンカーブを使って変換すると
このような画像になります
先ほど非常に見づらかったこの辺の草の
いろんな形状がよりはっきりとわかるようになったと思います
このようなトンカーブは
暗いところを押し上げますから
暗いところが非常に幅広く分布するように
変換された画像になっているわけです
これがγ補正になります
他にはヒストグラムの平坦化という方法もあります
先ほどはこのようなトンカーブを
γ2というのをγ2にするのか
γ3にするのかγ0.2にするのか
我々が決めなくちゃいけませんでした
その値を設定しなくて
自動的によりはっきりとするような画像に
変換しようと
一方、ヒストグラムの平坦化を行うと
このような画像に変わります
これをどうやって実現するかというと
頻度の値が大きい、たくさんあったり少ないというのがありますが
なるべくこれを一定になるようにしようというわけです
ぴったり一定にはならないのですが
ビンというのを決めます
ビンというのは元々の画像サイズを
どれくらいの種類に分けるかというわけです
その一つ一つがビンと呼ばれて
その一つ何種類か
ビンが大きいとその種類が少なくなるわけですが
一つのビンのサイズをまず決めます
そして小さな値から頻度を計算して
その頻度にたどり着いたら
例えば10というふうに決めたら
小さいところを数えて10個到達する後の
10個の値を一つに割り当てます
そして次の10個をまた次のところに割り当てます
というふうにすると基本的には
ビンに足したら画素値を割り当てることによって
このようなヒストグラムに変わる
つまり各画素値が比較的なるべく均等に表現されるので
細かいところのテクスチャがよりはっきりと
わかるような画像にヒストグラムの平坦化を行うことによって
変換することができます
ではこのような入力画像に対して
このようなスローク画像をスロークしたい
この時どのようなトーンカーブにすればいいか
ということを考えてみましょう
この画像からこのような画像をスロークするための
トーンカーブはよく見ると
暗いところは白く
白いところは暗くなっています
ということは反転しているわけです
すなわち暗いところは白く
白いところは暗くなっていますから
こんなようなトーンカーブですね
反転するというのはこのようなトーンカーブになります
では今度は下に見てみましょう
下は白いところは白い、黒いところは黒い
だけど白と黒の2種類しかないわけです
デジタル画像は白と黒しかないわけです
なのでどうなるかというと
暗いところは暗いまま
そして白いところは白いところですから
このようなトーンカーブになる
写真家の方でマンレイという写真家がいます
この人はまだデジタル画像がなくて
何をしたかというと
写真を実際にアナログのカメラで撮影して
写真にするときに現像をします
現像はわかりますか?
暗室、真っ暗なところで現像するわけですけど
そのときに現像するときに光を当ててしまうと
これがうまくいかなくなっちゃう
ちゃんとしたフィルムがダメになっちゃうんですけど
逆にそのことを逆手にとって
このような作品を
アナログのカメラで撮影して
このような作品をカメラで写真で撮影しています
これちょっと面白いのは
輪郭のところがちょっと黒くなっていますよね
これ何かというと
現像するときに濃い光をフィルムに当てることによって
このような効果を得たわけです
このような効果のことをソラリゼーションと言います
このソラリゼーションも実際に
デジタル画像では実現できます
ちなみにこのマンレーは
このソラリゼーションという技術は
現像をしているときにたまたま暗室の扉を開けてしまった
ということによって
そういった偶然によってこのような効果を実現したというわけです
さあトンカーブはどんなようになるのか考えてみましょう
このような画像からこのような画像にしてみたいというわけです
あるところはそのまま出ていますよね
でもあるところは反転したような形になっています
これ実はどうなっているかというと
トンカーブはあるところはまっすぐ右肩上がりになって
あるところはネガで反転して
またあるところはこうやって上がる
このようなトンカーブを設計すると
実際にソラリゼーションと同じような効果を
実現することができます
今日は画像の標本化、量子化
そして濃淡画像集について行いました
次回は画像の空間フィルタリングについて説明したいと思います
それではまた

みなさんこんにちは。今日は第2回目です。空間フィルタリングについて説明します。
今日は画像をですね、いろんな処理、フィルタリングという処理を行っていきます。
まず最初に画像処理の演算について説明します。
これは数学と一緒で単行演算という画像処理のモデル、演算方法があります。
まずこの fij これは画素を表します。
i、j で指定された座標、場所の画素を表します。これを fij とします。
画像はこの画素の集合なので大文字の f で表します。これは集合を表しています。
このような画素の集合である入力画像を入力して何らかの処理の演算を行います。
そして出力画像が g となります。当然出力画像の g も画素の集合となっています。
ではこの演算に、この単行演算の演算の種類にどういったものがあるかということですが、
実は前回やった線形変換やガンマ変換といった濃淡変換は入力画像をスロック画像に演算する単行演算の一種と言えます。
今日はですね、この平滑化という演算について後でやります。他にはサドゥンだとか
強調だとかいろんな演算がこの単行演算に含まれます。
さらにですね、二項演算という処理も可能です。二項演算、単行演算の場合は入力が f 画像1枚を表していましたが、
二項演算の場合は f 1と f 2という2つの画像を入力します。 f 1、f 2、それぞれの画素の集合である f 1と f 2という2枚の画像を入力して何らか演算を行い
一枚のスロック画像を計算します。ではこの二項演算に用いる演算としてどんな種類があるかということですけれども、ある画像とある画像を加算したり減算したり、もしくはブレンディングしたりするという時にこういった二項演算を行います。
今日はこの単行演算における平滑化を説明します。ではまず演算形態として画素単位における演算について説明します。
画素単位における演算においてもですね、2種類の演算方法があります。まず点演算です。
名前の通りなんですけども、入力画像のある画素に注目します。出力画像のあるこの画素の画素値を決定するために入力画像の同じ、基本的には同じ座標の一つのこの画素値に注目して何らか演算をしてここに代入する。
そしてこれをすべての画素で行うことで出力画像が出力することができる。これは前回やった線形変換やガンマ変換は画素ごとに濃淡変換を計算して出力値を計算するというわけですから、点演算を行っていたということになります。
さあ、今日後で平滑化という処理をします。平滑化という処理は画像の周辺である近傍の値を使って計算し、一つの画素に対応する画素の出力値を計算するというものです。
この出力画像における画素と同じ座標を注目画素とします。その注目画素の隣接したこの集合3×3の領域をこの場合は近傍というふうに呼びます。
したがって出力画像のある画素値を決定するために入力画像の複数の画素値である近傍に注目する演算ということになります。さあ、この近傍演算を数式で表現してみましょう。
今、この注目画像のここの画素に対して同じ入力画像の画素がこのちょうど100といったところになります。ここですね。ここを中心とした近傍がこの3×3の値です。
それぞれ上から50、50、50、50、100、100、120、120、120という画素値を持っているというわけです。この周りの近傍の画素値の値を使って何らか演算を行い、この出力画像における画素値の値を計算します。
例えば積和計算を行います。このフィルターというものをあらかじめ用意しておきます。このフィルターはこの近傍と同じサイズで3×3となっています。
それぞれ左上をA、B、C、D、E、F、G、H、Iとします。ここにいろんな値を落ちます。ここでは一般式と表現するため、AからIのアルファベットで表現しています。では積和計算はどのように行うかというと、Aと左上の50の値を持つ
F、I-1、J-1と積をとります。続いてBの位置に対応するここの50ですから、BとF、I-1、Jとの積を計算します。
Aはこちらと、Bは隣と、Cはここと、Dはここと、Eは真ん中と、Fはここと、Gはここ、Hはこの画素、Iはこの画素とのそれぞれ
A、B、C、D、E、F、G、H、Iということで、それぞれ積を計算し、その積の和をとる相和処理が積和演算といいます。一般的にフィルタリングと言われる処理はこのような処理計算方法になります。
では、このAからIの値をどのように設計するかによって、その画像をどのような処理をしたいかといったことを決めることになります。そこでまず今日は平滑化という処理を実現したいと思います。さて平滑化とは何でしょう。平滑化は画像上の濃淡変動を滑らかにする処理です。
平滑化の目的は一つは、例えばノイズ、画像に何らか高周波のノイズが乗っていたとします。そうするとこのようなノイズの低減を図る場合に平滑化を行います。もう一つは、画像をぼかすような効果を得たい場合にも用います。
例えば、パワーポイントの背景にカメラで撮った写真を貼り付けます。その上に文字を書きます。背景が非常に複雑な画像、複雑なパターンを持つ画像ですと、前の上に表示した文字が読みづらいですね。なので、例えば背景をなるべくぼかしましょうというような効果を得たいときに用いたりします。
この平滑化の方法なんですが、いろんな方法があります。ここでは代表的な3つを紹介します。まず、移動平均フィルターです。移動平均フィルターは、局所領域のすべての画素値に対して同じ係数で平滑化する方法です。
次に、荷重平均フィルターです。荷重平均フィルターは注目画素に対して周辺はその規模が小さいとするフィルターです。要は、真ん中注目画素の重みは高くして周りは小さくしましょうというようなフィルターを使って設計して平滑化するという方法です。
続いて、ランクフィルターです。このランクフィルターはノイズを除去しながら平滑化するときに使われます。では順番に各フィルターがどのようにできているのか説明します。
まず移動平均フィルターです。移動平均フィルターを数式で表すとこのような式になっています。移動平均フィルターは注目画素を中心とする画素の局所領域の平均値を注目画素の数録とします。平均を計算しますので、F画素をFiプラスK、JプラスLというふうに表現します。
この時にKとLをそれぞれこの3メンションとこの3メンションで操作をします。Kは-MからM、Lは-NからNです。ここでMイコール1とするとKは-1、0、1となることがわかります。
さらにもう一方のNを同じように1と設定するとLは-Nですから-1、0、1というふうになるのがわかると思います。
従ってまずKが1の時にLが-1、0、1というふうになります。続いて今度Kが0の時にLが-1、0、1となります。
Kがある値の時に3つ行いますので、3×3のすべてで9つの画素値に対応する相和を計算するということになります。
その後、こちらの式で計算するとMは今1、5は1でした。同じようにNも1ですから2×1たす1ですから2たす1で3になります。
同じように2×1たす1ですから同じように3になります。すなわち3×3ですから、結果この式はすべての1、2、3、4、5、6、7、8、9つの画素の相和を計算した後、9の値で割っていることと同じになります。
この式が上の式になるわけですけれども、この式のこの9で割っている9分の1をそれぞれの画素に分配してあげます。
当然下のような式を変換することができます。そうすると9分の1の画素値というふうにこのような式で書くことができるわけですね。
この時、それぞれこちらは画素を各画素を表しています。その各画素にかかる係数が9分の1、9分の1、9分の1、9分の1、すべて9分の1ということになるわけですね。
なので先ほどのここで紹介したこのフィルターの各係数の値がすべてこの場合は9分の1になりますよということになるわけです。
ではこちらが入力画像です。この入力画像に3×3の移動平均フィルターを施した時の出力画像がこちらになります。
さあ見てみてどうですか。よく見るとこの目のあたりちょっとぼけているのが少しわかるんじゃないかな。
あと髪の毛のこの辺かな。この辺を比較してみると違いがよくわかるかもしれません。
動画上ではわかりづらければ配布したpdfを確認してみてください。
さあ今度はこの移動平均したこの金棒のサイズを大きくします。今はmが1、nが1としていました。
なので3×3になりました。今度はmを2、nを2とします。
そうするとここに2を代入しますので2×2プラス1なので5。
nにも2を代入しますので2×2たす1ですから5。すなわち5×5のフィルターになります。
5×5の場合は係数は1Ⅶという係数となります。
この入力画像に5×5の移動平均を施すとこのような出力画像になります。
ここまでフィルターのサイズを大きくするとより平滑化の効果が出ているということがわかると思います。
さあではもうさらに大きくしましょう。今度はmを5、nを5としました。
そうするとそのフィルターのサイズは11×11になります。
実は値は1つ1つが0.008という係数を持ちます。
そうするとこの入力画像に対して11×11の移動平均を施すとこのような平滑化画像になるというわけです。
先ほどの5×5の時の結果と見てみると11×11にするとさらにボケの度合いが大きくなっているということがわかると思います。
これは移動平均する範囲を広くしていくので結果ローパスフィルターの遮断周波数がどんどんどんどん低くなるわけです。
そうなるとよりローパス、低周波の成分だけ残るような状態になっているという。
今の考え方は一次元のデジタル信号処理でも全く同じです。
さあまず移動平均フィルターをやりました。移動平均フィルターによって近傍領域の平均をとることで平滑化ができました。
ただし移動平均する近傍領域の大きさを大きくしていくとかなりボケてしまいます。
そこで平滑化はするんだけどもできるだけ元の画像の情報を残すように平滑化したいというわけです。
注目画素も周りも全く同じ係数でした。
いやそうではなくて注目画素はより出力されて、でも周辺の周りの値をうまく使いながら平滑化しようという方法がこの可重平均フィルターになります。
可重平均フィルターは注目画素に対して周辺がその規模が小さいとするフィルターによる平滑化になります。
このように3×3の場合の平滑化の係数というものを設計します。
どのように設計するか。注目画素が真ん中ですから注目画素だけこの場合係数を2という風にします。
周りはすべて1にします。分子を見ていただくとわかると思います。
今回この場合は分子の値をすべて足した値が分母の値になります。
すなわち1たす1たす1たす1たす2たす1たす1たす1たす1ですから10ですね。
ですからこの場合は分母は10で注目画素だけ2それ以外は1となります。
例えば上下左右と斜めの場所では係数を変えてみましょう。
例えば一番端、遠いところは1とします。上下左右は2とします。
じゃあ真ん中は4にしました。この黒い値が黒くなればなるほどこの四角形の色がグレースケールの色が黒いければ黒いほど重みが大きいということを表しています。
このような場合は分母の値は1たす2たす1たす2たす4たす2たす1たす2たす1で16になります。
なので分母はすべて16になります。
さあでは今度は5かけ5の値を考えてみましょう。
5かけ5ですから真ん中上下左右斜めさらに上という形で値が変わってきます。
この分子の値を見ると1、4、6、16、24、36というような形で係数分子の値が重みが変わっています。
この場合の分母の値は分子の値をすべて計算すると256になりますので分母は256というわけです。
こうするとより広い範囲だけどかつ注目画像に注目したような形での過重平均フィルターを実現することができるというわけです。
ではみなさん今度は3かける3、5かける5が出ましたので7かける7のフィルターを設計してみましょう。
7かける7ということは49個の係数を何らかの方法で決定しないといけません。
だんだん大変になってしまいます。
そこでどうするかということなんですが、こうやって今まで過重平均フィルターのように係数を我々人が設計していくと大変です。
これぐらい小さいところであればいいんですけども、7かける7とか9かける9になると大変です。
そこでこの係数、重みの値を何らかの分布に従って自動的に計算しようという方法がこちらのガウシアンフィルターというものです。
ガウシアンフィルターのガウシアンとはガウス分布のことですね。正規分布と同じです。
このような形状を持つもの。これの2次元のガウシアン分布はこのエクスポネンシャルのこの式で表現できます。
iとjは中心からの距離で、真ん中が0、0という値です。
なので離れれば中心が一番大きくて、中心から離れれば離れるほど値が小さくなるような値をするのがこのエクスポネンシャルというものになります。
この値を各このフィルターの係数として採用しようというのがガウシアンフィルターです。
ガウシアンフィルターにはここにシグマという値、パラメータを設定しないといけません。
シグマはこのガウシアン分布の傾き度合いを表すものです。
次の資料で説明します。例えばシグマが3というふうに設定すると、このような分布の形状を持つ分布がガウシアン分布となります。
この各格子の値をフィルターの係数としてこの入力画像にガウシアンフィルターの平滑化をすると、このような平滑化画像が計算できます。
先ほどの移動平均フィルターによる平滑化と比較すると、よりはっきりと平滑化されていることがわかると思います。
例えば髪の毛のところだとか、こういったセーターのところのガタガタしたところがかなりボケていると思います。
だけど全体的にはっきりと映っているような画像にすることができるというのがわかる。
今度はシグマを大きくしました。シグマを大きくするとこの傾きが滑らかになります。
シグマ3のとき、シグマ5、このような変わります。
シグマ5で平滑化すると、より広い範囲に対して影響を受けるような形で平滑化をしますので、
この平滑化度合いが大きくなったというのがわかります。
もう一回シグマ3がこんな形。
次にシグマ5にすると、かなりボケの度合いが強くなっているのがわかる。
当然傾きが広くなっていきますから、周りの情報より広く含んだ形で平滑化を行うので、
平滑化度合いの高い、よりボケた画像になるということがわかります。
さらにシグマを10とするとだんだん平らになっていきます。
平らになればなるほど移動平均に近くなりますね。
なので従ってこの場合は、より平滑化度合いの大きい、こういったボケた画像になるというわけです。
最後に平滑化の最後としまして、ランクフィルターについて説明します。
ランクフィルターは背景成分の局所特徴をできるだけ保存しながら平滑化しようという方法です。
この方法は、まず近傍領域における画素値を並び替え、ソーティングします。
小さい順にその画素値で並び替えをするわけです。
その並び替えした後に最大値を取れば最大値フィルター、中央値を取れば中央値フィルター、メディアンフィルター、
そして最小値を取れば最小値フィルターとなります。
例えば近傍の値がこのような画素値を持つとし、
200、210、220、150、200、100、200、210、230、これをまずソーティングします。
9つの値があります。小さい順に並べていきますので、
まず100、150、200、200、200、210、210、220、230という形で9つの値が小さい順に並びました。
今9つの値がありますので、真ん中である中央は5番目になります。
したがって1、2、3、4、5の5番目の200と値を採用しますよという時は、
いわゆるこの中央値フィルター、中央値の値ですから、メディアンフィルターという方法になります。
この方法の良い点は、もしこの値がとんでもなく間違った値が入ったとし、
例えば0という値が入ったとし、この時0の値が入って全体の平均値を計算すると、
この間違った値であるノイズとしましょう。
このノイズの影響を平均値フィルターの場合は大きく影響を受けてしまいます。
一方、この中央値フィルターの場合は、ここの値が100という値が0となったとしても、
中央値の値は変わりませんね。
ということで、このランクフィルターはできるだけ局所特徴を保存しながら、
実際にフィルターを作ることができます。

平滑化するといったことができる。では実際に処理結果の画像を見ながら各手法の特徴を見てみましょう。
まずこれが入力画像です。この入力画像に移動平均フィルターを施しました。移動平均フィルターを施すと全体に非常にこのベーッとしたような平滑化が行われるということがわかると思います。
次にガオシアンフィルターです。ガオシアンフィルターは先ほどの移動平均フィルターに比べると元の画像の情報をなるべく残しつつ、かつ平滑化しているといったことがわかります。
続いてメディアンフィルターです。メディアンフィルターを施すとややちょっと違和感のある画像になりますが、とはいえ何らか平滑化が行われているということがわかるかと思います。
あとでメディアンフィルターの良い点については説明したいと思います。
続いて違う入力画像におけるそれぞれの平滑化手法の比較を行います。
今度は入力画像にこのようなゴマ仕様ノイズが付与されたとします。
このような画像に対して移動平均フィルターをかけます。そうするとそれなりに平滑化がうまくいっていると思います。
ノイズの影響がこういった入力画像にすると非常にゴマ仕様的な細かいノイズが乗っているのがよくわかるんですけれども、
移動平均フィルターをかけることで比較的その影響は受けないようになるということがわかると思います。
続いてガオシアンフィルターです。ガオシアンフィルターは注目画像に重みをつけて平滑化をしますので、
残念ながらノイズが大きく乗ったところにはそのノイズが注目画像であるとそのノイズをそのまま残すような形で平滑化を行ってしまいますので、
背景のところを見てわかるようにもともとのノイズの影響がそのまま出てくるといったデメリットがあります。
続いてメディアンフィルターです。メディアンフィルターも残念ながらあまりノイズ除去、
ごま塩ノイズに関してはノイズ除去という観点ではうまくいっていないことがわかります。
これはなぜかというと、近傍領域である3×3、すなわち9つの画像のうち5個以上にノイズが乗ってしまうと、
その中央値である値にもノイズが乗っていることになります。
したがってこのようなごま塩ノイズ、非常に広い範囲に多くの画素にノイズが乗るような場合であると、
残念ながらメディアンフィルターでもメディアンフィルターはうまく働かないということがわかります。
では最後に平滑化手法の比較です。今度はこのようなスパイク状のノイズが画像に付与されたとします。
例えば、最近はこのような例は少ないと思いますけど、何らか通信をしている画像を通信して送っている途中にノイズが入って、
そのノイズがあく影響としてこのようなスパイクノイズになったと思ってください。
このような画像に移動平均フィルターをかけるとどうなるでしょうか。
皆さん一度まず想像してみましょう。答えはこの形です。
移動平均フィルターですから、そのまま局所領域における平均値を計算しますから、
スパイクノイズの影響をこのように受けたような結果になってしまいます。
続いてガオシアンフィルターです。ガオシアンフィルターは先ほども説明したように、
注目画像に重みを大きくして平滑化を行いますから、スパイクノイズのところがより強調された形で平滑化してしまいますので、
このノイズの影響をするまま受けた状態となってしまいます。
ではランク中央値、メディアンフィルターはどうでしょうか。
メディアンフィルターは局所領域における中央値を取り出しますので、
スパイクノイズが中央値から離れたどこかに存在していてもその影響を受けないというメリットがあるわけです。
同じ平滑化においてもそれぞれ移動平均フィルター、ガオシアンフィルター、メディアンフィルターによってそれぞれ効果がこのように違うというわけです。
なので各平滑化手法の仕組みをちゃんと理解して、その問題に合わせて適応していく必要があります。
ではここまでのまとめです。
それぞれの平滑化効果について皆さんの考えを記入してください。
動画は一旦ここで一時停止しましょう。
一時停止して、このそれぞれに対して効果を代であれば20、効果がなければ×といった形でそれぞれのところに記入をしてみてください。
では始めてください。
では平滑化の効果をまとめていきましょう。
まずノイズない画像に対して移動平均フィルターは確かに効果ありました。なので○でしょう。
ガオシアンフィルターはとてもいい感じに平滑化できました。
メディアンフィルターはちょっと変な感じがしたのでやや効果ありというぐらいでしょうか。
さあゴマ塩ノイズをした場合移動平均フィルターはそれなりに効果ありましたよというわけです。
ガオシアンフィルターをゴマ塩ノイズにすると残念ながらあまり効果がありません。
同じようにメディアンフィルターをゴマ塩ノイズの画像にノイズ除去をしてみるとこれも効果がありませんでした。
じゃあ続いて今度はスパイクノイズです。
スパイクノイズは移動平均の場合は残念ながらこれも効果はありませんでした。
ガオシアンフィルターも強調しますので効果がなし。
一方スパイクノイズに対してメディアンフィルターは効果がありますよというような形。
なのでこのように各手法によって得意不得意とするところが異なるわけです。
これらの性質をちゃんと理解してどういうような画像に対してどのような効果を得たいのか。
平滑化のどのような効果を得たいのかということを理解した上でちゃんと適切なフィルターを適用しましょう。

では続いてエッジ抽出について説明をします まずエッジというのはどういったところでしょうか
エッジというのは濃淡が画素値が急激に変動するようなところを エッジと呼びます
画像中で明るさが急に変化するようなエッジを 何らか画像処理によって抽出をしたいというわけです
このような変化を捉えるためには微分ということを行います
さあ画像における微分というものを考えなくてはいけません まず一般に x に関する変微分という式を出します
f x y が画像です この x と y は i と j ではなくてアナログ画像
連続的な値を持つものとして表現しています いわゆる連続関数です
なのでこれはアナログ画像です アナログ画像における x 方向 x に関する変微分はこれはよく微分積分で出てきたようなこのような式で表現できて
まあこれをそのまま解ければいいんですが この f x y は残念ながら連続関数です
我々が扱う画像はデジタル画像です デジタル画像はそもそもこの delta x を 0 に近づけるということができません
なぜかというと画素というものはもう離散化されていますので整数値になっています 0 に近づけるといったことができないわけです
そこで画素の最小単位は 1 画素になりますのでこの delta x を 1 というふうに置くわけです
そうするとこの分母が 1 このここの delta x も 1 になりますので結果式は f x y の x に関する微分は
x プラス 1 の画素値から f x の 1 の画素値を引き算
すなわち差を計算するということになります したがって画像の微分というものは差分近似で表現することができます
くどいですけどもデジタル画像における微分は結果 隣り合う画素値の差として表現することができるわけです
それでは先ほど計算した微分を用いてエッジ抽出を行います これを微分フィルターと呼びます
まず横方向に微分を計算します 横方向に微分を計算するとそれを delta x と呼びますが縦値が計算することができます
この横方向に微分を取った値を勾配と言います 続いて今度は縦方向に差分を計算します
y 方向の差分です 結果 y 方向に勾配を計算すると横エッジが出ますよというわけです
この時それぞれの勾配 delta x と delta y には正の値とか負の値を持ちます 当然ながら差分を計算しますので
不正が存在するわけです そこでこの横方向の勾配と縦方向の勾配を合成して勾配を求めます
どうするかというと x 方向の勾配を自乗して y 方向の勾配を自乗して足したものをルートを取ります
それが各画素における勾配の大きさということになります それを示したのはこの画像となります
さらに各画素に対して x 方向と y 方向の勾配がありますので x 方向の勾配分の y 方向の勾配を計算して
アークタンジェントを取ることでこの勾配の傾き どちらの向きに勾配がすなわちエッジが発生しているのかということも知ることができます
これが微分フィルターです 他にもですねいろんなフィルターがあります
クリューイットフィルターというフィルターはなるべくノイズを抑えながらエッジを抽出しようというものです
ノイズを除去する効果としては平滑化がありました 平滑化は局所領域における平均値を計算するというようなものでした
なので微分は隣り合う画素の差分を計算するわけですね なので微分と平滑化を
2つの効果を持つようなフィルターを作ろう その場合は微分のフィルターを表すマスクパターンと平滑化を表すマスクパターンを
合成して一つにするわけです これとこれを合成するとこのようなプリウイットフィルターができます
これは横方向に差分をとりますので横方向にはそれぞれ差分をとっていますが縦方向に見ると1、1、1
ここも-1、-1、-1 同じ係数がかかっていますから縦方向には平滑化をして横方向には差分をとるというような方法
これによってノイズを抑えながらエッジを注視することができるようになるわけです このプリウイットフィルターをもう少し拡張したのがソーベルフィルターとなります
平滑化の中で出てきたように移動平均に対して荷重平均 いわゆる重み付きフィルターというのがありました
重み付きフィルターは収穫画素により重みを大きくするというものです したがって微分は先ほどと同じです
平滑化のところ先ほどのプリウイットフィルターはすべて同じ値だったんですけども ソーベルフィルターでは真ん中を2、上と下を係数を1、1にします
そうすると分子の値を足すと4になりますから1Ⅳ、2Ⅳ、1Ⅳという縦方向に対して真ん中に重みを置きながら平滑化するというわけです
この重み付き平滑化フィルターと微分フィルターを合成してできたのがソーベルフィルターというものです
どのような係数になっているかというと横方向には差分を計算します 縦方向には平滑化を行うんですが収穫画素に近い真ん中の値に重み付けをするという
このようにして平滑化と微分フィルターを合成してエッジを抽出しようという方法です
同様に縦方向の勾配を計算するためには横方向に平滑化して縦方向にそれぞれ差分を計算するというものです
では実際の画像を見ながらどのような効果があるかを見てみましょう まず入力画像です
この入力画像にプリウィットフィルターを施します エッジのところがかなりはっきりと表現されている
続いてソーベルフィルターです ソーベルフィルターは先ほどのプリウィットフィルターよりより
注目画素が強調したような形でエッジを抽出していることがわかると思います では続いてエッジ抽出の中にラプラシアンフィルターという方法があります
ラプラシアンというのは二次微分を表します 二次微分とは一次微分したものをまた微分する
もう一度微分するというもので得られたものが二次微分と言います この二次微分を使ってフィルターに用いたものをラプラシアンフィルターと言います
式ではこのような式で書けます 何を言っているかというと画像に対して x 方向に2回微分をする
画像に対して y 方向に2回微分する それぞれの方向で2回微分したものを二次微分とったものを
足し算しますよという じゃあこのフィルターをどうやって設計するかというのを説明します
まずこちらの x 方向の二次微分の計算を見てみましょう まず
x 方向すなわち横方向の一次微分を計算します 一次微分は
i 隣り合うガスで差分を計算しますので こことここの差分を計算します
もう一つ1個隣にずれてここからここを引くというような一時微分差分を計算します それぞれ一時微分を計算しました
一時微分を計算するフィルターになっています さらに今度は二次微分をしないといけませんから
一時微分したフィルターとさらに一時微分したフィルターの差を取ることで 二次微分フィルターを作ることができる
じゃあ実際にどうやって計算するか見てみましょう ここの画素値はフィルターの値は1-0ですから1になる
続いて-1-1ですから-2になります 続いて0--1ですからマイナスとマイナス1がかかって1になります
すなわちこの二次微分フィルターが x 方向横方向の二次微分を取るためのフィルターになるわけです
同様に y 方向に対しての一時微分をまず計算します そしてこちらからこちらの一時微分フィルターから一時微分フィルターの差を取って
二次微分フィルターを計算します これも同様に1-2-1という値を持つ二次微分フィルターが計算できます
最後はこちらがこちらでした 最後は足し算が残っています
なので足し算をしましょうというわけです 足し算をすると0たす0は0 0たす1は1 0たす0は0 1たす0は1
-2たす-2は-4 1たす0は0 0たす0は0 0たす1は1 0たす0は0ということで最終的にこのような係数を持つ
二次微分であるラプラシアンフィルターを設計することができたというわけです さあこのラプラシアンフィルターを使うときに平滑化というものと組み合わせて使おうというのが
だんだんちょっと複雑になってきましたがラプラシアンオブガオシアンという方法があります これは何かというとラプラシアンオブガオシアン
ガオシアンの二次微分をログフィルター 略してlogフィルターというふうに呼びます
ガオシアンフィルターで平滑化しさらにラプラシアンフィルターを適用するような フィルターを作ろうというわけです
これログフィルターの形状はこのような形状になります ガオシアンの形状は二次微分をとりますからこのような形状になります
このログフィルターの面白い点はシグマの値がありますので このシグマの値を変えることによってそのエッジの抽出できる効果が変わります
ガオシアンフィルターのシグマを変えると平滑化する度合いが変わりました したがってシグマを2としてログフィルターを計算するとこのような形になります
シグマを4としてログフィルターを計算するとこのようなスロックになり シグマを6としてログフィルターのスロック値を計算するとこのような形になります
このログフィルターのスロックに対して 正から負もしくは負から正に切り替わるところのゼロ交差する場所を抜き出すと
このようにエッジの抽出をすることができます 面白いのは同じ入力画像に対してシグマを変えることでこのシグマに合わせたサイズの
スケールと言いますけどもシグマに合わせたスケールのエッジ シグマの値が小さいとより細かいエッジを
シグマの値が大きいとより大まかなエッジのみを抽出することができるようになります これをログフィルターと呼びます
さあ先ほど出てきたラプラシアンフィルターを使うと画像を鮮明化することができます 画像の鮮明化というのはどういうことかと言いますと
少しぼけた画像が入力されると少しシャキッとした引き締まったような 画像を作るようなことを鮮明化と言います
鮮明化は何をしているかというとなだらかに変化しているようなところをですねより エッジをですね強調するようにする
エッジを強調するということは斜めの傾きだったものを傾きが大きくなるようにきつく なるようにするということによって画像の鮮明化
はっきりくっきり写ったような画像にすることができる どうやって実現するかというと注目画像から注目画素から
ラプラシアンフィルターを引いてあげればその鮮明化が実現できます 注目画素だけを取り出すためには周り0で真ん中だけ1といったフィルターを作れば
okです このフィルターからラプラシアンフィルターを3分を取ります そうするとこのようなマイナス1マイナス1マイナス1マイナス1
真ん中だけ5といった鮮明化フィルターが計算できます この鮮明化フィルターを画像に演算します
フィルタリングを行います これが入力画像です この入力画像に対して鮮明化を行うとこのような画像になります
もう一度元に戻します こちらが入力画像 鮮明化した画像がこのような形 このセーターのですねパターンを見ていただくと
あんまりはっきりわかってないんです 最初の入力画像の時ははっきりしてないんですけども
鮮明化するとくっきりとですねこういうラインが見えるようになっているのがわかると さあこれどういうふうに実現できているのか実際にもう少し詳しく見てみましょう
元の画像におけるこのラインですね ここのライン上の画素値を縦軸に横軸はここのライン上の値を表示しました
そうするとこのようなプロファイルが見られます このあたりは背景ですからぼやーっとしてますね
ここセーターのところは黒になったり白になったり黒になったり白になったりしてますから こういうギザギザとしたところがこのセーターのところになると思い
ここ二つ値が急に落ち込んでますね それは多分ここの服の色のところですねパンツの服のところが黒くなっていますから
値が小さくなっているのがわかる これは入力画像のこの1ラインにおける画素値の変動を表したもの
では繊維化した後の同じラインにおける画素値の変動を見てみましょう そうすると先ほどはこれぐらいの範囲で濃淡の変動が起こってたものが
繊維化を行うことでより大きく離れるように変化したというのはわかると思う
すなわち傾きがよりきつく大きくなったというわけです この繊維化の効果というのがこういったところを見るとよくわかると思います
さあでは二次微分を用いた繊維化の原理というものを復習してみましょう まず入力画像に対して
一次微分をとります この破線のこのデータが入力データだと思ってください
この入力データにまず一次微分を計算します 皆さんもですねぜひノートか何かに書き込んでやってみましょう
まず一次微分を計算するとここまで変化がないところはゼロです ここから値が右肩上がりに大きくなりますから大きくなってここで最大となってまた大きさが
減っていきますここまたゼロですね傾きがありません 今度は右下がりになってますから下に今度はマイナスにこうなってこのような感じになる
この赤色で書いたのが一次微分した結果になります 次に二次微分を計算します
二次微分はこの一次微分をとったものをまた微分をとるわけです 今度はゼロの位置がここに変わっていますからそこ気をつけて書きましょう
変化がないとこはゼロのままここで上に上がって今度ここの頂点のところは傾きがないというわけですけどここはゼロになる
そしてそこから今度は右下がりになっていきますからマイナスの方向に行ってこうなるわけですね これでここのとゼロになってここは次下に行ってますから下に下がってから上に上がって
こうなる形になります これが二次微分をとったものになります 最後原画像からこの二次微分を引きます差をとりますそうするとここはずっとゼロだからこうなんですがここからここは正の値を差をとりますから負に変わりますけどちょっとここからだんだんこうなってここがゼロになってこうなってまたゼロになってちょっと上がってこんな形でしょうか
これが原画像から二次微分を計算したものです 元の画像のこの傾きに対して原画像から二次微分を引いたものはよりきつい傾きになっているのはわかると思う
これによって千円化を実現したということになるわけです 今日は空間フィルタリングとして平滑化とエッジ抽出について説明をしました
次回は画像の気化変化について説明をします それではまた

皆さんこんにちは 今日は画像の気化変換とイメージモザイクについて説明したいと思います
まず画像の気化変換ということですけれども これまでの画像処理は画像の濃淡に注目して
その濃淡を近傍領域の例えば赤和演算を行って なんらか平滑化だとか
園地抽出といったことをしてきました 今日はですねそういう処理ではなくて画像そのものを
気化的に回転したりだとか拡大縮小したりする 変換処理について説明をします
この画像の気化変換は非常に面白いところです この画像処理というですねこの分野っていうのは
非常にですねなんでしょう 工学的アプローチによる解決が重要となっています
どういうことかというと数学は当然重要なんですけども 数学の世界だけでは残念ながら画像処理はうまくいきません
その数学的基礎知識を使って かつ工学的なエンジニアリングといったアプローチをうまく組み合わせることで
より良く画像処理を実現するといったことになります そこで今日はですねまず最初に画像の
気化変換をするために座標変換について説明をします そしてその後工学的なエンジニアリングによる解決方法として
画像の再配列と画像データの内装について紹介します ではまず画像の座標変換をするわけですけども
その座標変換をするときにはどのような 比較的な変換をするかによってその変換式が異なっていく
座標を計算するための変換式が異なっていく 座標変換はどのように行われるかというと
x と y が変換前の座標になります そして x' y' が変換後の座標になります
この式を見ていただくと変換前の座標である x と y に a b c d という何らかの変換パラメータがあって
その変換パラメータを x と y にかけることで 変換後の座標である x' と y' を計算します
この場合 x' は a x プラス b y y' は c x プラス d y というふうに表現できる
これは気化変換の一般的な式表現になります それでは具体的に一つ一つのいろんな気化変換についてその座標変換の数式を見てみましょう
まずは平行移動です 平行移動は英語では translation というふうに呼ばれます
このとき移動量をまずパラメータとしてあらかじめ決めておきます この tx こちらは x 方向の移動量を表します
一方この ty というものはこれは y 方向の移動量を表します
このようにまず tx と ty をあらかじめ皆さんがどのように変換したいかということで値を決めます
そしてそのパラメータ tx, ty を使って変換前の座標である x, y を x, y にそれぞれ tx と ty を足すことによって
平行移動の変換後である移動後である x' と y' を求めるという
下の例はこのような入力画像に対して x 方向にプラス100 y 方向にプラス50したときの平行移動した後の画像はこのようになります
それでは続いて拡大縮小です 拡大縮小は英語でスケーリングと呼ばれます
ここも同じようにあらかじめ皆さんがどのようなパラメータで拡大をするか縮小するかといった値を決めておく必要があります
拡大縮小においては x 方向の拡大縮小率である sx と y 方向の拡大縮小率である sy というパラメータがあります
この sx と sy の値が1より小さければ当然ながら縮小 そして1より大きければ拡大を実現します
さあこの sx sy を使って拡大縮小後の座標を x' y' とするとどのように表現できるかというと
x に対しては sx を乗算し y に対しては sy を乗算するということになります
この式を一つの行列で表現すると sx 0 0 sy といった行列で式で表現することができるのがわかります
このような入力画像に対して x 方向 y 方向ともに sx sy が 1.5 とした場合はこのように拡大することができるわけです
続いて回転であるローテーションです ローテーションはこれは高校生の時の数学でやったと思いますけれども
cosθ-sinθ sinθ cosθ というのが回転をするための行列となります
この式の動質に関しては今日はやりません 高校数学を見返していただくと出てきているはずです
この r のことを回転行列というふうに呼びます この回転行列は変換前の座標である x,y にかけることになります
したがって変換後の x' は cosθx-sinθy が x の変換後の座標
そして y 座標は sinθx たす cosθy になってこちらが y 変換後の y 座標となります
このような入力画像に対して半時計方向に15度回転するとこのような画像が得られるわけです
続いてスキューという変換です このスキューは長方形を横に水平方向にギュッと変更した変形に変形するような処理を言います
このスキューは水平方向のスキューと垂直方向のスキューがあります まず水平方向のスキューですけれども
どのようにするかというと y 座標はここ y 座標に対しては横方向に見ると同じですね
同じ座標になっていますから y 変換後の y 座標に対しては変換後も同じになります
それに対して x の座標だけを変化していく どのように変化していくかというと x に y タンジェントシーターを足します
この y タンジェントシーターですけれども これは基本的には x 方向に対する移動量になるわけです
その移動量が最初 y の値が 0 ですよね 0 からだんだん下に行くほど大きくなるわけです
したがって最初 0 の時はここが 0 になって x' は x のそのままということですけどここになるわけです
これが y の値が下になればなるほど大きくなるにしたがってこの平行移動量がどんどん増えていくというのがこの水平方向のスキューになります
垂直方向のスキューは先ほどと逆です x に関しては全く同じ値です
一方 y については x の値が増加するにつれて y の移動量が増えるということになる
そうすることによってこのような垂直方向のスキューといったものを実現することができます
それでは実際に座標変換を計算してみましょう
今 x1 y1 x2 y2 x3 y3 の3点が与えられています
この3点の各座標を順番に座標変換をしていきます
まず最初に平行移動を行います x 方向にマイナス30 y 方向にマイナス30ですからその一般式はどのようになるかというと
x-30 y-30となります
ではこの一般式を使って x1 y1の変換後の座標をこちらに x2 y2の変換後の座標をこちらに
x3 y3の変換後の座標をここに記入してください計算してください
そしてその計算した座標を使ってこのグラフ上にプロットをしてください
では続いて先ほど平行移動した後の座標に対して今度は xy 共に0.5倍に拡大してみましょう
まず一般式は0.5倍ですから0.5x 下も同様に0.5yというのが一般式になります
では先ほど変換した後の x1 y1 x2 y2 x3 y3に対してこの式を使って変換後の座標を求め
そしてこのグラフ上に3点をプロットするようにしてください
続いて回転を行います
まず回転の一般式はcosθxプラスマイナスマイナスsinθy sinθxプラスcosθyでした
この式に45度ですからsinθを45度として計算をしてください
そして同じように x1 y1 x2 y2 x3 y3の各座標の変換後の座標を計算し
そしてこのグラフ上にプロットをしてください
続けて最後にもう一度 x 方向に30 y 方向に30 平行移動します
先ほどの回転した座標にこの今度は平行移動を適用します
最終的にこの x1 y1 x2 y2 x3 y3がどのように変換されたかというのは
課題の方で提出をしてもらいます
さあここまで座標変換を説明しました
座標変換ができればこの画像の基下変換ができたかというと
残念ながらそうではありません
数学として座標変換をするということはできるわけなんですけども
残念ながら画像はですねデジタル画像を扱います
デジタル画像というのはどういう性質があったかというと
そもそも座標が整数系の座標です
小数点がありませんとですね
なので実はこういった問題が起こります
例えばどういうことかというといろんな変換を組み合わせてやっていきます
じゃあ実際に今から変換をしていきます
こちらの入力画像を x 方向に-320
y 方向に-240して変換をするとこのような画像が得られます
ここは大きな問題ありません
続いて次は変換2として先ほどの平行移動した画像に対して
今度は0.5倍に縮小します
そうするとこちらの画像がこのような画像に縮小されます
ここまでも見た感じ問題ありません
さあじゃあ次は今度は先ほどの縮小した画像に対して
45度の回転を施します
そうするとこのような画像です
45度回転しました
そしてこの45度回転した画像にもう一度320、240ということで
真ん中に移動します
そうするとどうなるかというとこんな画像になってしまいます
これって本当はここをもっとこうあるわけですよね
ですけども元の変換前の画像と変換後の画像は
例えば同じ大きさで用意する画像ですね
2次元配列として用意していくと
はみ出たところはマイナスという値をとります
そもそも2次元配列を扱うときに
マイナスのところをアクセスすることができませんね
なので用意したところだけには値を置いておくことができるんですけども
外れたところはなくなっちゃう
なのでこのようにもう一度平行移動して拡大縮小をして
そして回転して平行移動すると
外にはみ出たところがなくなってしまったような画像となってしまう
これでは本来我々が意図した変換ではないわけです
そこでこの問題を解決するためにどうするかというわけなんですけども
行列を使いましょうということです
先ほどのまず変換するためのパラメータをそれぞれ行列で表現しました
平行移動の場合はTx、Tyが縦に
拡大縮小もSx、Syが縦に
そして回転行列は2×2の行列という風になってました
これらのいろんな変換をですね
まず一つの行列で表現できるようにしようということをします
この時3×3の行列で全ての変換を表現するようにします
3×3にするということは変換後の座標がx'、y'
そしてもう一つ出てきますのでここに1というのを入れます
こういったのを同時座標という風に呼びます
この1という同時座標を導入することによって
この変換行列を3×3で表現することができる
ただしこの一番下に関しては
この一般式はまず上から順番に行くと
x'イコールaxプラスbyプラスc×1ですからこのような式になります
次y'に関してはdxプラスbyプラスfということになります
さらにもう一つ式が出てきますね
1イコール0×xプラス0×yプラス1×1ですから
最後の式は1イコール1という式が表現されていることになります
この一番下のところを追加することによって
上のこのaからfの値にそれぞれ拡大縮小回転平行移動といった
パラメータを当てはめることで
それぞれ行列の形や違うものを
全て3行3列の行列で表現することができるようになるわけです
順番に見てみましょう
平行移動はTxTyというものでした
x方向の移動量がTx、y方向の移動量がTyでした
この場合、平行移動をするためには
このような3行3列を表現します
実際に見てみると
x'変換後の座標x'イコール1×xプラス0×yプラスTx×1ですから
xプラスTxがx'になることがわかります
同じようにy'イコール0×xプラス1×yプラスTy×1ですから
yプラスTyというのがy'になることがわかります
例えば、x方向に30ピクセル、すなわちTx
y方向に30ピクセル、Tyを30とすると
こことここにそれぞれの30、30を代入してあげれば
3行3列で平行移動、x方向に30、y方向に30移動する
平行移動を表す行列が表現できるようになりました
続いて拡大縮小です
拡大縮小率であるxx、syは
このxとyという変換前の座標に直接乗算することになります
なのでx'イコールsx×xプラス0×yプラス0×1となりますから
残るのはsxxです
同様にy'イコール0×xプラスsy×yプラス0×1ですからsyyとなります
例えば、x方向に0.5倍、y方向に0.5倍縮小する場合は
sxに0.5、syに0.5を入れたこの3行3列が拡大縮小を表す行列式となります
続いて回転です
回転行列はコサインシーター、マイナスサインシーター、サインシーター、コサインシーターでした
それがここのa、b、c、d、e、fのここの2行2列のところに代入されます
したがって90度回転する場合はコサインシーター、サインシーターが90ですから1になります
マイナスがつきますからマイナス1、コサインシーターは1のままですね
コサインシーターの90を代入した場合は0、0となりますので
x方向、ごめんなさい、90度回転する場合はこのような式になります
ここまでそれぞれの変換行列を3行3列で表現することができました
これをこの3行3列の行列を使って何ができるかというと
今までは平行移動の行列、拡大縮小の行列、回転の行列がありました
どうやって計算したかというと
まず変換前の座標に対して平行移動の行列をかけて計算して
平行移動した後の座標を計算しました
そしてその変換した後の座標に次はこのHSという行列、すなわち拡大縮小を表す行列をかけます
そしてさらにその変換した座標に回転する行列をかけるわけです
このようにまずこれを計算して、次これを計算して、次これを計算してということを順番にやって
変換後の座標を求めました
これはこれでいいわけなんですけども
この変換後の座標と変換するための行列というものを順番にこの座標にかけていくと
残念ながら計算するときに例えばコサインシーターとかですと小数点がいっぱい並ぶわけです
そういったときにどこで桁数を区切ってしまうと誤差が出ます
その誤差をまた変換座標にはその誤差が含まれており
それをまた変換してまた変換していくと
どんどん誤差がですね変換による誤差が蓄積されるという問題があります
またこの変換行列が例えばいろんな変換をたくさん繰り返していくと
毎回毎回座標を変換するので計算がですね
その変換の数分だけしないといけないという問題がある
これ非常に時間がかかってしまって
そこでどうするかというと
このまず平行移動するための行列と拡大縮小するための行列は
それぞれ産業算列で表現していますから
この行列のまず掛け算を先にして求めます
そうするとこの2つの行列の積がこのような行列になりまして
今度はこの行列と残っている回転の行列との積をとって一つにまとめます
要は座標を順番に変換していくのではなくて
あらかじめこの変換行列をこのように一つにまとめておくことをするわけです
この変換行列をですね
例えばプログラムであればダブルみたいな倍精度で精度をよく求めておいて
そして変換前の座標xyにこの変換行列を計算して
変換後のx'y'を1回の計算で求めるようにすれば
非常に計算時間がかからずに
そして高精度な誤差が乗らないような状態で変換をすることができるわけです
このような方法を使うことによって変換行列を一つにまとめて
その一つにまとめた変換行列を1回座標変換するだけで
もともとであれば途中で外はみ出たところがなくなっていたものが
この変換行列を作ってからちゃんと一度の変換計算によって
好ましい画像をすることができるようになるわけです
はいではですねぜひ皆さん最初にですね座標変換したものを
一つの行列にまとめてみてください
これはぜひ一度手で計算してみてください
答えは皆さんの回答はですね課題の方で提出してもらいます

それでは続いて座標変換をした後に 画像を実際に作っていきましょう
例えば順方向、これまで説明してきた変換では
入力変換前の画像座標に対して拡大1.5倍します
そうすると変換後の座標を計算します
この時にこの変換前の座標が持つ画素値を 変換後の座標に割り当てます
続いてどうするかというと 入力画像の隣の画素に対して変換後の座標を計算して
ここの画素が持つ画素値を代入します
これを順番に入力画像の1画素ずつ変換座標を求めて 値を代入していくわけです
そうするとこのような拡大した画像が得られるわけなんですが
よく見ていただくと特に1.5倍という形で拡大をすると この間抜けてしまったところが出てきます
なのでこのように白い値を持たない画素が あるような画像として画像になってしまいます
これは好ましい変換とは言えません
ではこの問題をどうやって解決するか
実は逆方向の変換ということを行います
逆方向の変換というのはどういうことかというと
先ほどは入力画像から変換後の座標を計算しましたが
そうではなくて変換後の座標が 入力画像のどこから来るかということを調べます
そしてその入力画像の座標が持つ画素値を代入するということをします
そしてこの出力する画像の座標を次隣のところを見て
この座標に対応したところの入力画像はどこかを探して
その入力画像の画素が持つ画素値を代入します
このように出力画像の1画素1画素ごとにこの処理を行うことで
穴が開くような画像ではなく綺麗な拡大をすることができるようになるというわけです
これが画像の再配列というアプローチで逆変換に基づく手法です
これは実際にどうやって計算するかということなんですが
今まで変換する行列というものを求めました
この変換する行列はここでは h で表していますが
変換前の座標から変換後の座標を計算しました
この h この逆を求めないといけません
変換後の座標から変換前の座標がどこであるかということを調べないといけません
そのためにどうすればいいかというと
この式にそれぞれこの変換行列である h に
逆行列をそれぞれ左辺と右辺にかけてあげて計算すると
ここは単一行列になりますので結果はこのような式になります
すなわちどういうことかというと
変換後の座標である x ダッシュ y ダッシュに対して
もともとの変換行列の逆行列をかけてあげれば
変換前の座標である x と y が求まるという
なのでまず変換行列を求め
その変換行列の逆行列を計算して
変換後の座標が変換前の座標のどこに対応するかを調べ
そしてそこの座標の画素値をもとの変換後の座標の画素に
画素値として代入すればいいわけです
これで穴が開かないような画像を
機械変換してもできるようになりました
最後に内装について説明します
内装というのはどういうことかということなんですが
先ほどの逆変換に基づく方法
こちらが変換後の出力
こちらが変換前の入力画像だと思ってください
出力画像の各画素が
入力画像のどこに対応するかというのを調べます
この時にそもそも入力画像はデジタル画像ですから
座標が理算的な整数形になっています
なのでこのような小数点を持つというようなところの
画素が存在しません
そこでどうするかというと
まず一番単純な方法としては
最近傍方、ニュアレストネイバー法というものがあります
変換後の画像の座標から逆行列を求めて
変換前の画像の座標を計算しました
この時赤色のこの点だとしてください
これは小数点を持つ座標なので
ここから一番近いところ
周り4点の整数度を持つ画像の
一番近いところを見つけて
この一番近いところの値を代入しましょうというわけです
これがニュアレストネイバーという方法で
一番近いところを使いますよという方法です
これを実現するためにはどうすればいいかというと
非常に単純です
四捨五入をすればいいだけ
四捨五入をすれば
小数点以下がなくなって一番近いところになります
これはプログラムでは
xに0.5を足して
yに0.5を出して
それで整数化してあげれば
いわゆる四捨五入ができるというわけです
さらにですね
よりきれいな気化的内装変換をするためには
一番近いところだけの画素値を取り出して当てはめると
少し拡大した時とか
ガタガタとしたような画像になってしまう
より滑らかな画像を作るために
どうするかというと
バイリニア保管という方法があります
S6の座標から
入力画像の座標を止めて
この点xyに対して
周りの4点の画素値の値を使って
内装しましょうという方法です
これは近ければ近い画素
例えばここがfa,fb,fc,fdという
それぞれの画素値を持つとしてください
もしこのxyという値が
aに近い座標であれば
aの値はより大きく
それ以降b,c,tの値は小さくなるように
4つの周りの4点の値を使って
画素値を内装するという方法になります
これをバイリニア保管という風に言います
一方、バイキュービック保管といって
周りのより16点を使って
変換、内装するという方法があります
この周りの16点を使うと
より周りの滑らかな画素値が
その領域に対してどのように変動しているか
ということを見つけることができます
例えばバイリニア保管の場合は
x方向だけに関して考えてみると
ここがfa,ここがfbとすると
例えば今この値が
faの値がこれぐらい
fbの値がこれぐらいだとします
この時今この値がxとして
観測されたとすると
この値を計算することになります
すなわち周りの2点を使って
内装したというわけです
一方、バイキュービックは
このような周りの4点を使います
その時に例えばこのxが観測されたとして
同じようにこれぐらいの値と
これぐらいの値で
この時にこの周りの値が
もしこのような値だとすると
ここはこういう感じになっている可能性があります
したがってこの値が
内装されることになります
なのでこれを見てわかるように
バイリニア保管の場合は
周りの2点を使って
直線、実際は2次元空間ですから
平面としてこうやって内装します
一方、バイキュービック保管は
周りの点をこうやって使いますので
こういうような曲線上で
どこになっているかといったことを
求めますので
よりバイキュービックの方が
きれいな内装をすることができます
この結果については
テキストの方に画像がありますので
確認をしておいてください
では実際に画像の
機械変換した時の
内装による違いを紹介します
まず、細菌防法です
細菌防法を画像を拡大すると
やはりちょっとガタガタとしたような
形になっているのはわかります
続いて、バイリニ保管です
バイリニ保管することで
より先ほどの内装に比べて
よりきれいな保管ができて
拡大してもよりきれいな
画像になるというわけです
では続いて、機械変換を
利用した処理として
イメージモザイクについて紹介します
イメージモザイクは
非常に面白い処理でして
例えばどういったところに
使われるものかというと
航空写真でこのような画像を
いっぱい撮影します
これらの画像から
1枚の大きな画像を
作ったりするときに使われます
例えばGoogleのストリートビュー
これも複数のカメラで
撮影したものを
1枚の画像のように見せています
こういったところに
このイメージモザイクという処理が使われます
この処理手順は
まず画像上で特徴点というものを見つけて
その特徴点と特徴点が
画像間でどこが一致しているか
といったところを探します
これをマッチングと呼びます
そのマッチングした後に
変換するための行列
気化変換の行列を推定します
そして推定した変換行列を使って
画像の気化変換と合成を行います
ではこのイメージモザイクの
処理について説明します
まず最初に多様点を求めます
今3枚の画像A、画像B、画像Cがあるとします
それでは具体的に
イメージモザイクの処理手順を説明します
まず最初に多様点を求めます
多様点というものは
Aの画像の座標が
Bの画像のどの座標と
対応しているかという座標を
それぞれペアを求めます
これを4点求めます
4点求まると
4つの座標のペアから
3行3列の変換行列を
連想として解くことによって求めます
このHBAは
Bの座標系から
Aの座標系に
変換するための行列となります
この時気をつけなくちゃいけないのは
この対応点なんですが
この対応点は
同一3次元空間の中において
同一平面上にある点を与えます
これは3次元的な
この空間の位置合わせをするものではなくて
あくまでも画像から画像
すなわち2次元同士の
対応を求めるだけです
したがって
例えば床面と
それとは異なる平面である
この机のような点を
対応点として与えて
変換行列が求まるかというと
残念ながら求まりません
なのでこのような平面上の
同一平面上の対応点を与えて
その同一平面がどのように変換されるかという
機械変換できるかという
この行列を求めます
そして先ほどの
画像BからAへの
変換行列が求まりましたので
実際に画像Bを変換します
まず画像Aを置いておきます
この画像Aの座標形に
HBAという画像BからAへの
変換行列を使って
画像Bを変換すると
このような画像になります
そして先ほどの元々のAの画像と
Bの画像をAの座標形に変換した
この画像を合成して内装することで
このような一枚の画像になります
そしてじゃあ次どうするかというと
今度はこの変換した座標形
画像における点と
画像Cにおける同一の点の
対応点を与えます
ここではスライドでは
2点しか与えていませんけれども
実際に4点以上与えて
そして画像Cから
画像Aの座標形への
変換行列HCAを
連立方程式により求め
そして同じように一回変換をします
そうするとCの画像が
このように変換されました
そして先ほどの画像と
合わせることで
このように一枚の画像が
作ることができるようになるというわけです
この処理をイメージモザイというふうに呼びます
実際には先ほどは
対応点を手で与えましたけれども
この対応点を自動的に取るためには
特徴点というものを
牽引する必要があります
特徴点は何かというと
画像中の中で特徴的な点を言います
例えばコーナーみたいな点だとか
テクスチャーが非常に多いような所といったのを
自動的に見つけます
この特徴点を見つける方法には
Harrisのコーナー検出器だとか
あとはシフトだとか
サーフとかいろんな方法があります
この手法については
後ろの方の講義で紹介したいと思います
そのような検出した特徴点から
どの特徴点とどの特徴点が対応しているかを求める
対応点マッチングを施して
このように対応点を求めれば
その対応点から変換行列を求めて
気化変換をすることができるようになります
この特徴点を牽引する方法に
より優れた方法として
シフトというアルゴリズムがあります
これはスケールインバリアントフィーチャートランスフォームといいます
スケールというのは拡大縮小ですね
それに対してインバリアント
普遍という意味です
すなわちこの方法は
スケール変化に普遍な特徴変換器というものです
どういうものかといいますと
このような画像とこのような画像
画像と画像の間に拡大と回転が含まれています
こういうときどの点とどの点が対応しているかを調べるのは
非常に難しいのですが
このシフトという方法を使うと
特徴点ごとに大きさと方向を取り出してくれるので
その大きさを一定のサイズに
かつ方向を同じ方向に揃えてあげることで
この点とこの点は一致していますよということを
見つけることができるようになります
そうするとこのように
一枚の画像と拡大した回転した画像間においても
どの点とどの点が対応しているのかということを
マッチングすることがこのシフトによってできるようになります
今日はですね
画像の気化変換ということで
画像の気化変換における各処理手順と
その応用としてイメージモザイクについて説明しました
次回は画像から直線を牽引する
ハフ変換について紹介します
それではまた

みなさんこんにちは。今日は4回目です。今日はハフ変換について説明します。
まずですね、ハフ変換を説明する前に、僕はですね、中部大学で働く前、
赴任する前にいたカーネギーメロン大学での研究についてちょっと紹介したいと思います。
ご存知の人多いかもしれませんが、僕は皆さんと同じ中部大学を卒業しました。
中部大学で学部大学院の修士課程、そして博士講義課程を進んで、中部大学で博士号を取得し、
その後アメリカのピッツバグという町にあるカーネギーメロン大学という大学で研究員を3年間やってました。
ちょうど1997年から2000年です。今から20年くらい前ですね。
そのカーネギーメロン大学というのはロボット工学研究所というのがありまして、非常にロボットの研究では有名な研究所です。
ですから僕は中部大学を卒業した後ですね、ロボットで非常に有名な研究所でですね、3年間そのロボットに関する研究をすることができました。
そして今ですね、皆さんの前でロボットのビジョンの話をしているというわけです。
さあ、その時ですね、当時僕がやってた研究の一つにですね、ヒューマノイドビジョンプロジェクトというのがありました。
当時は1990年後半なんですけども、ヒューマノイドロボットがちょうど世の中に出てきたところです。
皆さん、ホンダが作るアシモ、ご存知ですか?
アシモの前にP2、P3というヒューマノイドロボットをホンダが研究開発をしていました。
それが最初に発表されたのがだいたい1997年ぐらいですね。
その頃に、ホンダが作った、世界で初めて作った2足歩行ができるヒューマノイドロボットに目の機能、視覚機能、ビジョン機能をつけようということで、
金木メロン大学のロボットコア研究所は視覚機能、ロボットビジョンの研究でも非常に有名な研究所でしたので、
そこでホンダとCMUで共同研究が始まったんです。
ちなみに僕はそのヒューマノイドビジョンプロジェクトの研究を担当していました。
今からお見せする動画は、当時研究をやっていた内容を紹介しているビデオになります。
ヒューマノイドロボットを使って、ヒューマノイドロボットが階段だとかドアを認識するような研究をやっていました。
残念ながら当時は輸出規制があって、ロボットをアメリカに送ることができなかったので、
実際のヒューマノイドロボットで動かすことができなかったんですけども、
そのヒューマノイドロボットを見立てて、カメラを実は3台つけて、
そのカメラから取得した画像からドアを認識したり、さらにドアが開いてる、開いてないを認識します。
例えばこのシーンでは赤色のドアは閉めてあるというわけです。
そしてこっちの方では複数のドアを認識していることがわかります。
ドアを認識するだけではなくて、ドアが我々が住んでいる3次元空間の中でどこにあるかということも理解しています。
これは次は階段の例です。階段の近くまで近づくと階段を認識するようになります。
例えば、ちなみに今ドアを開けていただいたのは本田から来ていたお定田さんです。
このように階段の形状を認識して、ロボットから見てどれぐらいの位置にどの大きさの階段があるかということをちゃんと理解できるわけです。
そうすることによってようやくヒューマノイドロボットがこのような階段を歩いていくことができるようになるというわけです。
この処理、僕は先ほどの研究の中で画像からドアを認識するというところを研究を担当していました。
当時やってた処理を簡単に紹介します。どういうことをやっているかというと、まずロボットに積んであるカメラから画像を取得します。
そしてその画像から今この講義で学んでいる画像処理の技術を使って、この場合はまず最初に地を抽出します。
エッジを抽出した後に今度は直線を検出してあげます。この直線はもうたくさんあります。直線の数だけ言うと500ぐらいから1000ぐらいの線が検出されます。
そしてそこからこの後にパターン認識、画像分類という技術を使ってドアらしい形状の組み合わせとなる線分だけを取り出すようなことをしていきます。
そうするとこの画像からどこにドアがあるかということをロボットは認識することができたというわけです。
さあ、じゃあこれでこのヒューマノイドロボットはここにあるドアのところまで歩いていくことができるのでしょうか。残念ながらできません。
今このドアを認識したというのは、この二次元画像上でどこにあるかということを理解しただけです。
残念ながらロボットは二次元座標からそのまま歩くことはできません。我々が今いるこの空間は三次元空間です。
ですのでロボットが立っている位置、ここからドアがどのような先にどれぐらい先にどこにあるのかということを向きも合わせて知ることができないとロボットはその中を歩いてアプローチすることができない。
この例ですと、二次元の画像から2枚のカメラ、先ほどの動画の中でカメラが複数だったのがわかるかと思います。
人間も目が左目、右目、左目であるようにステローシーをします。
ステローシーの技術によってこのような認識結果、ドアがそれぞれのカメラでどこにあるかわかれば、
結果、ロボットから見て1メートル、2メートル、3メートル、そして左に1メートル行ったところにドアがこのようにありますよということを理解することができるようになるというわけです。
さあ、今日はここです。画像からこの直線を検出するというアルゴリズムについて説明します。
さあ、皆さんご存知のように直線の式は y イコール ax プラス b です。この式の a が傾きを表していて、
b が y 軸の切片を表します。今、横軸が x、縦軸が y で、この直線式はこのような線です。
この時、角ですけど、傾きが a、切片が b ということになるわけです。さあ、この直線式をですね、軸が今は x と y という空間なわけなんですけども、
この式の中に出てくる a と b これをパラメータと言います。その a という軸と b という軸で空間を張ります。
これを a b パラメータ空間と呼びます。この x y 画像空間における直線 l は a b パラメータ空間においては一つの点になるということがわかります。
当然そうですよね。2の直線を決定するためには a と b はある値になっているわけです。
なので、くどいですが、この直線を a b パラメータ空間で表現すると、ある a と b という値の点で表現されるということになります。
さあ、では画像中の直線は実際はデジタル画像であれば点が並んでいるだけです。そうですね、エッジの点がたくさん並んでいるわけです。
なので、観測するものは x y 画像空間においてはエッジ上のある点になるわけです。ここではこの点を x 1 y 1 とします。
ではこの x 1 y 1 という画像空間、x y 画像空間における点を先ほどの y イコール a x プラス b の y と x にそれぞれ x 1 と y 1 を代入します。
そうすると今度はこの点の値を代入すると b i イコールマイナス x i a プラス y j という式が導出されます。
こちらの式は、例えばここが x がそうですね、例えば 3y が 5 だったとすると x がここが 3y が 5 ですから b イコールマイナス 3a プラス 5 というような式になるわけです。
これは見ていただいてわかるように、今度は a と b という a と b というパラメータ空間で直線になるというわけです。
当たり前ですよね。先ほどは x y 画像空間で直線だったものはその直線を表す a b が a b パラメータ空間で1点です。
今度今ここで説明しているのは、そもそもこの画像空間はデジタル画像なので点の集まり、画素の集まりです。
なので直線上に乗っている点を、1点を a b パラメータ空間に写像してあげると、今度は直線になりますよというわけです。
じゃあ次もう1点追加します。消しておきますね。じゃあ次は2点目が観測されました。
2点目の直線上の点 x 2 y 2 が観測されましたので、この点を a b パラメータ空間に写像します。
そうすると新たなもう一つ直線が観測できます。さらにもう1点 x 3 y 3 という点を同じように a b パラメータ空間に写像します。
そうするとまたもう一つ新たな直線が引かれるわけです。さあこの時こちらの a b パラメータ空間を見てみるとわかるように
交点があります。この交点というものは何かというと x 1 y 1 x 2 y 2 x 3 y 3 は同一直線上に乗っています。
ということはこの同一直線を表現する a と b のところでこの直線は交わるということになる。
すなわち画像上で点を観測できれば、この点を a と b のパラメータ空間に写像して、写像した空間における交点を求めることで元の直線のパラメータである a と b を知ることができるわけです。
さあここまでは数学の世界です。残念ながらこの今我々が使うデジタル画像では簡単にはうまくいきません。
前回もお話ししたかもしれませんが、この画像処理というのはいろんなアイディアが詰まってできています。
特にこの幅変換も数学だけでは解けないので、そこをどうやって工学的センス、エンジニアリング的な要素、エンジニアリング的な解決でうまく実現するかというところが非常に面白いところかと思います。
さあではデジタル画像ですとどういうことになるかということなんですが、本来この直線上にこのエッジが表現されて各点がですね、存在していればいいんですけども、黒いですけども、この x と y は連続的な空間ではありません。
理算的な整数ですよね。 なっています。なのでぴったりこの直線上にこの点が観測、そもそもされないわけです。
そうなるとどうなるかというと、それぞれの点を a、b パラメータ空間に写像すると、残念ながらこのように1点にさらわりません。
すなわち連立方程式で解けないということになるわけです。 さあじゃあそこでどうしようかということです。そこで考えた方法が1点に混ざらないけど、よく見るとそれなりに近いところに集まっている感は集まっているというのがわかります。
そこでこの a、b パラメータ空間をこのようにセルというものに分割します。 a と b は本来は連続値なんですけども、この a と b のパラメータ空間も
理算化しようというわけです。そしてこのセルというところに通過したところに投票をしていきます。例えば1つ目の直線に通過したところに正の字を書いていきます。
ここ通過して、ここ通過して、ここ通過、通過、通過、ここ、ここ、ここ。続けてこの2つ目の直線を投票していきます。
ここは同じところですが2票目が入りました。
こんな感じですね。次3票目投票していきます。3つ目の直線ですね。
という感じになります。これよく見るとここだけ投票数が3票となっているのがわかると思います。
すなわちこの紫で表示したここのセルですね。ここのセルはこの3つの直線がそのセル上を通過したということになるわけです。
すなわちこのセルに対応した a と b をこの直線とパラメータとして求めることになるわけです。
それではこのハフ変換をプログラムで実現するときのことを考えてみたいと思います。
先ほど説明したように、実際の画像中の座標は直線上にありません。そのためコーテが1位に定まらないので、どうするかというと、
a b パラメータ空間をセルに分割して直線上のセルに投票することをします。
このパラメータ空間に分割して投票するという処理は、2次元配列をプログラムで作ろうとすると、
2次元配列を用意して直線が通過する要素のところをインクリメントしてあげればプログラムとしてできるわけです。
c 言語のプログラムを作るときに最初にやることは、この配列の大きさを決めないといけません。
すなわち直線のパラメータである a と b がいくつからいくつの値の範囲を取り得るかということを考えて、
それを2次元配列の大きさとしてする必要があるわけです。
さあ、じゃあ y コード x プラス b の a と b の取り得る範囲を考えてみましょう。
直線はいろんな傾きがあります。切片もいろんなところがあります。
そうなんです。 a の取り得る値はマイナス無限大からプラス無限大。
同じように b も、b は画像の縦幅の範囲内ということが多いんですが、
実際に取り得る範囲としてはマイナス無限大からプラス無限大まで取り得る可能性があるわけです。
ということは、このような2次元配列を作ることができるかということです。
これはできませんね。ということで、残念ながら a b パラメータ空間をセルに分割して、
そのセルを直線上のセルに投票していくという方法はプログラムで実現できないということがわかります。
じゃあ、そこでどうすればいいかというと、この範囲がある程度の大きさの中に収まるようになればいいわけです。
すなわちパラメータ空間の大きさを限定してあげましょうということで、
そこでこの下の式、ここで出てきた row イコール x コサインシータープラス y サインシーターという式を使って投票を行います。
これどういう意味かというと、row は原点から直線までのこの距離を表します。
シーターはこの角度を表します。それで実際に波幅変換をしてみます。
先ほど直線上にある点が観測されました。
その各点 x i y i の座標を先ほどの row イコール x コサインシーター y サインシーターという式の x と y に代入します。
そうするとこの式は横軸がシーター、縦軸が row になります。
シーターを 0 から 2π まで変化したときにこの式を計算した結果、
row の値がこうなって角度ができる。
すなわち一つの直線に対してこの1個のサインカーブが、この2つのやつに対しては2つ目が、
そしてこの3つ目には3つ目のそれぞれのこの正弦波が表現できます。
そしてこの時この直線上に乗っている点であれば、
ロー、シーター、パラメータ空間においても交点が観測され、
その交点となるローハット、シーターハットがその直線の同一線上に乗っているということになります。
このローとシーターを求めてから直線式に変換してあげることで、
直線のパラメータである a と b を知ることができます。
もちろんこのロー、シーター、パラメータ空間においても一点にぴったり定まりません。
これはなぜかというと、xy 画像空間が理算的な空間、整数座標形だからですね。
なのでこの場合も同様に下に書いてあるように投票空間をセルに区切って投票します。
この場合、ローとシーター、シーターは2π、実際はここはπまででokです。
π以降、180°以降は反転しているだけなので、交点は1点だけ見つければいいのでπまでで大丈夫です。
じゃあローに関して考えると、ローはxcosθ、ysinθですから、
もし画像が640×480とすると、最大値はこのxの640cosθが0度のときは1になって640になりますね。
なので、すなわちこのローの最大値は画像の大きさによって決定することができる。
すなわち、ローとシーターともにある範囲内の値にちゃんと収まっていますから、
これを二次元配列として表現して投票していくことができるようになる。
もう一度では、ハフ変換の直線検出の流れについて説明します。
まず最初、一番です。画像から直線の候補座標となるエッジを検出します。
このエッジを検出する方法には、ソーベルフィルターなどを使います。
そして、各検出したエッジの点の座標について、傾きシーターにおける距離ローをこの式で算出します。
ここでは、xiとyiを代入します。ある座標というのはxyiを表します。
そして傾くシーターに対する距離ローに対するセルに投票していく。
この処理ですね。この2番から3番という処理をすべてのエッジの点に対して2と3を繰り返します。
ただし、エッジの強さが非常に小さいところは、そもそも直線である可能性が少ないので、
そういった点は投票に使いません。そして2と3を全画像中で投票した後、
投票が一定数以上あった傾きのシーターとローを求めます。
そしてそのシーターとローから、今度はxとyにこの式を使って逆変換すれば、直線のパラメータとなるというわけです。
ではですね、実際に処理例を見ていきましょう。
この画像わかりますか?これはiPodです。初期のiPodです。これはちなみに僕の私物です。
ちなみにこのiPodはですね、初期型、一番最初のものなので、
なんと中身はですね、SSD、メモリーではなくてハードディスクです。
物理的にくるくる回るハードディスクで入っていた時代のiPodです。
このiPodも脱線していますけども、iPodは非常に画期的な製品で、
今それまではですね、音楽、例えば自分が持っているCDですよね。
CDの音楽をすべて外に持ち出すなんてできなかったわけです。
それまではCDプレイヤー、CDとCDプレイヤーを持って行っていくので、
家にある大量のですね、CD、音楽CDを外に持っていくことって普通はできなかったわけですね。
だけどこのiPodによって、すべて持っている、僕が持っている音楽をiPodに入れて、
それでどこでも聴けるっていうのは衝撃的な体験でした。
それぐらい非常に思い入れのある初期のiPodです。
それは良いとしても、今は普通ですけどね、iPhoneとかでも普通にできますけども、
さあ、それは置いておいて、半変換をやります。
まずこの入力画像に対して、ソーベルフィルターで字を抽出します。
そしてエッジの強度が大きいところだけを抜き出します。
こういうのを2値化と言います。
ある、例えば100以上の強度を持つエッジのところだけを取り出すと、
そのエッジの点だけを黒、それ以外を白にすると、
こういう2値化といってエッジの点だけを取り出すことができます。
さあ、ではこのエッジ上の点1点1点をローシータパラメータ空間に写像するわけです。
そうするとこの点はかなりたくさんありますよね。
なのでそれに合わせていろんな曲線が描けるわけです。
このローシータパラメータ空間を拡大してみると、
こういったセルに1個を区切ります。
例えばこれぐらいの大きさの

セルに区切って投票すると、投票数が多いところが
1、2、3、4、5、6、7、8、9、9点出てきます。
結果、この9点に対応した直線を元の画像で描くと
こういった直線になるわけです。
これ、複数の直線が観測されているのが分かると思いますが
まあでも、iPodのこの4つの直線を
ちゃんとそれぞれ1、2、3、4含むようなことができて
ただし、ここの線上も複数の直線が
ここ分かりますかね、これ複数の直線が
検出されていることが分かります。
これはなぜかというと、先ほどセルに区切ったときに
このような近いところに複数投票があるわけですね。
そこで、じゃあどうするかというと
セルを少し大きめにして投票をします。
そうすると1、2、3、4と4つが
投票が多いセルが観測されて
これを元の画像で直線を表現すると
このような1、2、3、4本だけになるというわけです。
ただし、残念ながらセルを大きくすると
どうなるかというと直線の当てはめが
少し精度が悪くなります。
そうですね、ぴったりと、こういうふうにぴったりと
本当はあってほしいんですけども
セルを大きくしているので
残念ながら直線の精度が悪くなるということがあります。
これは求める対象としている画像において
どのような直線を検出したいか
たくさんの直線を検出したいのか
少しぐらいパラメータが違うのも
別の直線として検出したいのか
そういう問題に合わせて
先ほどのセル空間をどれぐらいのサイズに区切るか
ということを設定してください。
はい、ではですね、ここで皆さんに質問です。
今までは直線をどのように検出するか
といったことを説明しました。
ではこのような画像から円を検出しましょう。
どうすれば円が検出できるか考えてみてください。
では今から一旦動画を一時停止して考えて
その考えたアイデア
こういうふうにできるだろうというのを
ここにですね、記入しておいてください。
はい、では円をどうやって検出するか続いて説明します。
円、先ほど直線を検出するときには
まず直線の式を考えました。
同じように円の式を考えてみましょう。
円の式はここにあるように
x-aの次乗プラスy-bの次乗イコールrの次乗になります。
よく知っているかと思いますが
aは中心座標のx座標
bは中心座標の円の中心のy座標を表します。
そしてrは半径を表します。
ということはこのような画像空間において
円上の点が4つもし観測されたとしますね。
その時にではこの1つの点に対して
同様にこの場合パラメータは
aとbとrとあります。
この3つですね。3つありますので
本来はこのパラメータ空間は
aとbとrなんですけれども
ここでは一旦rを10と固定にします。
そうした状態でaとbとパラメータ空間において
この点を斜像します。
この場合1個の点がaに与えられると
その点半径、この場合rは10というふうに設定していますので
この点を中心とした半径10のところが
こういった斜像した円になるわけです。
点が円になるわけです。
この点を中心に半径10で描いた円です。
今度は2つ目の点を中心に描いた円。
3つ目の点を中心に描いた円。
4つ目の点を中心に描いた円になります。
この時見てわかるようにここですね。
同じ半径が10
もし元々観測している円が10であれば
ここが1つ交点として交わるわけです。
すなわちこの交点の位置が半径のパラメータ
ごめんなさい、円のパラメータであるaとbを表すということになります。
さあ画像を入力して
実際にどうやって円を検出するかという処理の流れです。
まず画像を入力します。
この画像をよく見てみると
ちなみにこれは1セント
これは10セントかな
この1セントとこの10セントこれも僕の私物です。
日本円にすると1円と10円ぐらいですか。
これよく見るとここを見てください。
この1セント、これ欠けてますよね。
綺麗な円じゃありません。
日課した例を見るとよくわかりますね。
このような状態でちゃんと円が検出できるかということを試してみます。
まずソーベルフィルターをかけてエッジを抽出して
エッジの強度の強いところだけを抜き出しました。
そして投票していきます。
これがまず投票する前のエッジの各点です。
各点を中心に円を描いていくわけですね。
まず半径を二重として投票した結果です。
こんな感じですね。
まだ1点に定まっていません。
では次半径を大きくしてまた投票します。
今度半径が30です。
まだまだですね。
今度は半径を40です。
これもまだまだであるというのはわかります。
続いて半径が54。
実際は半径は細かく変えていきます。
ここは半径を54としました。
そうするとこちら側を見ていただくとわかるように
1点に投票が集中していることがわかります。
続いて半径を大きくします。
半径は60です。
60にすると今度はこちらが1点に集まりました。
という形で今度は半径70。
どんどんどんどん大きくしていきます。
そうすると1点に集まっていたのがまた離れていってしまいます。
半径80です。
先ほどの各半径ごとに最大投票数をプロットすると
こんな感じになります。
これを見るとわかるようにピークというのは
こういうふうに大きく尖ったところですね。
そこを調べるとまず1つ目のピークがここ。
2つ目のピークがここになります。
1つ目のピークは54。
ちなみにここの値が54でしたので
この54というところですね。
ここですね。
ここが一番最大投票数が多かった座標になりますので
この点を中心に半径54の円を描くと
こんな感じになるわけです。
こちらの円の中心と半径がちゃんと合っていることがわかります。
しかも欠けたところがあっても大丈夫です。
なぜかというと他の円周上の点がちょうどぴったり合っていれば
投票が1点に集まることです。
もちろん欠けたところの投票は違う中心に投票してしまいますが
より多くの円周上の点があれば
投票によってちゃんと求めることができるというわけです。
じゃあもう1点。
こちらの半径60の方ですね。
ここの座標を中心に半径60の円を描くとこんな感じになります。
こちらもちゃんと中心座標と
それがちゃんと合った半径60というのを
計算することが求めることができたというわけです。
このように円圏位数のパラメータは
円圏位数におけるパラメータはA、B、R
直線の場合はAとBだけでした。
パラメータ数は増えていますが
実は円の方が比較的楽ですね。
なぜかというとAとBとRが取りうる値というのは
必ず画像の大きさの中に入っています。
そうですよね。
画像の中心は必ず画像の中に見えているとして
半径が画像よりも大きいとはみ出ちゃうから
そもそも円が観測されません。
なので円を検出する場合は
実はA、B、Rというパラメータが
ある範囲の中に収まっているので
直線の時のようにローシーターといった
異なるパラメータ空間を使わなくても
直接うまくいくというわけです。
これが円圏位数の仕組みです。
まず、ハフ変換として
直線の圏位数と円の圏位数について説明しました。

はい、では続いて画像のフーリエ変換について説明します。
そもそもフーリエ変換はデジタル信号処理の中でも出てきたものだと思います。
フーリエ変換はいろんなところで使われます。
もちろん画像処理においてもこのフーリエ変換を使うことがあります。
まずどういったところで使うかということなんですけども、
フーリエ変換することによってできることは、
周波数成分、その画像の中に含まれる周波数成分を
解析することができます。
したがっていろんなテクスチャーを解析するのに使われます。
例えば、木目みたいなテクスチャーであるだとか、
タイル状のテクスチャーであるだとか、
いろんなこういったテクスチャーの違いを見つけるときに
周波数成分を使って観測することがあります。
では画像ですね。どのように周波数成分、
画像から周波数成分を獲得するかということなんですが、
まずここでは最初から2次元で考えるのではなくて、
1次元の信号におけるフーリエ変換を説明します。
フーリエ変換を説明する前に、
まずフーリエ変換の原理として、
2位の周期関数は三角関数の和で表されるということを説明します。
どういうことかというと、最終的にこのような波形があったとします。
このような波形は、まず1周期における1Hzにおけるこのような三角関数と、
次は1周期、2周期、3周期、
大体3周期強の制限波と、
1周期、2周期、3周期、4周期、5周期弱の制限波と、
さらには1、2、3、4、5、6、7、7周期の7Hzの三角関数があります。
ここで見ておくべきものはこの高さです。
この高さがここの高さと一致しています。
続いてここの幅がここの幅、ここの幅がこの幅、この幅がここの幅になるというわけです。
この波形とこの波形を組み合わせたものがこのような波形になって、
さらにこの波形にこの周波数を組み合わせるとこのような波形になって、
この波形にこの制限波を組み合わせるとこのような波形になります。
要はこの波形というものは周波数、この1の振幅がこの幅の制限波の成分と、
この周波数のこの高さの成分と、
この周波数のこの高さの成分と、
この周波数のこの高さの成分の制限波が和としてこのような複雑な波形が表現できているわけです。
なので2の周期関数、必ず周期性を持たないといけませんが、
周期性のある関数であればこのように三角関数の和として表現できる。
この時各周波数ごとのその成分の大きさといったものを
縦軸に振幅として表現したものがいわゆるスペクトルといわれるものです。
フリエ変換はこのような波形が観測された時に、
このような波形の中に各三角関数の成分がどれぐらい入っているかということを求めることになるわけです。
ではこれを式で書きます。
フリエ変換の原理として周期関数をFTという式で書くとこのようになります。
これは何かと言いますと、まずA0と言います。
A0はここにもあるように直流を表します。直流性です。
その後、A1、A2、A3といった係数がかかった
コサイン4πt、コサイン6πtということで周波数が2πtに対して2倍、3倍という風に変わっていくコサイン波が
それぞれの係数A1、A2、A3、A4という風に続くというわけです。
すなわち周期t分の1の整数倍、2に対してこれ2倍してますよね。
今度これに対して3倍になってます。ということで4、5、6という風に続くというわけです。
これがコサイン成分の成分を表していて、もう一方B1、B2、B3というのがあります。
これもよく見るとわかるようにサイン波、サイン成分でこの周期t分の1のものに対して
同じようにここは2πtですから2倍、ここは3倍、ここも同じように4、5、6という風に続くということになるわけです。
このように2の周期関数はこのような式で表現できますよということを言っているわけです。
このA1からたくさんあるAnとB1からだーっとあるBnをちょっと隠れてしまってますが、
風利恵係数というわけです。風利恵変換をやることは何かというと、
このAからAn、Bnという風利恵係数を元の入力された波形の中から風利恵係数を求めることが風利恵変換のやることになる。
なのでそれぞれ直流成分A0、コサイン波の成分An、サイン波の成分Bn、風利恵係数を求めましょうということをします。
これはどうやって求めるかというと直流成分は単純ですね。
入力されたftという信号を0からtまで積分して、積分ということは面積を求めるものですから、それをtで割ってあげれば平均、すなわち直流成分を計算することができます。
一方、こちらのAとBですけれども、Aに関してはA1の場合は、もしA1を求めるということであれば、
A1の場合はt分の1、0からtのftのコサイン2πt分のt、dtとなります。
これはこのftとコサイン2πtですね、この波形ですよね。
この波形とこのftの相関を求めていることです。
こことここの相関を求めていることになります。
この相関というのは何を求めているかというと、似ていれば似ているほど値が大きくなる。
なので、もしこのftに含まれている成分の中に1Hzのコサイン波が入っていれば、この掛け算した値は非常に大きな値になるという。
これでこのAの値を1から次に2にして、ここが4πt、ここを3にすると6πtということで、
元の波形とその周波数のコサイン波との相関を見て、その相関がどれくらい大きいかということがそれぞれのAnを計算することになります。
同様にBnはサイン波で、周波数1、周波数2、周波数3のサイン波との同様に相関を計算して、相関が高いということは、
そのft元の波形の中にその周波数成分のコサイン波、サイン波が多く含まれているということを表しているという。
これが風流変化になります。
ぜひプログラムを作ってみるとわかりやすいと思います。
このプログラムはまずこのような入力信号を作りました。これがftです。
このftを入力したときに出てきた風形係数がAnとPnになります。
そもそもこの入力はどうやって計算しているかというと、
これはわざと小さくて見づらいかもしれませんが、皆さんはpdfの方でちょっと見ておいてください。
これ見ていただくとわかるように、周波数1と2と4の波形を組み合わせて入れています。
なのでこのAn、Bnを見ると、しかもコサイン成分だけですから、
ここのサイン波のところを見てみるとほとんどゼロだということがわかります。
一方、コサイン波を見ると、ここの1のところと2のところと4のところに非常に大きな値になっていると思います。
係数を見てみるとわかるように、1、2、4という係数が変わっていますから、
8、16、32ですから、ちょうど2倍、4倍という風に振幅が変わっているのが確認できます。
同じこのような波形を生成して、それをここの風理変換によって求めたところ、
AnとBnにはこういう値が出力されたという風理変換の原理ということになります。
画像における風理変換なんですけども、非常に単純で、先ほどの風理変換を画像のx方向とy方向に合わせてやることが
二次元風理変換ということになります。なので画像はxとyがあります。
この画像空間を二次元風理変換すると、あるUとVの周波数、横方向の周波数成分と縦方向の周波数成分における
UとVにおける周波数領域の成分として表現できます。
ちなみに風理変換は逆変換ができます。
当然、二次元風理変換においても、この周波数成分から元の二次元風理へ逆変換をすることによって、
元の画像空間に戻すことができます。
では実際に二次元風理変換の例を見てみましょう。
この上の画像は顔のいろんなパーツが含まれています。
特に髪の毛ですね。細かい成分が入っているのがわかります。
細かい成分とはどういうことかというと、
例えば横方向に見ると強弱、振幅が激しいわけですよね。激しく変化している。
ということは周波数成分が比較的高いものが含まれているということがわかります。
実際にこの辺りですかね。
比較的高い成分が白く映っているということは、高い周波数成分が含まれていますよということがわかります。
一方、下の例を見てみますと、この下の例はかなりぼけた状態のものです。
周波数成分が高いということは、急激な変動が起こっているものが含まれているということですが、
この場合は平滑化されているような状態に近いわけですから、急激に変動があまりありません。
なので上の画像と比べると、周波数成分を見ると、
より低い周波数、これ真ん中に行けば行くほど0,0、低周波になりますから、
低い領域にですね、上の方は広く分布しているに対して、下の画像は狭い範囲にしか周波数成分がない。
すなわち低い周波数成分だけであるということが、このフーリエ変換した画像の結果から、
この2つの画像の性質を分けるというか、知ることができるわけです。
では他の画像を見てみましょう。これは指紋画像です。
ちなみに僕はですね、大学4年生の時に、この指紋画像の称号の研究をやっていました。
この指紋画像は非常に面白いんですね。
この指紋画像を2次元フーリエ変換すると、このようなリング状の周波数成分が観測できます。
なぜか、横方向に見ても白黒白黒とかパターンが変わりますよね。
斜め方向に見ても同じように白黒白黒とかパターンが変わります。
縦方向に見てもまた白黒白黒とかパターンが変わります。
すなわち、ある一定の周波数のところに、いろんな向きの、
すべての方向に対してある一定の周波数領域のところに、
この辺ですね、こういったある一定のところに、
この指紋の情報が載っているということがわかりますね。
なので、このフーリエ変換のパターンを見ることによって、
元の原画像がどのような構造を持っているのかということも比較的知ることができます。
下の例は、絨毯の例ですけども、これは細かいパターンが含まれていますので、
このようなパターンになります。
さあ、これは木目のパターンです。
この木目のパターン、まず上を見てみましょう。
上は横方向に見ると木目が縦に発生していますので、
かなり変動が激しくなるのがわかります。
すなわち、横方向に対しては広い範囲に対して、
低い周波数から高い周波数まで成分が含まれていますよというわけです。
一方、この画像を90度回転しました。
90度回転した画像の二次元フーリエ変換をすると、周波数成分もこのように変わります。
横方向に見ていくと、あまり色の変化がありませんね。
だから横方向には周波数成分が、
低周波はありますけど、高い周波数成分はないということがわかります。
今度は縦に見てみましょう。
縦に見ると、先ほどの横に見た時と同じようなことになりますから、
変動が激しくなります。
明るく暗くなったり、明るく暗くなったりします。
なので、その成分がこういうふうに出てきているというわけです。
なので、この木目の向きに合わせて周波数成分がどういうふうに表現されているかというと、
向きに合わせてフーリエ変換のパターンもこのように変わってくるということがわかります。
今度は画像を一旦フーリエ変換して、
その周波数成分である成分をなくした状態をまた使って、
今度は逆変換して元の画像に変換するとどうなるかということをやってみたいと思います。
まずこの入力画像を2次元フーリエ変換するとこのような周波数成分が得られます。
ここで真ん中のカットオフ10以上のところをカットしました。
ここだけを残して、すなわち低周波成分だけを残して、
もう一回逆フーリエ変換してあげるとこんな画像に変わります。
当然ながら高い周波数成分がなくなっちゃいます。
先ほど多分この辺にいろいろありましたね。
こういった高周波の成分がなくなってしまうので、
この領域だけを使って復元した画像はこのように受けたような画像になってしまうというわけです。
続いて今度はカットオフする周波数をちょっと広くしました。
すなわちこれぐらいまで大きくしました。
そうするとやっぱり先ほどより少し高い周波数成分が含まれるようになっていますので、
それに合わせて顔の画像も少し細かいパターンが見えるようになってきたのがわかるかと思います。
続けて今度カットオフを30にします。これぐらいです。
かなり多く含まれるようにしました。
そうするとほとんど見た目があまり変わらないということがわかります。
すなわちこの顔画像の周波数成分は30以下の中に含まれているということがわかるわけです。
次は40です。40にするともうほとんどこのような形で変化がありません。
こうやって風利変換をした結果、周波数領域である成分をなくしてあげて、
それを逆風利変換することでいろんな画像の変換ができるというわけです。
さあでは今度は逆をやってみましょう。
先ほどは低い周波数成分を残して高い周波数成分をカットしました。
今度は逆です。低い周波数成分をカットして高い周波数成分だけでどうなるかということで
ハイパスフィルターをやってみたいと思います。
指紋画像を2次元風利変換すると先ほども説明したように
ある周波数のところにリング状にこういう周波数成分を観測することができます。
ここで低周波のこの部分をカットします。ここをカットしちゃいました。
そうするとこちらの逆風利変換した画像はどのようになると思いますか。
一度想像してみましょう。この真ん中は直流成分でした。
ちなみに画像は0から255の値ですから正の値しかありません。
正の値があるしかないということは確実に直流成分が存在するという。
したがってこの真ん中の直流成分をなくして逆風利変換すると
こんな感じで色がドーンと落ちてしまいます。
直流成分がないから値が下がってこういう画像になります。
だけど指紋自身のこのリング状の周波数成分が残っていますから
この画像を見てわかるように指紋のこの凹凸の成分は
ちゃんと見て取れることがわかります。
ではカットオフ周波数を少し上げましょう。
少しこれぐらい上げました。
これを上げてもまだ指紋の領域のリング状の分布が残っていますので
逆変換した場合はこんな感じで指紋のパターンがちゃんと見て読み取れます。
さらにカットオフ周波数を上げます。半分切れちゃいました。
そうするとだんだんちょっとパターンがこのように崩れてきます。
指紋のパターンが見づらくなってきました。
さらにもっとカットオフを上げるとほとんど指紋のリング状の領域がなくなってしまいましたので
それで逆風里変換するともちろん指紋のパターンが見えなくなるというわけです。
このように風里変換、二次元風里変換して
その周波数成分がどういうふうに含まれているかだけでなく
その周波数成分で必要なところ、不必要なところをカットして戻してあげることによって
逆風里変換することによっていろんな効果を得ることができるわけです。
ではこの二次元画像風里変換を試してみましょう。
ここにURLのところにデモンストレーションが公開されています。
非常に面白いデモになっていますのでぜひ試してみてください。
まず画像を選択します。選択した画像からクリックをして
クリックをした周波数成分がこのようなパターンで
これらの全てのこの白の領域のみの周波数成分を使って戻したパターンがこの画像になります。
すなわちこの顔の周辺の領域はこの辺の周波数に含まれているということがわかります。
もう一度今度は違うことをやってみましょう。
今度は高周波の成分だけをパターンを超えて作って
その逆風里変換した数を作ってみます。
この場合いつまで経っても顔画像がここに出てきませんね。
これなぜかというとこの復元している周りのところの高周波成分には
そもそもこの顔の周波成分が含まれていないから
復元した逆風里変換した画像の中に顔が現れないというわけです。
ただし細かいパターンが復元できているのがわかるかと思います。
ぜひこのサイトに行って試してみてください。
では今日はこれで終わります。それではまた。

皆さんこんにちは。では今日は日課について説明をします。
まず日課を説明する前に、これまで扱ってきたグレースケール画像を
そもそもカラー画像からグレースケールにどうやって変換するかという例を紹介します。
そしてその後、そのグレースケールの画像をどのように日の値に変換する話を説明します。
まずRGBカラーなんですけども、そもそもRGBカラーというものは赤とグリーンとブルーで表現する
RGBの3つの空間で表現されたカラー空間ということになります。
当然この元のところが0、0、0で黒を表して、この一番端のところが255、255、255で白を表しています。
このような空間をRGBカラー空間というふうに呼びます。
このRGBカラー空間はこの値が小さいほど白で、上に行くほど白になっているわけなんですけども、
例えば赤み全体的に色の感じを変える、好ましい色の雰囲気に変えるということがなかなか難しい値です。
それに対してHSIという変換方法があります。
このHSIのHは色相であるヒュー、Sはサチュレーションであるサイド、Iはインテンシティである明度を表します。
このHSI変換をRGBカラーからHSIに一旦変換します。
まずわかりやすいIから説明します。
Iはインテンシティで、このHSIの中ではこの縦軸を表します。
一番下が黒で、上に行くほど白になります。
真ん中のあたりはグレーになるわけです。
そこである例えばグレーのところで区切った断面がこのような断面になります。
この断面はIイコール0.5、すなわちグレーの時の断面になります。
この断面の中はこの断面はこのように円周状になっています。
ということは0から始まって2π周期、ぐるぐるぐるぐる回るような色になっています。
これをこのシーターをHUEと言います。
HUEは色相です。
例えばHが0の時は赤、90度の時は紫というふうにシーターに対応する値、Hが色に対応します。
さあもう一つサイドです。
このサイドという方法は何かというと、色の鮮やかさみたいなのを、色がどれくらいついているかということを表します。
例えばこのIイコール0.5における断面の真ん中はグレーですから色がありません。
この真ん中から外に行けば行くほど色が鮮やかについていくということがわかります。
これが中心からの距離がサチュレーションであるSになるわけです。
なので例えば今画像のカラーの値がこの中心に寄っているような値しかない場合は
Sの値を全体的に外に出してあげれば、大きくしてあげれば色鮮やかなカラー画像に変換することができます。
RGBカラー空間では鮮やかにするということがなかなかできません。
なんとなくRもGもBも値をですね大きくしてあげればいいんじゃないか。
RGBカラー空間にすれば大きくすればいいんじゃないかというふうに思うかもしれませんが、
値を大きくすればそれはここから白の方向に向かうわけですから、全体的に画像が白っぽくなっちゃうんです。
なので一度RGBカラーをHSIに変換して、例えばサイドのSを外に大きくなるように広げて元の画像を見ると、
より前より色彩豊かな鮮やかな画像に変わったりすることができます。
このような式、こちらに書いている式を使ってRGBの値からそれぞれHSIに変形します。
ここではちょっと細かい説明は省略します。
このHSIのIに注目すると、Iは黒から白のグレーですから、
まさにカラー画像をグレースケールの画像に変換した値と変換したものとして扱うことができるわけです。
これがHSI変換におけるIntensity Iを扱うアプローチです。
他にはどういう方法があるかと言いますと、テレビの企画で使われているPAL企画というものでは、
軌道信号と色差信号にRGBの値を変換しています。
軌道信号のことをY、色差信号をUとVと言います。
これはRGBのカラーの値をまず軌道であるYに変換するときに、
Rには0.299という係数を、グリーンには0.587、ブルーには0.114という係数をかけます。
この係数の値はどうやって決められたものなのでしょうか。
皆さん、色を見たときに、その色が持つ明るさという印象があると思います。
例えば、緑と青を比べてみましょう。
緑と青を比べた場合、なんとなくやっぱり青の方が暗いイメージがあると思います。
緑の方が明るいんじゃないかというのがわかると思います。
例えば赤と緑でも比べてみると、やっぱり赤の方が少し暗い印象になると思います。
なので、我々人間がカラーの明るさをどのように感じているか、その度合いをこのように数値で表現した。
結果、明るさに表すためにはグリーンの成分が大きく、
次にレッド、そしてブルーがこういう係数になったというわけです。
ちなみに、このUとVは色差信号と言いまして、色と色の差をとります。
差をとって表現しているものはUVという方法です。
このYUVを使ってカラーを表現しているというのは、テレビで使われているパル規格というものです。
ちなみに、このUとVはなぜ差をとるかということなんですが、
RGBで表現したときは、それぞれRGBが最大の値までとります。
YUVに変換するとYは最大の値をとりますが、UとVは差をとっている分、値が少し小さくなりますので、
そういった効果があるというわけです。
グレースケール画像にはこのYUVのYが使われたりします。
では実際に画像の例を見てみましょう。
これはカラー画像です。
このカラー画像をHSI変換のIだけにして、要は彩度を0、S、サチュレーションを0にして表示すると、このような画像になります。
結構グレースケールになっていると思いますが、ちょっとこの辺は違和感が感じるようなところもあるかもしれません。
では続いてRGBの成分を単純に足して3で割った平均を見てみましょう。
そうするとこのような画像になります。
割りかしRGBを足して3で割っても、比較的良さそうなグレースケール画像になっていることがわかると思います。
では続いて最後に紹介したYUVの軌道信号であるYです。
Yを採用して軌道信号をグレースケールで表現するとこのようなグレースケール画像になります。
このYUVを見るとかなりカラースケール、カラー画像の印象を残したままグレースケールにうまく変換できていることが確認できると思います。
ではそれぞれの方法がどういうふうに変換しているのかもう少し詳しく見てみましょう。
HSI空間における彩度を0にした場合はこのように色が変わっていたとしてもすべて同じ値になってしまいます。
なので色の違いによる明るさを彩度を0にした方法ではうまく表現できていないということがわかります。
次にRGBの値を足して3で割ったという平均を計算した方法です。
この場合は色が変わっていくにつれて明るさが変わっているので上の方法よりもいい方法であるというのが見て取れます。
しかし例えば赤色のところ、緑のところ、ブルーのところを見るとRとGとBの平均をとっていますので
RとGとBがそれぞれ同じ強い値であるところはここを見てもらってわかるように同じ値になってしまう。
すなわち赤、緑、青の色の明るさの差がこの平均値の計算方法では表現できていないということがわかります。
続いてYUVです。YUVのYはRに0.299、グリーンに0.587、ブルーに0.114という係数をかけていますので、
同じようにR、G、Bのところを見るとグレース系の色が変わっていることが確認できると思います。
このようにYUVのYを利用することでより綺麗なグレースケールに変換できることがわかります。
では3つの方法を並べて比較してみましょう。
このカラー画像に対してHSIのS、サチュレーションを0にしてインテンシティを表示するとこのようなグレースケール画像。
RGBの平均をとったグレースケール画像。そして最後にYUVに変換したYの軌道信号を表示したものです。
こうやって並べてみてみると、1番、2番、3番、3つのうちどれが元のカラースケールの画像をよりグレースケールに変換しているかということが比較できると思います。
見て通りこのYUVのYがより綺麗な元のカラー画像の明るさの印象を残したような形でグレースケールに変換できていると言えるでしょう。
これでグレースケールの画像に変換することができました。
続いては日課を行います。
日課というのはグレースケールの画像を白と黒の2種類の値にすることを日課というふうに呼びます。
ではこの日課をどういったところに使うかということなんですが、
皆さんよく知っている文字認識だとかQRコードの読み取りの前処理としてまず日課を行ったりして、
例えば買い物したときのレシートをカメラで撮影してそこから買ったものをテキストで起こしたりすることができるようなアプリがあります。
あとは名詞をカメラで撮影すると名詞上の文字だとか名前とか住所をテキストで起こしてくれるようなアプリがあります。
そういったアプリはまずグレースケールで得られた画像から日課を行って文字のところと背景を白と黒で日にして表現しているわけです。
そうするとこの文字のところだけを取り出してきて、
アルファベットB、O、T、Hというのを認識することができるようになるわけです。
なのでこの日課という処理は文字認識だとかQRコードも同じようにカメラで撮影した後はこういうグレースケールもしくはカラー画像です。
それをグレースケールに変換した後、QRコードの四角形がありますのでその四角形を認識するというところには日課が使われているというわけです。
さあ日課というものは今ではバイナライゼーションというふうに呼ばれます。
これはグレースケールの画像0から255の値を0と1の2つの値に変換をします。
式で書くとこのような式になります。
これはまずfxyが入力画像になります。
なのでこちらがF、こちらがGになります。
xとyで指定した座標の画素値fを入力したときにこれが式値t。
式値tよりも大きい画素値はすべて255にしますよというわけです。
入力した画素値がこの式値tよりも大きいところはすべて255の値にしますよというわけです。
それ以外のところはこのotherwiseというもので表現します。
それ以外のところは式値tよりも低いところはすべて0ですよというわけです。
さあこの日課を行うためにあらかじめ決めないといけないのがこの式値tになるわけです。
この式値をどのように設定するのかという方法にPタイル法、モード法、判別分析法という手法が提案あります。
では順番にこの方法を説明します。
まずPタイル法です。Pタイル法のPはパーセントです。
この名前を聞いてなんとなく想像できるかもしれませんが、どういう方法かというと、まずノータンヒストグラムを計算します。
ノータンヒストグラムのこの黒の面積は画像の面積サイズと一致しますね。
そこでこの画像値の低いところから頻度の累積を計算していって、もともとその画像中にどれぐらい何割のパーセンテージの物体が含まれているかということを決めておくことによって、
そのパーセンテージに対応する画像値を超えたときにその画像値を式値としましょうという方法です。
そうすると物体の画像値に含まれる割合をある0、それ以外を1とかというふうに設定しましょうというわけです。
例えば全画素数が300×150という画素数であれば、そこにおいてあらかじめその画像の中にどれぐらい物体の画素数が含まれているかということを予測しておきます。
180×100ぐらいだっていうことであれば比率は0.4になるので、0.4という値に対応した式値を採用しましょうというのがPタイル法になります。
続いてモード法です。モード法という方法は、そもそも日誌化をしたい画像は明るいところ、
例えば背景の紙の明るいところと文字の黒いところの2つに分かりやすいわけです。
すなわちこういう2つの山を持つような濃淡ヒストグラムの分布が得られることになります。
であれば山と山の谷を式値として採用すればいいんじゃないかという方法がこのモード法という方法になります。
この方法は概念としては非常にわかりやすいんですが、
実際に谷をどのように定義するかによってなかなか決定方法が実は難しかったりします。
きれいな谷ではなくてガタガタしたりしてますので、どこを谷として採用するかということが問題となったりします。
まずPタイル法における割合を変えたときに、どのように日誌化画像が変化するかを見てみましょう。
全体の画像の中の20%を物体領域として日誌化すると、式値が53というふうに計算されてこのような画像になります。
40%だと式値は171、60%だと198、80%だと式値が202となって、それぞれの日誌化した画像がこのように求めることができます。
この画像、そもそも元々の原画像が原画像中に何パーセント物体を含まれているか、なかなかわかりづらい画像になっています。
この4つを比較すると、なんとなくこの40%とした式値が一番好ましいのかなというふうに見ることがわかります。
Pタイル法においては、あらかじめ何パーセントであるということをしっかり理解し、わかっていないとうまく適用できないということになります。
そこで、事前知識を必要とせず、自動的に式値を決定するという方法が判別分析法という方法になります。
ちなみにこの判別分析法は、別の名をオーツの日誌化法とも呼ばれます。
これはオーツ先生が提案したアルゴリズムですので、オーツの日誌化法というふうに世界的に呼ばれたりして、
ぜひ皆さんもですね、新しいより良い画像処理のアルゴリズムを考えて、それを論文として発表して、皆さんの名前のついた手法を提案できるといいんじゃないかと思います。
さあ、この判別分析法はどういう方法かというわけなんですけども、このようなヒストグラムから式値によって2つのクラスに分割したわけですね。
その2つのクラス間の分離度が最大となるような画素値を式値として決定するという方法です。
では、1つ順番にその処理を説明します。
まず、ある式値tにより2つに分けます。
式値tよりも小さい画素Fijはクラス1という集合に属しますよということです。
式値tは小さいわけですが、こちらからですね、こちらをクラス1というふうに属します。
ちなみにこのクラス1というものは、集合を今ここでは表しています。
クラス1という名前の集合にFijは属しました。
一方、この式値よりも大きいところはクラス2に属しますよというわけです。
なのでこちら側はクラス2というわけです。
そしてこの2つのクラスに分割できましたので、2つの集合ができましたので、
それぞれの集合であるクラス1と2ごとにそれぞれの平均値のnと平均値のmと分散を止めます。
同じように今度はクラス2のみのこちらだけの画素を使って、その画素数と平均値と分散を計算します。
次にこの計算した各分散平均画素値、画素数を使ってクラス間分散とクラス内分散を求めます。
クラス間分散はどういうものかということですが、
ノータンヒストグラムが2つのこんなような山になっていると思います。
ここで式1tで2つに分けました。
そうするとC1とC2になりました。
その時ここの中心、C1の平均がm1、C2の平均がm2となっていました。
このクラスC1に属する画素数はn1、クラスC2に属する画素数がn2でした。
ちなみにn1たすn2は全体の画素数nになります。
さあクラス間分散を見てみましょう。
クラス間分散の分母はn1プラスn2ですから全体の画素数になるということがわかります。
なのでクラス間分散を決める重要なものはこの分子になります。
この分子のまず最初n1かけるn2、n1の値とn2をかけるわけですけども、
n1とn2の掛け算した値が大きくなるためには両方の値がなるべく同じ値になってないといけないわけですね。
足してnですから半分に分けた時にn1かけるn2は一番大きくなります。
さあ重要なのはこちらのところです。
これはm1からm2を引きます。
m1はクラス1の平均、m2はクラス2の平均ですから
この差を計算するというわけです。
しかも差を計算するとm2の方が大きい値であるとマイナス値になってしまいます。
そこでそれを自乗します。
すなわちここの距離みたいなものを表すわけですから
これ自乗したものですから分散と言います。
ちなみにこのクラス間分散はn1とn2の画素数が同じぐらいで
かつできるだけm1とm2が離れていれば離れているほど
クラス間分散は大きくなるというわけです。
続いてクラス内分散です。
例えばこのクラス内分散を見てみると
この打1項と打1項に分けられています。
分母はn1たすn2がnですからどちらも

変わりません。上がN1、N2、それぞれに分散をかけたものであることがわかります。
じゃあこのクラス内分散が、上はクラス内分散が大きくて、下はクラス内分散が小さい状態になります。
クラス内分散が小さいということはどういうことかというと、N1とN1に分散をかけます。分散はここの広がりを表していることになりますから、
このクラス1の分散が小さければ小さいほど、上は非常に分散が大きい状態ですから、
分散が小さければ小さいほどクラス1のクラス内分散、クラス2の分散も小さくなればなるほど、このクラス内分散は小さくなるということがわかります。
そしてその後に分離度を計算します。分離度はクラス内分散とクラス間分散の比です。
すなわち、クラス内分散分のクラス間分散をします。分離度が最大となるような式位置を自動的に見つけますので、分離度が大きくなるためにはどうなればいいかというと、
分母は小さくなってほしいし、分子は大きくなってほしいわけです。
SB、クラス間分散が大きいというのはどういう状態かというと、平均がなるべく離れていて、かつ分母であるクラス内分散が小さくなる状態は各クラスの分散が小さくなっている。
すなわち、それぞれのクラスが、クラス内分散が小さくなって平均が離れているような式位置を決定するといいですよというのが判別分析法です。
実際には画素の値は0から255ですから、式位置の取り得る範囲というのはここにあるように1から254になります。
なので、まず式位置を1として、クラス間分散とクラス内分散を計算して分離度を計算します。
次に式位置を2として、クラス間分散、クラス内分散、分離度を計算します。
というように、式位置を1から254まで変化させたとき、それぞれ分離度を計算し、その分離度が最大となる式位置を最終的な一番好ましい式位置として決定するというわけです。
さあ、この式はアーグマックスと呼ばれる式です。これは何を計算するかというと、s分離度を計算します。
この分離度を計算するときに式位置を1から254まで変えて、それぞれ分離度sを計算して、そのマックスを取りましょうと言っています。
このargというところがない状態だと、sの最大値をthに代入するということになりますが、このアーグマックスという方法は最大値を代入するのではなくて、
sの最大となるときのtの値をthに代入するというわけです。こうやって数式で表現することができますので覚えておいてください。
さあ、では判別分析法の処理過程を見ていきましょう。この入力画像に対してノータンヒストグラムを計算し、式位置を1から254まで変えてきます。
式位置を1としたときのクラス間分散とクラス内分散を計算し、分離度を計算します。これを式位置1から254まで変えていって、クラス間分散、クラス内分散と分離度を計算するというわけです。
さあ、クラス間分散、分離度が高くなるところはクラス間分散が大きくて、クラス内分散が小さいところです。
一番高いところはここですかね。なのでこの辺りになって、結果式位置は132となりました。その132という式位置を使って日誌化するとこのような画像になります。
どうでしょう?このグレースケールの画像が入力されたら自動的にこの132という値を計算して判別分析法によって計算し、日誌化すると綺麗な日誌化画像を得ることができるようになりました。
では、これまでのPタイル法と判別分析法の結果を比較してみましょう。判別分析法は自動的に132という値を計算してくれます。
一方、Pタイル法はパーセントを決めないといけません。細かく65.2とか決めれませんので、60とか70とか大予想の値を設定していくわけです。
そうするとそれに合わせた式位置171だとか100といった値が計算できるわけなんですけども、どうでしょう?この3つを比べたときに元の画像をより良く日誌化できているのはというふうに見ると、こちらはやや黒画素が多すぎるような気もしますし、こちらは白の画素が少なすぎる、
足りないところがちょっと削られすぎているということもわかります。判別分析法は元の画像の情報をより残したような形でうまく日誌化できていると言えるということがわかると思います。これが各手法における比較になって判別分析法はより好ましい日誌化方法であるということがわかると思います。

それでは続けて、日誌化した画像の処理について説明します。日誌化によって背景に属する画像と
前景に属する画像に分離することができました。 では、前景に分離した画像は物体に属していることになりますから
その画素の何らかの特徴を計測することで、この画像中に含まれている物体の特徴・性質を知ることができます。
その一つの調べる特徴領として、連結性というものがあります。 まず、四連結というものは、四近法に対して注目画素の連結を定義したものです。
例えば注目画素に対して、前景が上、左、右、下、2のどれかに物体が、黒画素が存在すれば、これは四連結で繋がっていますよということになります。
この緑とは上下左右では繋がっていませんので、この赤の画素は繋がっているということになります。
一方、八連結の場合は、注目画素に対して、1、2、3、4、5、6、7、8の周りの八画素を見て連結しているかということを定義しますので、
緑は八連結としては繋がっているということになるわけです。 このように連結しているこの画素の集合を連結成分と呼びますし、
対象連結成分の中にあって、背景と連結していない白画素の集合を穴というふうに呼びます。
ではまず最初に、四連結で定義した場合どうなるかというのを見てみましょう。
四連結を使ってこの物体のところ連結性を求めると、緑の連結成分と青の連結成分と2つの連結成分、
すなわち2つの物体がこの画像に含まれているということがわかります。 ここは繋がっているように見えますが、先ほど紹介したように四連結は上下左右だけなので、
この斜めとは繋がっていることになりません。 したがってこの場合は2つの連結成分があるというわけです。
四連結で連結成分を見た場合、この中にある穴は八連結として捉えます。
八連結として捉えると穴はここ繋がったことになりますので、ここは穴でないです。 一方こちらの2つは八連結でこの穴のところが繋がって外とは繋がっていないので、
八連結の穴が1個あるということになるわけです。 続いて今度は8連結で見てみましょう。
8連結で見るとここが繋がっていることになりますので、全体でこの青の1つの対象が含まれているということになります。
8連結で連結成分を見たときは4連結で穴を確認しますので、ここはここと背景とは繋がっていないということになりますから、独立していますから、これが一つの穴になります。
こことここも背景とは4連結で繋がっていないので、ちゃんと穴になります。 さらにこのオレンジとこのオレンジは4連結では繋がっていませんね。
なのでこの場合は1個、2個、3個の3つの穴があるということになります。 このような日課画像から4連結の場合はこのような性質が、8連結で定義するとこのような性質をですね
計算することができるようになるというわけです。 続いて今度は輪郭線追跡です。輪郭線追跡も日課画像処理の中でよく行う処理の一つです。
これは連結成分、一つの連結成分の境界を求めるという方法です。 じゃあどのように求めていくか順番に見ていきます。まず1番ラスタスキャンにより白画素から黒画素に変わる画素を探索します。
これラスタスキャンというのはどういうことかと言いますと、画像の左上から順番にこうやってスキャンしていくことをラスタスキャンと言います。
これが右まで来ると1個下のところをまた順番にこうやって見ていきます。 今白から黒画素に変わるところを見つけますので、上から順番に見ていくと白から黒に変わるところはありませんでした。
次の行を行って白から黒行くと、ここですね。白から黒に今変わりましたというわけです。 白から黒に変わる画素を見つけたので、そうするとここをですね
まず輪郭線のスタートとして、最初はこのような領域に対して、ここは真ん中ですね。領域に対して2番です。
進入方向、起点に番号順に右回りに黒画素を探索します。 今左から右に進入しました。左から右に進入しました。
ここのピンクが、ここに今対応しているという状態です。 なのでこの1番、ここから順番に番号順に右側に黒画素を探索しますから、
1番、2番、3番、4番という風に探索します。 ここにおいて1番はここですね。1番、2番、3番、4番と行くと黒画素が見つかりました。
なので黒画素が見つかったところに移動します。 これがこっちに移動するわけです。
隣の画素にこうやって移動した。今度は隣の画素に移動しましたので、また同じように、
今ここから見るとここが1番ですから、ここから見るとここが1番ですが、 1番、2番、3番、4番、5番と見ると5番のところに黒画素が出てきましたので、今度はここから黒画素に移動するというわけです。
なので今度は注目画素はここになって、この領域で確認をします。 この時に斜めに進行しました。
斜めに進入したので、斜めに進入した時は進入方向に対するここが1番になります。 こっちからこういうふうに進入しましたから、こういう形に進入しましたので、1番はここになりますね。
なので1番、2番、3番、4番、次5番のところで黒画素が発見できましたので、なのでこの場合は今度は黒画素がこちらに移動して、また同じように進行方向が超えてきましたから、ぐるっと探索をするというわけです。
これをどんどんどんどん繰り返していくことで、こうやって輪郭線を追跡することができるようになります。 さあこの輪郭線追跡をどのように実際に使うかということですけれども、
今日最初にレシートの数字とか文字を読んだりするという話をしました。まさに文字認識をするためにこの輪郭線追跡が使われています。 じゃあどういうふうに使えているかというと、まずこのように日化をした画像があります。日化した画像から輪郭線を取り出します。
この輪郭線から特徴を各小領域ごとに取り出します。 例えばこの小領域を注目すると、このような輪郭線とどちらに移動したかというものを方向コードで表したものがあります。
例えば横に移動した場合は0番、斜め上に移動した場合は7番ということがわかるわけです。 この小領域からこの方向コードによるヒストグラムを計算します。どういうことかというと、1番、2番、3番、4番、5番、6番、7番という方向コードがあります。
この中に各方向コードが何個あるか数えるわけです。 0番は右に移動したものですから、1個、2個、3個、4個ありました。なので0番は4つありましたよというわけです。
続いて1番下ですね。斜め下はありませんね。なので0。次2番下です。下に移動もこの中には含まれていません。次3番斜め下です。斜め下はここに1つありますね。なので3番は斜め下が1つありましたということで1個です。
次4番。4番は左に移動ですけれども、1、2、3、4、5、6個ありますね。結構たくさんありました。なのでこれは6個ありましたよということで6個ありました。続いて5番行ってみましょう。5番は斜め上ですけれども斜め上はここで1、2、3、3つありましたので5番は3つありました。
そして7番は斜め上1個、1個だけですね。なので1つ1個ありました。最後6番はありませんでしたので6番はこれなしです。
次に7番が1個ありますので7番が終わったということで、この方向各黒画素の輪郭線追跡したときの方向コードがどれぐらい何個方向コードごとにあるかというのをヒストグラムで表現した。
すなわちここの同じ領域のヒストグラムがここの領域の特徴領になるわけです。この処理をすべての小領域に対して計算したものがこのチェーンコードの方向ヒストグラムという特徴になります。
そうするとこのアーという画像だったものがどっちの方向の成分が多いかによってア、イ、ウ、エ、オで文字ごとによってこの特徴パターンが変わってくるわけです。それを具体的には主成分分析という方法を使って特徴のベクトル次元数が大きいので小さく次元圧縮して、
いらない冗長な情報も含まれているのでそういうのをなくしたような形で次元圧縮をして、そして次元圧縮した特徴ベクトルをあらかじめ登録したアの特徴ベクトルと距離計算をして距離が小さければアという文字ですよというふうに識別するというものです。
このようにして先ほどの輪郭線追跡した結果を使って文字認識が実際に使われているというわけです。では他の日課画像処理としてノイズ除去について説明します。
ノイズ除去、日課画像のノイズ除去手法として収縮と同調という処理があります。収縮という処理は背景または穴に接する対象画像の一回りを剥ぎ取る処理です。
どういうことかというと、このような日課画像に対して収縮をすると一回り分剥ぎ取っちゃいますから、痩せたような文字に変わります。小さいこういう点の値はなくなっちゃうというわけです。一方、膨張は背景または穴に接する対象画像の一回りを加えるという処理です。これは一回り太らせるような処理です。
膨張処理をこの画像に施すと一回り増えますから、太く線が変わりますし、小さな点も大きくなるというわけです。この収縮と膨張を行うわけですけども、クロージングと呼ばれる処理では、原画像に対してまず膨張を2回します。
そうすると、太ったような日課画像になって、2回膨張したので、その後2回収縮を行います。そうすると、2回膨張したときに穴が埋まってますから、この収縮したものは穴が埋まった状態になっています。
続いて、オープニングという処理です。オープニングは、まず収縮を2回行います。この画像に2回収縮をすると、この小さい細かなノイズがなくなります。だけど、穴が非常に大きくなっちゃいます。そこで2回収縮したので、次に2回膨張をします。
そうすると、穴の大きさは元と同じだけど、この小さなノイズがなくなった状態になっています。では、最初に原画像に対してクロージングをしてから、その後クロージングした画像にオープニングをすると、穴が埋まって小さなノイズがなくなるようなノイズ処理、ノイズ除去をすることができるようになります。
これが、日誌化画像におけるノイズ除去の手法であるクロージングとオープニングです。このクロージングオープニングは、それぞれ収縮と膨張という処理からなります。
続いて、ラベリングです。ラベリングは、このような日誌化画像に対して番号であるラベルを振っていきます。
同一ラベル、例えば1というラベルを持つものは1つの連結成分、2というラベルを持つものは2という物体というふうに属しているということが分離することができるようになります。
さあ、どのように行うかということなんですが、まず左上からラッサスキャンしていきます。そして、まずラベルがついていない画像を見つけます。
最初はラベルがついていない状態ですから、ラベルがついていないので、ここにまずラベルを最初につけます。
初めて見つかったので1番としました。また続けて見ていきます。そうすると、ここも最初はラベルがついていない状態ですから、新しく、さっきも1番はラベルがついていましたので、
1番というラベルが存在していますから、今度は2番というラベルをつきます。そして、またどんどんどんどん探していくと、今度また新しいのが見つかりました。
この時、上を見ます。注目画像の上の画像がラベルを持つか見ていますので、この場合まだラベルを持ちませんから、新たなラベル3をつけます。
さあ、それで次です。ここを見たときに、上の画像がラベルを持っていることがわかります。なので、その上の画像が持っているラベルをここに採用します。
そうすると1番になります。ただし、この時に注目画像の左の画像がラベルを持ち、注目画像のラベルと異なるとき、
Lookup tableにそれらのラベルが同一連携成分であるよということを記録しておきます。
すなわち、ここの注目画像は上に1番というラベルがついていて、その1番をこの注目画像に採用しました。
左を見ると異なるラベルがついています。これは本来同じラベルになるべきですので、
Lookup tableに1番と3番は同じものですよというふうに控えておく。
これを次にどんどんどんどん繰り返して新しいラベルをつけていくんですが、ここをつけたときに上に2番がついていますから、
ここは2、隣見ると4ですから、この4は2である可能性があるということですから、
Lookup tableに4と2は同一ですよというふうにラベルをどんどん付けていく。
これを左上から下まで繰り返したときに、Lookup tableを参照して、それぞれ1番と3番は1番にして、
5番と1番は1番ですよ、4番は2番ですよ、6番は2番ですよというふうにまとめてあげることによって、
1番と2番の2つのラベルのついたラベリング処理ができるということになるわけです。
このラベリング処理した画素に対して、形状特徴というパラメータを計算することができます。
例えば、重心、外設長方形、面積、周囲長、円形度、オイラー数、モーメント特徴というのがあります。
重心は図形全体の重さの中心です。
外設長方形はこの囲む外設長方形を表します。
面積は全体の画素数になります。
周囲長は輪郭線追跡したときのその長さになります。
面白いのは円形度ですね。円形度はこのような式で計算できます。
この物体が円にどれくらい近いかを表す尺度になっています。
他にはオイラー数。オイラー数は先ほど求めた連結数、連結成分の数から穴の数を引いたものです。
もう一つはモーメント特徴です。
モーメント特徴はこの式で計算できます。
pとqに0と1の値を代入したときにこの式を計算します。
pはiというxの座標にかかりますし、qはjというyの座標にかかります。
もし0と0という風に代入すると、x座標の値を0乗、yの座標を9乗も0乗しますから、それぞれ1になりますから、
Fijは2値化した画像ですから、0もしくは1になっています。
Fij自体は0か1の値を持っている状態ですから、この場合は1の値が相和されます。
すなわち図形の面積を求めることができます。
ちなみにpを1、qを0とすると、x軸の一時モーメントを求めることができ、
m10からm00、すなわちx軸の一時モーメントから図形の面積を割ってあげると、
重心のx座標を出すことができます。
このように0と1の組み合わせによっていろんな数値を計算することができるのを、
モーメント特徴という風に呼びます。
今日は2値化した手法と、その2値化した画像のいろんな処理について説明をしました。
次回はテンプレートマッチングについて紹介したいと思います。
今日はこれで終わります。それではまた。

みなさんこんにちは。今日はテンプレートマッチングです。
ここまでこの講義では画像をどのように標本化、量子化して濃淡を変化させるか、
そして平滑化、エッジチュースのような空間フィルタリング、
さらには画像を変形する気化変換、直線の検出をする波幅変換、
テクスチャーを解析するフリエ変換、
そして日課処理と日課画像処理について紹介しました。
今日はこの処理から次は認識に近いところでテンプレートマッチングという手法について説明をします。
テンプレートマッチングはどういったところで使うかというと、
このような入力画像の中からこのテンプレートと同じものがどこにあるかといったことを探す技術になります。
こちらの画像をテンプレートと言います。
具体的には外観検査、例えば工場の中で商品、いろんな部品がこう、
例えばロボットからすると部品を取り出したい時にピックアップ、グラスピング、
掴む部品をどこにあるか、画像中の中のどこにあるかということを検出する際に
このようなテンプレートマッチングを使ったりします。
このテンプレートマッチングはどのようにできるかというと、
画素同士の比較によって類似度が最大となる位置を見つけるというわけです。
そのためにはテンプレート画像とテンプレート画像と同じ大きさの入力画像の領域との類似度計算を行います。
今度はこの探索するウィンドウと言いますけど、
これを横にちょっとずらして、また類似度計算をします。
隣にずらして、ここと類似度計算をするというのをずっとラスタスキャンしていって、
それぞれの領域に、それぞれの位置において類似度を計算します。
この右の図は各点をスキャンした時の縦軸が類似度で表したものです。
類似度が高いければ高いほど似ているということを表現します。
この中で一番類似度が高いところは色々ピークが超えてあるわけですけども、
一番高いところはここのわけです。
なのでこの座標のところにテンプレートが存在しますよということを検出することができるようになります。
すなわちこの類似度が最大となる座標を結果として出力するということになります。
このテンプレートマッチングではどのように類似度を計算するかということが重要になります。
この類似度の尺度にはまず SSD という方法と SAD という方法があります。
SSD は sum of squared difference リファレンスは差、スクエアードは事情ですから、差の事情の相和を計算したものが SSD ということになります。
一方 SAD は sum of absolute difference になっています。
なので差の絶対値の相和が SAD と言われるものです。
この両者ともにこの尺度は似ているほど小さな値になるので相異度とも呼ばれます。
i が画像の座標、Tij がテンプレートの座標です。
なので IJ で指定した座標同士の画素値の差を計算して事情したものが SSD、差の絶対値をとったものが SAD です。
これがテンプレート画像だとすると N × M のサイズになっていますので、
画像と同じサイズのところの画素の各画素の差の事情をとってその相和をします。
このように計算するのが SSD と SAD になります。
もう一つよく使われる方法に正規化相互相関という方法があります。
これは Normalized Cross Correlation というふうに呼ばれます。
計算式はこのような式になります。
ちょっと複雑な式になっていますが、これは何を表しているかと言いますと、
2つのベクトル、i という画像と T という画像がありますけれども、
その i,T,i と T をそれぞれベクトルで表現すると、そのベクトル間のナス角を実は計算していることになります。
例えばこのテンプレート画像実際は 10 × 10 の画像とします。
そうするとこの場合は100次元のデータということになります。
入力画像の方も同じように10 × 10 とするとこちらも100次元のデータとなります。
今ここの原点は本当はですね、画像絵で描く場合は2次元、3次元しか描けても3次元しか描けませんが、
ここが100次元空間を表していると思ってください。
その100次元空間の中にこの1個1個の値がありますから、1つ T というベクトルが決定されるわけです。
ここちょっとわかりづらいのでもう1回説明します。
画像が10 × 10 であれば全てで100個の値を持ちます。これが100次元ということになるわけです。
100次元の空間を軸で表現することはちょっと可視化ができませんが、ここに100次元の空間を表現していると。
その1個目の値、2個目の値、3個目の値を使うと1つのベクトルが決定できるわけです。
これがこの T を表していることになります。100次元空間におけるベクトルです。
同じように I も10 × 10 の100次元ですから、同じ100次元の空間において I はこのようなベクトルとして表現できますよというわけで、
この正規化相互相関は何を計算するかというと、ベクトル T とベクトル I のナス、このシーター、
すなわち正確にはコサインシーターを求めることがこの式によってできるわけです。
ちなみに SSD は、この SSD した後をルートすれば平方根の定理と同じでここの距離になるわけです。
すなわち SSD はベクトル間のここの距離を計算していることになりますし、
SAD はこことここを足した距離が SAD となっています。このように SAD のことを市街地距離、
SSD のことをユークリット距離の事情、そして NCC はベクトルのナス角のコサインということになります。
このように類似度を計算するためにも、1種類、2種類、3種類とあり、それぞれ意味が異なるというわけです。
さらにこの距離尺度、類似度を計算する方法のアプローチとして、相互相関係数というのがあります。
英語で言うと Zero Means Normalized Cross Correlation です。先ほどの NCC と正規化相互相関と何が違うかというと、ここがありませんね。
ゼロ、すなわち新しくこの Zero Means というところが入っています。これは何かというと、先ほどの式と違うところだけを書きますと、
ここ、ここ、ここ、ここが違います。これ何をやっているかというと、i のバーと t のバーで、それぞれの i からは i のバーを、t からは t のバーを引いているということがわかります。
じゃあこの i のバーと t のバーは何かということなんですが、i のバーは、これは入力画像の領域の平均ですね。
なのでこちらは平均入力画像となっているわけです。入力画像の平均値を計算しています。
一方こちらはテンプレート画像の平均値を計算しています。なのでそれぞれ平均値を引いているということになります。
そうすると、先ほどはこのベクトルのナース角を、こことこのベクトルのナース角で見たわけです。
なので、例えば i と t がそれぞれこのようなベクトルだとすると、この間のナース角、コサインシーターを求めていたわけなんですけども、この時にこの角度は、この t のベクトルが長くても短くても基本的には影響を受けないんですが、
できるだけこの長さをちゃんと揃えて相関を求めましょうという。なのでそれぞれ i に対して、i の平均値で引き算をして、t に対しても t の平均値で引き算することによって、
この正規化相互相関のゼロミーン、すなわち平均で引くといったことを実現している。平均値を引くので、2つの画像領域の平均値が異なっていても、ちゃんと類似度が1となる、いわゆる平均的な明るさの変動を吸収することができるわけです。
どういうことかもう一度確認をすると、この図が平均で正規化した i と正規化した t において、いわゆるコサインシーカー、類似度、相互相関係数を計算することができます。
では実際に具体的に、これらの距離尺度を使って、類似度尺度を使って、どのような結果になるか見てみます。ここではこのような ic チップをテンプレート画像として、このテンプレート画像がこの入力画像の中のどこにあるかということを探しています。
皆さんどこにあるかすぐわかりますか?ここですね。これはこのテンプレート画像と、この入力画像と似たような環境下、いわゆる照明環境下で撮影されているので、すぐパッとわかると思います。
これはテンプレートが撮影した時と同じような環境で入力画像を撮影していれば、比較的簡単に見つけることができます。
しかし、例えば入力画像を撮影するときに、テンプレートを撮影したときと異なる照明環境で撮影したりすると、例えば窓辺で撮影したりすると、太陽の光が外から入ってきますので、それによってこのようにちょっとハレーションが起きたような画像になってしまうことがあります。
そうすると、これと同じものがどこにあるかというのを探すのが少し難しくなるというのがわかると思います。では、今から実際にそれぞれの画像でどのような結果になるのか見ていきましょう。
まず、通常のこちらの通常の画像に対するテンプレートマッチングの結果です。これがSAD、縦軸が類似度を表します。
SAD、SSD、NCC、ZNCCです。このSADとSSDは距離になりますから、距離が小さいほどテンプレートと同じものがあるというわけです。
従って値が小さいところを見てみると、一番小さいところはこの例ですとここですね。ここもちょっとわかりづらいかもしれませんが、ここになっています。すなわちここの座標、これですね。ここが一番近いですよというふうに出てきたところです。ちゃんと同じところが検出できているのがわかると思います。
一方、今度はNCCとZNCCを見てみましょう。NCCとZNCCはコサインシーターを計算しますので、値が1になればなるほど近いことになります。ちゃんと書いておきましょう。こちらは距離を計算して、こちらは類似度になっていますので、距離は小さければ小さいほど似たものがありますよということになります。
類似度は似ているものだと最大が1になりますけど、1に近ければ近いほど似たものがありますよという。さあNCC見てみましょう。これよく見ないとなかなかわかりづらいんですけど、ここにちょっとピークが出ているのがわかります。なのでここの位置座標のところにちゃんとテンプレートと同じ画像が存在していることがわかります。
ZNCCはもう見た感じすぐわかりますね。ここがピークになってますから、ここの位置のところに対象のテンプレートと同じものが映っているということがこれでわかるわけです。続いて今度は照明変動が起こった画像です。こちらの画像ですね。こちらの画像においてどう変わるかというのを見てみましょう。
同じようにSSADとSSDは距離を計算します。縦軸は距離です。なので似ているほど距離は小さくなる。一方下のNCCとZNCCは類似度ですけど高くなるほど似てますよということを表します。
さあこれをラストスキャンしてみると一番距離が小さかったというのはSSADはここになっています。SSDも間違ってしまってこんなところになってしまいます。それはなぜかというと軌道値の変動が起こってますよね。テンプレート画像と入力画像で軌道値が変わってしまいますから、例えば同じこちらがテンプレート画像としましょう。
入力画像も同じものであれば同じような方向を向きます。しかし暗くなってしまうと入力画像が本来であれば入力画像も同じようなこういうベクトルになるので距離を計算すると小さくなるわけです。
しかし今これは照明変動で全体的に、この場合は明るくなっちゃってますね。明るくなったってのはどういうことかというと、このiが明るくなったので持った値がすべての画像において大きくなっちゃったわけです。このとき例えばSSDは何だったかというと、このベクトルのここの距離を計算しますよね。これがSSDでした。
なので照明変動が起こったことによってこの距離は大きくなるというのがこのSSD、SSADにおけるこの例におけるうまくいかない例です。
さあじゃあ一方類似度を見てみましょう。類似度はこれよく見ると一応正解のところに出てます。
わかりづらいですけどちょっとピークが出ています。一方ZNCCはもうはっきりとピークが出ています。じゃあなぜ今度NCCとZNCCではうまくちゃんと正解の位置が見つけられるかということなんですが、先ほども話をしたように
テンプレートのベクトルTとIが照明変動があったとしてもそのナス角シーターは変わらないのでちゃんと求めることができますよってことです。
だけどより正確に求めるためには一旦平均で引いて計算すると周りと大きく差が離れるような形でテンプレートマッチングをすることができるようになるというわけです。
このように手法によってどういう入力画像に対してうまく適応するかできるかできないかということがあるわけです。
ここでは照明変動が起こるとこのナス角を計算するNCC、ZNCCは問題ありません。うまくいきますよ。
一方、ベクトル間の距離をそのまま計算するようなSADとかSSDは照明変動が起こるとうまくいかないという。
例えば入力画像が必ず限定された環境下で撮影される場合であれば照明変動は必ず起こらないということであればNCCやZNCCを使う必要なく
SADとかSSDを使えばいいわけです。実際こちらの方がこちらの方法よりも計算量が少ないので早く計算できるというメリットがあるわけです。
まず距離計算方法ルーチとの尺度によって得意不得意があるといったことを理解しておいてください。

続いてこのテンプレートマッチングを 少しでも早く計算しようという方法が
粗密探索と言われる方法です どういうことかと言いますと
この一番下の画像が 入力画像の原画像だと思ってください
ここを左上からこうやってラスタスキャンをしていくと
解像度が非常に高い画像だと テンプレートマッチングする回数が
かなり増えますので 計算量がどんどん時間がかかってしまうわけです
そこでなるべくこのテンプレートマッチングを 高速に行う手法として
粗密探索という方法があります これは英語ではCourse-to-Findと呼ばれたりします
さあどうするかというと まずこの入力画像をダウンサンプリングして
低解像度の画像を段階的に作っておき まず最初はこの低解像度の画像に対して
テンプレートマッチングをします この場合は4×4の領域に対して
テンプレートマッチングをして 16回するわけですけども
その中でここの位置が一番距離が 小さかったですよとなります
そうするとこの以外のところにはテンプレートは おそらくもう存在しないであろうというわけですから
この辺を他の周りのこの辺りですね こういった周りのところを
もう探索しなくていいよというわけです その代わり実際にここは今低解像度の
4×4のところは低解像度の状態ですから 今度はここの辺にあるってことがわかったので
1段階上の解像度の画像の同じ対象とする領域内で またテンプレートマッチングを順番にします
その中で一番似ている距離が 類似度が一番高いところを探して
その高いところがここというふうに分かりますので
今度はここに対応した領域の原画像の解像度で またテンプレートマッチングをして
最終的にここですよということを見つけるというわけです このように低解像度で全探索して
その見つけたところの領域に対応する 一つ上の高解像度のところでまたテンプレートマッチングをして
さらにその見つけたところに対応する さらに高解像度のところをまたテンプレートマッチングをする
そうすると全体を細かくテンプレートマッチングするより 計算回数を大幅に減少させることができるというわけです
では続いてアクティブ探索という テンプレートマッチング法について説明します
このアクティブ探索法は 相当り計算を間引いて効率的に探索します
例えばこのようなテンプレートが この入力画像のどこにあるかということを探します
その時に通常のテンプレートマッチングであれば 全てのところをガーッとラスタスキャンしていったわけなんですが
このアクティブ探索法を用いると テンプレートと似ていないところは荒く探索します
一方テンプレートと似ているところは細かく探索します
この黒い一個一個の点はそこの座標において 実際に類似度計算をしたというところです
本来であれば細かく全てのところを 類似度計算するわけなんですけども
この点の数を見てわかるように かなり計算回数を減らすことができるようになっているわけです
じゃあなぜこれができるかということなんですが
そこには1つこのテンプレート画像から カラーヒストグラムというものを求めます
このカラーヒストグラム ヒストグラム化した性質を使って
カラーヒストグラムの類似度が低ければ 周辺の重なりを持つ領域の類似度も低いと考えて
周辺の類似度計算を大幅に省略するというアプローチです
ではもう少し詳しく説明しましょう
まずAという領域とBという領域があったとします
Aという領域からカラーヒストグラムを計算しました
入力画像がカラーであればカラーヒストグラムになりますし
入力画像がグレースケールの画像であれば
これはこれまでやってきたノータンヒストグラムと一緒です
横軸は0から255という値を持つヒストグラムになります
Bの方に対しても このBという領域のカラーヒストグラム
ノータンヒストグラムを計算します
この時Aという領域とBという領域には
ここの重なり領域を持ちます
このAキャップBの領域です
ここはどういうことかというと
ここの共通領域のヒストグラムは
この緑のヒストグラムになる
この緑は上のヒストグラムにも下のヒストグラムにも
当然ながら含まれているというわけです
この含まれている共通領域とAの領によって
Bを計算した後にAを計算しなくても
どれぐらい類似度の上限値がいくつになるか
ということを知ることができます
ではもう少し詳しく説明します
まずテンプレート画像Mがあります
入力画像のあるBという領域と
距離計算をした時の類似度があります
これが既に計算した類似度がSBMです
これは既に計算しているわけなんですけども
次このテンプレートをAという領域と
類似度計算をするときに
本当にAという領域とM
テンプレートMと類似度を計算するべきかどうか
それをこのBとの類似度と
重なり度合いによって
Aとの類似度の上限値を
あらかじめ計算しましょうというわけです
その上限値によっては
Aはもしかしたらテンプレートの可能性があれば
類似度を計算しますし
上限値がそもそも低ければ
そこにはテンプレートは存在しないわけですから
その場合は類似の計算をやめればいいですよというわけです
どのように上限値を計算するかというと
これがその式です
類似度Bとの類似度SBMに
Bの面積をかけたものと
AとPの共通領域のミニマムをとって
そこにA-Bをしたものを計算します
これが設定してある上限値と比べて低ければ
そもそもこのAという領域に似ているものはないので
SAMの計算は計算を省略しますよということができるわけです
もう少しわかりやすく説明します
今事前に計算できているものは
この類似度のSBMです
この類似度のSBMが低い場合
もう一回書いておきましょう
こちらがA、こちらがBと別にMというテンプレートがあるわけですね
今計算できているのは
AとBの距離、MとBの距離、SBMになります
類似度が結論に計算できて
そのここの類似度が0.2だった場合ですね
ちなみにここがA-Bの重なり率になります
なのでまず類似度を計算して
この類似度が0.2と小さい時は
そもそもBとの共通領域の重なり率が
大きいとここですね
上限値は0.2が最大になるというわけです
ということは0.2ということはほとんど
もうMとAは似てないよということになります
したがって類似度が低くて
重なり率、ここのA-Bが重なり率が高い時は
上限値が小さくなるので
MとAは類似していません
すなわちMとAの類似度を計算する必要はありません
では逆にMとAの類似度を計算しないといけない時は
どういう時かというと
まず重なり率が低い時は
これ見ていただくと分かるように1ばっかりになっています
ということはテンプレートMとAの類似度の上限値は1なので
テンプレートのある可能性がありますよ
それはなぜかというと
BとAの重なりが低いので
あまり重なってないから
Bにある可能性はあるかもしれません
ということを言っているわけです
その時はちゃんとMとBの距離
SAMを計算しましょうということになります
もう一つ類似度が高い時も横に見て分かるように
当然SBMの類似度が高ければ
このBの周辺にMがある可能性があるわけですけど
高くなりますよというわけですね
この方法のいいところは
この辺が省略できるわけです
この辺の類似度計算の省略ができることによって
テンプレートマッチングの高速化
探索の効率化ができるわけです
くどいですがこの方法は画像と画像ではなくて
画像それぞれを一旦ヒストグラムで表現します
ヒストグラムで表現するから
重なり領域というものが共通情報を持つから
このような方法で上限値を求めて
その上限値がそもそも低ければ
実際の類似度計算をしなくて
次に飛ばしていくことができるようになるという
アクティブ探索の流れをもう一度説明します
アクティブ探索は類似度の上限値を用いた
アクティブ探索による探索の枝刈りというものを行っています
まずステップ1です
目標類似度の初期値を設定します
最初初期値は入力画像全体と
テンプレートの類似度を初期値として入れておきましょう
そしてテンプレートと探索領域
左上の局所領域との類似度をまず計算します
この上でステップ2で求めた類似度と
周辺領域の上限値を計算して
上限値が目標類似度よりも小さい場合は
類似度の計算が省略続いて
上限値が目標類似度より大きい場合は
これは似たものが周辺にあるだろうということで
類似度を計算します
この時、類似度が
そもそもの目標類似度よりも大きい場合は
目標類似度を更新します
そしてこの処理をぐるぐる回していくと
最終的に類似度が最大値となる位置を
テンプレートの位置として出力することができます
この方法のいいところは
テンプレートに類似した金棒では細かく探索して
テンプレートに類似していない領域では
粗く探索するという手法になっています
実際にアクティブ探索による
処理時間のグラフを作ってみました
こちらは一般的な総当たり法全探索です
一枚の画像に見たところ
テンプレートを探し出すのに
約4.5秒かかっているというわけで
それに対して粗密探索を導入したようなものですと
1.6秒になります
一方アクティブ探索法は0.21秒ですから
かなりこの上限値を用いた枝刈りによって
高速化できているということがわかると思います
今日はテンプレートマッチングとして
テンプレートマッチングにおける
距離計算方法について紹介しました
そして粗密探索
そしてアクティブ探索の
アルゴリズムについて紹介しました
次回は特徴点検出Q2の手法である
SHIFTというアルゴリズムの説明をします
今日はこれで終わります
それではまた

みなさんこんにちは 今日は特徴点検出 記述ということで
SHIFTというアルゴリズムについて説明をします
まずSHIFTというアルゴリズムを
まず僕自身がいつ使ったかということを
そのきっかけを紹介したいと思います
よくですね こういったいろんなアルゴリズムというものは
まず最初に本で知るのではなくて
われわれ研究者は論文で知ります
論文でこういう方法があるんだということを知るわけです
一般的にです 一般的に
これ良さそうな論文だと思って読んで
それを実際にプログラムを作って
自分のデータで試してみるわけです
最近はジットハブというサイトがあって
いろんな方がですね
論文で提案した手法のプログラムを公開していますので
そういうのをダウンロードして使ったりするわけです
そういった時にですね
よくあることはどういうことかというと
論文に書いてあるよりも
思ったより性能が出ないということが
よくあったりしたわけです
だったんですが このSHIFTに関しては
僕個人的に意見としては
論文を読んで実際に試してみると
思ってた以上にできたというわけです
その最初にですね
衝撃を受けた画像がまさにこの画像だった
これは皆さん何かわからないと思いますけども
これは僕が以前働いていた
カーネギーメロン大学のキャンパスの画像です
これは2回目カーネギーメロン大学にですね
行ってた時に撮影した画像なんですけども
カーネギーメロン大学の
ロボット工学研究所というところは
白紙工記課程から
いろんな世界中から入ってくるわけです
すごい倍率が高いところです
そこで研究をやってたんですけども
その時に向こうの僕の先生
ボスの先生から
ドクターの学生を1人面倒を見てくれ
というふうに言われたわけです
それで20倍ぐらいの倍率を勝ち抜いてきた学生と
僕は仕事ができるわけで楽しみにしてました
その時に彼の研究としては
2枚の画像からシーンの構造を認識するということで
まず人間の目と同じように
2枚の画像を撮影しようといって
その彼が撮影してきた画像なんです
本来はステレオ視、ステレオビジョン
これまた次回のところでも紹介しますけども
ステレオビジョンをするためには
右のカメラと左のカメラがどこに置いてあって
どこを向いているのかということを
ちゃんと知らないとどこにあるかということを
いわゆるステレオ視
三角測量で求めることができません
この学生が撮ってきた画像は
そういったキャリブレーションと言いますけども
キャリブレーションするためのデータが一切なく
撮ってきたのは本当にこの2枚だけだった
そこでじゃあそういうカメラの情報を使わずに
どうやってこの中の画像の理解をするか
シーンの理解をするかということで
その時にそういえばシフトという論文があったので
それを試してみようかと言って試してみたのです
そしたらですね
これも何の工夫もせずに実際に試した結果です
何を表しているかというと
この線の始点と終点が同じ位置
同じところですよ
これをですね
対応点というふうに呼びます
対応点を事前知識なしに
これぐらい複数のですね
対応点を求めることができました
よく見てみるとこちらの画像とこちらの画像と
色合いが違いますよね
しかも実際にカメラ感がですね
結構離れています
分かりづらいかもしれませんが
ここの同じところを見ると
かなり離れているということが分かると思います
この辺ですここですね
こことここですから
x 座標の差を見るとこれぐらい幅ありますよね
一方ではこちらは
ごめんなさいこれぐらいですね
なので差がこれぐらいずれているというわけです
この辺ですよね
なので対応点見つけるの結構大変なんですけども
Shiftを使うと何の事前知識もなくここまでできた
それで僕はですねすごく感動しました
こんなによくできるなんてすごい
それまでは結構実際に論文を読んで
実装してみて動かしてみると
もしくはソフトコードを持ってきて動かしてみると
思ったよりできなかったことは多かったんですけども
逆だったんですね
この論文は思った
僕は期待していた以上に良い結果だったということで
それでじゃあこのShiftというアルゴリズムが
どうやってできているんだろうということで
詳しく勉強して
そういったShiftを使った研究を
いろいろやるようになったというわけです
これが2004年ぐらいですね
その後日本の学会とかでも
このShiftに関するチュートリアルを
僕がやったりしたという流れです
さあではですねShiftの内容に入っていきましょう
まずShiftは
スケールインバリアントフィーチャートランスフォームと言います
スケールというのは
これ拡大縮小という意味ですよね
それに対してインバリアントという言葉が入っています
インバリアントというのは何かというと
普遍という意味です
なのでスケール変化に不変な特徴変換
特徴抽出ということになります
さあこのアルゴリズムは
ユニバーシティオブブリティッシュコロンビアの
デイビッド・ロー先生
この方が提案した方法です
実は元はもう1999年ですから
かなり古いものになるわけなんですが
今現在もなおですね
この方法は良い方法として使われています
画像処理のライブラリに
OpenCVというライブラリがあります
その中にもこのシフトを使用することが
できるように今はもうなっています
さあこのシフトのアイデアなんですけども
2つのプロセスからなります
まず1番特徴点
シフトのアルゴリズムの中では
特徴点のことをキーポイントと呼びます
この特徴点であるキーポイントの検出と
特徴量給出するということが1つ
もう1つは回転スケールに不変で
照明変化に頑堅な特徴量を算出してくれる
この2つの大きな特徴があります
では実際にこのシフトのアルゴリズムを
1つずつ見ていきましょう
まずシフトのアルゴリズムは
1、キーポイント検出と
2、特徴記述の2つに分かれます
キーポイント検出においては
スケールとキーポイント検出
これをReference of Gaussian
略してDOGという処理で行います
続けてキーポイントのローカライズとして
より良いキーポイント候補の中から
好ましいキーポイントを絞り込んで
選択をしてそしてより精度良くするために
サブピクセル位置推定を行います
これがキーポイント検出の流れです
続いて特徴記述です
特徴記述はまず最初に
オリエンテーションを算出します
そして特徴量を算出したオリエンテーションで
正規化した後、特徴量を記述します
こうすることによって
勾配方法ヒストグラムを作って
これを特徴量として記述していくわけです
さあでは順番にですね
このあるシフトの処理のアルゴリズムを
説明していきます
まずシフトのアルゴリズムに入る前に
ラプラシアン・オブ・ガオシアン
ログオペレーターについて見ていきます
ラプラシアン・オブ・ガオシアン
これはどういったアプローチだったかというと
ガオシアンのラプラシアン
2次微分を取るようなオペレーターです
このような式で表現されるものが
ログオペレーターなんですけども
このログオペレーターはガオシアンを含んでいますので
シグマというパラメータが存在します
このシグマというパラメータは
何を決めるものかというと
スケールです
スケールというのは
このログオペレーターのこの形状を
シグマによって変えることができます
例えばこのシグマの値を大きくしていくと
こんな感じに広くなっていくというもので
ここである画像に
このシグマを変化させた
このようなログオペレーターを
畳み込んでいくのです
積分計算していくのです
そうするとそれぞれのシグマに対応した
何らかの値がこうやって出てくるわけです
この時この値の変化を見てみると
極大値となる点が出てくるわけです
この極大値となるシグマの値が
ここを中心に畳み込んだ画像の周辺領域の
より特徴量を多く含むであろう
スケールということになるのです
例えばシグマ9の時に一番大きくなったとします
そうするとこの範囲に
ここからこの範囲に
より多くの情報が含まれているということが
言えるというわけです
このようにログオペレーターを使うと
そのある点に対して
どれぐらいの範囲により特徴量が
情報量が多く含まれているかということを
知ることができるというわけです
これをシフトのアルゴリズムでは
ログオペレーターの処理ではなく
DOGという処理によって実現しています
ログオペレーターは各点に対して
そのスケールを探すことができます
ただしその点が特徴的なキーポイントであるかどうか
といったことは残念ながら見つけてくれません
そこでどこに特徴点があって
かつどれぐらいのスケールで見ると
一番よりその点に対して情報量が多いかということを
同時に調べるアルゴリズムとして
処理としてこのDOGという方法を提案しています
ではDOGがどうやってできていくのか
どうやって計算するのか説明をしていきます
まずDOGはどういうことかというと
DOGはDifference of Gaussianですから
ガウシアンの差分、差をとるというわけです
ガウシアン、いわゆる平滑化画像は
ガウシアンフィルター、このGはXとYとシグマ
これガウス分布ですよね
このガウス分布を画像にかけたものが
平滑化画像になりますね
この平滑化画像の度合いを決めるのがシグマでした
例えばシグマ0だとこれぐらいの平滑化度合いの画像
そしてシグマを軽倍大きくして平滑化をすると
このような平滑化画像を計算することができます
そしてシグマの大きい、軽シグマで平滑化した
こちらの画像からこっちのシグマ0で
ここは今こっちはシグマ0ですね
シグマ0で平滑化した画像の差分を計算する
これがDOGとなります
実際にこの画像からこれを引き算するわけですから
結果このようなDOG画像を計算することになります
実際にLogオペレーター、ログフィルターをかけると
この画像にログオペレーターをかけると
このような結果になります
非常にこれらが同じような結果になっている
すなわちログと同じような効果を
DOGで得ることができるということがわかると思います
このDOGという処理を1回するだけではなく
何度も処理を行います
どのように行うかというと
平滑化するためのパラメーター、シグマか初期値をシグマ0とし
これに少し軽倍した大きなスケールで平滑化した画像があります
さらに軽を軽倍した軽次乗シグマというもので平滑化した画像
さらに軽の三乗シグマで平滑化した画像
なのでこの縦軸方向にはスケールの値をどんどん大きくしていって
それぞれの平滑化画像をこのようにまず作ります
そして DOGは平滑化した画像間の大きい方から小さい方の
大きい方から平滑化した画像間の大きい方から小さい方の差分をとりますので
この画像とこの画像の差分をとる
次はこの画像とこの画像の差分をとり
続いてこの画像とこの画像の差分をとります
連続して変化するスケール感で
DOG画像をこのように作成します
このようなガオシアンフィルターの平滑化画像の計算をするときに
気をつけなくちゃいけないことは
より大きなシグマ
こういうシグマから
だんだんこういうシグマになっていくわけですけども
大きなシグマのときは
そもそもフィルターが大きくなるので
画像全体に平滑化処理をするとかなり時間を要します
さらに画像の周辺には
このフィルターが大きくなると
処理できないという領域も出てきたりするわけです
そこでこのDOGの処理では
シグマの連続性を保持した平滑化処理を
効率よく計算するために
ダウンサンプリングと平滑化処理を
うまく使って実現します
まずどうするかというと
入力画像と同じ解像度において
まずシグマゼロで平滑化します
そして続いて
軽倍したスケールのシグマで平滑化をします
続いてもっと実はここもっと細かくやるわけですけども
2シグマまでなんとか計算できるということで
2シグマまでシグマゼロから2倍したところまで
平滑化した画像を作ります
実際はここにも間がまだ
画像がいっぱいあると思ってください
ここは元の画像と同じ解像度で作っていますから
何ら問題ありません
問題はこの2シグマより
より大きなガウシャンフィルターで
平滑化をしようとすると
計算時間と問題が出てくるというわけです
そこでどうするかというと
シグマゼロから2シグマゼロまで
平滑化画像を作成したら
入力画像を半分にダウンサンプリングするわけです
このダウンサンプリングした画像に
シグマゼロで平滑化を行います
ここと同じパラメータですね
このダウンサンプリングした画像に
シグマゼロで平滑化するということは
平滑化度合いの効果としては
どういうことになるかというと
入力画像の同じ解像度において
2シグマで平滑化したものと
ダウンサンプリングして
シグマゼロで平滑化したものは
もちろん画像の大きさは違いますけども
平滑化の度合いとしては同じになります
フィルターを大きくした場合
2倍に大きくした場合と
フィルターをそのままで
画像を半分にしたわけですから
平滑化の効果は同じですよ
ということになるわけです
なのでこのダウンサンプリングした
画像に対して
シグマゼロから軽シグマゼロで
いろいろ計算して
2シグマゼロまでここで計算をするわけです
これを2オクターブ目という風に呼びます
続いてもっとさらに
さらに大きなシグマでの平滑化を
計算したいので
今度は入力画像を
4分の1に以降ダウンサンプリングして
そしてまた同じように
シグマゼロで平滑化してあげます
そうするとこれは
3オクターブ目で
シグマゼロで平滑化したものは
2オクターブ目では
2シグマで平滑化したものと同等で
かつそれは元の画像では
4シグマで平滑化したものと
同じ効果ですよということになります
この3オクターブ目も
同じようにシグマゼロから
2シグマゼロまで平滑化をすることで
結果シグマゼロから2シグマ
ここ同じ効果ですから
2シグマから4シグマ
そして同じ効果ですから
4シグマから8シグマということで
非常に大きな平滑化度合いを得られるような
シグマのパラメータ
4シグマのパラメータによる
より平滑度合いの高い画像までを
このように効率的に計算することができるというわけです
実際にその平滑化した画像を見てみましょう
元の画像に対して
2シグマで平滑化しました
そしてダウンサンプリングしたものを
また2シグマで平滑化すると
これは原画像の解像度において
4シグマで平滑化したことと
同じ効果があるわけです
さらにダウンサンプリングして計算すると
8シグマで平滑化すると
かなりボケているのがわかると思います
このようなボケを入力画像と
同じ解像度で計算しようとすると
どうしても時間がかかってしまう
このようにシフトにおける
DOG処理においては
ダウンサンプリングを併用して
シグマ0から8シグマ0という
非常に大きなスケールまでの
平滑化画像を作成しています
では続いて特徴点である
キーポイントを検出する
処理について説明します
今何ができたかというと
シグマ0から2シグマ
元の解像度における
シグマ0から2シグマ0までの
効果画像と
同じ値での2オクターブ目における
2シグマから4シグマまでの効果画像と
さらにまた4シグマから8シグマというのがあります
ちょっとスペースの都合上
そこは省略しています
ということで平滑化画像が
シグマ0から8シグマ0まで
たくさんの平滑化画像が
用意されたというわけです
これをスケールスペースという風に呼びます
スケールですね
それぞれの連続する平滑化画像間で
DOG画像を
それぞれこのように計算をします
そしてこのDOG画像から
ここの画像がこの画像を表します
このDOG画像のスケールスペースを使って
その画素が特徴点
いわゆるキーポイントであるかないか
ということを確認します
前ログフィルターにおいて
シグマを変化すると変わってたところの
この極大値をスケールとして取り出しますよ
ということをしたと思います
これと同じことを
画像空間とスケール方向にします
こちらはスケール方向がこっちでしたね
なのでそれがDOGの板
こちらの軸になります
スケール方向だけではなくて
空間的にそこの点が
他よりも大きいか小さいかということで
特徴的なキーポイントであるかどうか
ということを判定します
どうするかというと
この青色を注目画素とします
この青色の注目画素の
DOGの値と周辺の赤色の画素と
1つ上の9つの画素
さらに1つスケールの下の9つの画像と
比較をします
青のDOGの値と周辺の値との
比較をするわけで
極大値となるという
このようなスケールとなる点は
一番大きな点になっていますので
青の点が赤の金棒
これはスケール方向にも合わせた
26金棒となるわけですけども
26金棒と比較して
そこが極大値であるか
もしくは極小値であるかといったこと
極値であるというところを検出します
この操作を左上から順番に
ラスタスキャンしていきます
順番にやっていくということをします
このときもし
ここの画素に対しては
ここのスケールで
極大値となった場合は
同じ画素のスケール方向には
もう探索はしません
ここで一旦極値となれば
さらにその上のところは
計算はしません
何が言いたいかというと
下の方がスケールが小さいわけですから
ここではより点に対して
より小さなスケールで
より情報量の多いところを見つけたら
もし画像に対して
より良いスケールを考えると
画像全体を含むと
一番良くなってしまうわけですよね
なので小さいスケールから探していって
一旦極大値が見つかったら
そこでストップします
それ以上はここが極大値となった場合は
上には探さないという
このような操作をすることで
DOG画像から極値
まずどこにその極値があって
そのときのスケールも覚えておくわけです

このDOGの処理を2次元に考えると分かりづらいので、少し1次元で考えてみたいと思います。
今、縦軸が画素値、明るさです。横があるXという座標です。
今画像が白くなって、黒くなって、白くなって、また黒くなってというようなプロファイルを持つ画像が得られたとします。
その時に、まず初期値であるSigmaで平滑化を行います。
ここの点を中心にスケールを探索しますので、ここを中心となるこのような平滑化、ガオシャンフィルターを畳み込むわけですが、
これどういった処理をするかというと、結果、ここの点から見た時、
この画素関数の間に入るところの、この元々の持つ画素値の情報を通過させていることになるわけです。
ここでは実際に2という量が計算されたとしてください。
では、続いてSigmaを大きくします。
Sigmaを大きくすると、この元の濃淡の情報がより含まれるようになりますから、
当然ながら平滑化したガオシャンフィルターの値スロックは大きくなるわけで、
DOGは、よりSigma6で平滑化した値からSigma4で平滑化した値の引き算でしたから、
引き算をすると、DOGの値は10になるわけです。
続いてSigmaをもっと大きくします。
そうすると、ここが広くなったので、その分元の情報がよりスロックされるようになるわけで、
DOGの差分を取ります。
そうすると41になりました。
これをどんどんどんどん続けます。
さらにSigmaを16にすると、全体がより含まれていますから、一番大きな値に変わりました。
そこで差分を計算すると、47になります。
もっと大きくしましょう。
もっと大きくすると、全体が十分に含まれていますから、
103という値になります。
そこで差分をとると、3になるというわけです。
今度はこのDOGのスケールスペースから、
この値が極大値であるかどうかということをチェックしていきます。
そうすると、当然ここの時は3と41と比較すると、
47ですから一番大きな値、極大値となっていますので、
この時のスケール、すなわちSigma10、この点に関しては、
Sigma10の時によりその範囲、Sigma10ですとこういう形状のガウシアンですから、
だいたいこれぐらいの範囲により多くの情報を含まれていますよということで、
このスケールの値を求めることができるというわけです。
これがDOGにおけるスケール探索の仕組みとなります。
これを二次元的にいろんなところを操作していって、
極大値のところを探していくということをします。
実際に画像でこのDOGの処理をしてみました。
これAという文字の画像です。
そこでDOG処理をするといろんな点が出てきますが、
ここでは一つの点に注目します。
この真ん中の×の点です。
この×の点を中心にSigmaを変えたときのDOGの値をプロットすると、
このような変化となりました。
このときの極大値はここですから、Sigmaが5ということになります。
このSigmaが5である範囲を元の画像で表現すると、
ちょうどこの赤色の範囲に対応しています。
すなわち、Aという文字の上の領域のよりパターン、特徴を含まれているようなところを
ちゃんとSigmaが捉えているということがわかります。
ではですね、画像を2倍に大きくしました。
その画像に、2倍に大きくした画像に対して、
DOG処理を行います。
このDOG処理を行うと、Sigmaを変えていくと、
このようなDOGの値が計算され、極大値は10というのが得られます。
当然ながら、画像を2倍に大きくしていますから、
このSigmaの値もちゃんと2倍のところで極大値が得られたという、
この画像におけるSigma10に対応する範囲は、
この赤色で囲った円の領域です。
すなわち、小さな画像でも大きな画像でも、
同じ範囲を捉えることができているというのがわかると思います。
なので、であればどうすればいいかというと、
この円の大きさを一定の大きさにして、
こちらもこの円の大きさをこちらの同じ大きさにして、
それでこの2枚同士を比較すれば、
大きさの変化、いわゆるスケール変化に関係ない状態で、
同じものであるかどうかという対応点を求めるときに
使うことができるわけです。
これがスケールに対する不変性を得る、
このDOGによる効果になります。
よって、シフト特徴量はスケールに対して不変な特徴量を
計算することができるようになるというわけです。
続いて、ここまでの処理、DOGによってキーポイントの点と
そのスケールを求めることができました。
これは今、いろんな画像のいろんな場所で、
このような特徴点、キーポイントの候補が出てきます。
残念ながら、キーポイントとして向かない点も今検出されていますので、
次の処理でよりキーポイントとして好ましい点だけを
取り出すようなことをしています。
まず、先ほどのDOGの処理によって出てきたキーポイント候補点を
画像に表示するとこれぐらいになります。
1枚の画像から1895点のキーポイント点というものが
候補として、DOGからDOG処理によって計算できます。
この中から見てもらうと、
例えば、対応点を計算するためには、
その特徴点、キーポイントが非常にユニークな特徴を
持っていないといけない。
すなわち、似たようなものがあるようなキーポイント、特徴点は
対応点を求めるためにあまり向かないということになるわけです。
例えば、どういう点が向かないかというと、
こういうエッジの上に乗った点ですね。
エッジの上に乗った点は、
例えば黒白の領域でこういう点が出たとしますね。
こういう点は、例えばここの点でこういう風に出たとした場合、
どちらも同じようなパターンですね。
なので、似たようなものがたくさん1枚の画像の中に含まれているようなものは
ユニークな対応点には向かないわけです。
なので、このようなエッジ状の点は対応点に向かないので削除したい。
それが最初にやることです。
すなわち、エッジ状のキーポイント候補点は
対応点の計算に向かないので削除してあげます。
そうすると、この1895点が1197点まで減ります。
どのように削除するかはこの後説明します。
そしてもう一つです。
DOGのキーポイント探索をするときには、
周辺と比べて27キンボウですね。
実際は27キンボウと注目画素を比較して
注目画素が極大値か極小値であるかということで
キーポイントがそうでないかという判定をしました。
その周辺領域だけを見ると極大値、極小値が
かなりたくさんのところで出てしまいます。
よりはっきりしたところで出る場合はいいんですけども、
このようにテーブルが真っ白ですよね。
テーブルが真っ白だけどこういう特徴点が出てしまう。
このテーブル上の点は非常にコントラストが低い点ですね。
明暗さがはっきりしていません。
だけどキンボウ、本当に周りの三画素、スケール上と下の
3×3画素の領域を見た場合は、
やっぱり真っ白とはいえ同じ値を持っているわけではありません。
多少微小な変動があります。
従ってその微小な変動を見て
このキーポイント候補が出てしまうことがあるので、
それでコントラストが低いような点というのは
そもそも対応点に向かない点ですから、
ローコントラストのキーポイントはノイズの影響を受けやすいので
このように削除しましょうということをするわけです。
ではこの2つの処理によって
候補点の絞り込みを行います。
ではまず事情の点を削除します。
事情の点を削除するために
ここではヘッセ行列というものを使います。
ヘッセ行列というものは何かというと、
例えば先ほどの方法で、DOGで
推定したスケールのDOG画像があります。
そのDOG画像のある点を中心としたときに
X方向の2回微分、Xと方向で1回微分して、
次にY方向で微分したDXYと、
あとY方向の2回微分を計算して
このように表現する行列をヘッセ行列、
ヘッシアマトリックスというふうに呼びます。
このヘッセ行列の第1個誘致をα、
第2個誘致をβというふうにします。
このときαの値もしくはβの値を見ることによって、
このヘッセ行列から計算した第1個誘致であるαと
第2個誘致であるβの関係で、
そこの点がどういう点であるかという
3種類に分類することができます。
αとβがともに小さいところは、
X方向にもY方向にも
変化がほとんどないということを表しますので、
フラットの領域ですよという、
すなわち濃淡変動が、明暗の変動が
ほとんどないところですということが分かります。
さらにαとβがともに大きいところ、
ともに大きいところはコーナー点と言えるわけです。
これはどういうことかというと、
X方向にも変化が勾配が大きくて、
Y方向にも変化が大きいわけですから、
コーナー点みたいなところである可能性が高いと言われます。
ではエッジの点はどういう性質があるかというと、
エッジはある方向に対しては勾配が大きいですよね。
縦エッジであれば横方向に対する変化、
勾配が大きいわけです。
だけど縦に見ると、縦エッジの場合は変化がないわけです。
なので片一方の勾配が大きくて、
片一方の勾配が小さいような領域がエッジとなるわけです。
もちろんこれ逆回りです。
βが大きくて第1勾配が小さい領域、ここですよね。
こことここがエッジの性質を持つ点であるということになるわけです。
そこでそれをこのような式を使って表現しています。
こちらのヘッセ行列の対角成分の和、いわゆるトレースですね。
トレースはαプラスβで計算できます。
同様にヘッセ行列のこの行列式、ディターミナントはαβで計算できます。
この値を使ってこのような式を考えます。
この式はトレースはαプラスβ、ディターミナントはαβですからこのような式になります。
この式を見てみますと、ここではエッジであるかどうかということを判定したいので、
エッジであるとよりこの値が大きくなるように表現したもの。
例えばフラットの点もしくはコーナー点ですと、αとβの値が同じ値ですよね。
ほとんど同じような値を持ちます。
ということはαとβがアルファと似ていれば、下は2αになりますよね。
同じようにαプラスここがαになりますから、2αですから同じ値になります。
なので要は全体的にこの、自乗して割りますが、
全体的にαとβの値が同じような値を持つときは、この比率の値は小さくなるというわけです。
ここでエッジの点を考えてみましょう。
エッジの点はαもしくはβのどちらかが一方的に大きいわけです。
大きい値と小さい値です。
その掛け算をすると値は小さくなります。
小さい方に引っ張られますよね。
分母は小さくなる。
一方分子はαプラスβですから、片一方が大きければその事情を取りますからどんどん大きくなります。
従って分子は大きくなって分母は小さくなるので、
この式はもしその点がエッジであればあるほど、
この比率の計算した結果は大きくなるように計算されるというわけです。
実際に、これはハリスのコーナー検出という方法で使われている方法の
先ほど紹介した勾配を実際に計算したものです。
例えばこのような縦エッジの場合ですと、
横方向に勾配を計算するとここにエッジ成分が出ます。
Y方向の勾配は縦方向にはほとんど変化がありませんから、
ノイズが強調されたような結果が出ます。
フラットのところはX方向にもY方向にもともにエッジが出ないことがわかります。
勾配が小さいということがわかります。
コーナー点は同じようにX方向とY方向にそれぞれ勾配をとったときに
大きな勾配が計算されるということがこの結果を見てわかると思います。
なので今とらえたいのはこのような点ですね。
ここを捨てたいのでこのようなところを見つけてきている。
それをこことここから見つけてきたというわけです。
実際にこのX方向とY方向の勾配の分布を調べてみます。
そうするとフラットの領域はX方向の勾配もY方向の勾配も小さく分布していることがわかります。
一方このようなコーナー点はX方向の勾配もY方向の勾配も
より大きく両者にそれぞれ広く分布しているということがわかります。
同様に続いてエッジの点はこの場合はX方向だけに対してこういう分布が大きいということがわかると思います。
ちなみにこの分布の楕円形状がそれぞれ
ラムダ1、ラムダ2、これはどんな感じでしょうか。
こちらがラムダ1、ラムダ2となります。
これもこうなると広い方がラムダ1、こっちがラムダ2になるわけです。
これを見てわかるようにこの分布の場合は片一方の固有値が長形がかなり長くて単形との差があるというわけですよね。
これが先ほどの片一方が大きくて片一方が小さいという分布を表しているわけです。
はいでは続いて今度はこのようにして比率を計算したのでそこに式値を導入します。
式値を導入してその比率が式値よりも大きい場合はこれはエッジの点ですからそれは捨てちゃいます。
一方比率が式値より小さい点はエッジの点でない可能性が高いのでキーポイント候補点として採用するわけです。
もともと1895点あったこのようなキーポイントの候補点がこの処理を施すことによって
こういったエッジの点がなくなることによって1197点までこの場合は絞り込むことができたというわけです。
では続いてキーポイントのローカライズを行います。
DOGを作成した時にダウンサンプリングを併用していますので空間的な解像度が落ちているわけです。
そこで元の原画像の位置においてどこになるかということをサブピクセル推定と言いますけども
サブピクセル推定を行います。
どうやってやるかというと、DOGの関数がありますので、DOGの関数をテーラー展開して
そしてこの周りのDOGの観測した赤色の点があるのでそこから0となるようなこの極大点ですよね。
この極大点を求めてその極大点の座標XとYと
実際はこれシグマというスケール方向に対しても内装というか一番好ましい0となる点を求めます。
そうすることによってその各点ごとにキーポイントのXとYとシグマというものを計算し直すということをします。
その時計算し直したXとYとシグマにおけるDOGの値を求め、そのDOGの値が式値0.03よりも小さいときは
そこはノイズの影響を受けやすい点であるということになりますので削除するわけです。
そうすると1897点の点が421点に絞り込むことができるわけです。
この残った421点がシフトのキーポイント候補点として使われるということになります。
ここまでがキーポイント検出のアルゴリズムでした。

続いてシフトアルゴリズムの特徴記述について説明します
特徴記述においてはまず最初にオリエンテーションの算出を行います
オリエンテーションの算出とは
このような点とスケールで表現された中の領域を使って
代表的な方向、方位というものを求めます
さあではなぜこのようなオリエンテーションを算出するのかということなんですが
画像が回転していたときに
その回転していても必ず同じ方向を取り出すことができれば
その方向を正規化してあげれば
回転に不変な特徴量を算出することができるようになるわけです
そのためにはその局所領域における代表的なオリエンテーション
これでここでオリエンテーションと呼んでいますが
例えば方位、方向を算出することをします
さあではどのようにオリエンテーションを算出するか説明します
まずキーポイントの点とスケールが検出されました
そのスケールが検出されたときの平滑化画像の領域を拡大してみます
この緑色の四角形が一つ一つが画素だと思ってください
まず何を計算するかというと
この画素ごとに、この一つ一つの緑色の画素ごとに
勾配強度とオリエンテーションを算出します
勾配は隣り合う画素同士の差分で
x 方向の勾配と y 方向の勾配を算出することができました
したがって u プラス 1 から u マイナス 1 を差をとって
ここでは強度にするので自乗をとります
同様に v プラス 1 から v マイナス 1 縦方向ですね
縦方向 y 方向の差分をとって
それを強度にするため自乗して
それぞれ自乗したものを足してルートをとるというわけです
これがこの各座標ごとに、各画素ごとに
勾配の強度を求めます
そしてここでは上はこれは縦方向ですから
デルタ y で下はデルタ x を表します
すなわち x 方向の勾配分の y 方向の勾配を表していますので
これはその勾配の方向であるオリエンテーションを
アークタンジェントによって計算することができるわけです
これはどういうことかというと
デルタ x デルタ y ともに大きな場合は
こちらの方向ですよというシーターを計算することができるというわけです
この計算、勾配強度とこの方向であるシーターを各画素ごとに求めます
この矢印があるかと思いますが
矢印の長さがこの勾配強度で
矢印の向きがこちらのシーター、オリエンテーションを表しているわけです
続けて今度はこの各画素で計算した勾配強度とオリエンテーションを使って
勾配方向ヒストグラムを算出します
勾配方向ヒストグラムは何を表すものかというと
横軸が勾配方向を表します
オリエンテーションは0から360度まであります
ここではこのオリエンテーションを360度を10度間隔に区切ります
それ一つ一つをビンと呼びます
一つのこの枠をビンと言いますけど
ここでは360度を10度ずつ36方向に理算化しますので
36個のビン、受け皿があるという状態です
そこにこの計算した勾配強度にさらに重み付けしたもの
重み付けした勾配強度を該当するオリエンテーション方向に投票していきます
例えばここの画素に対して勾配強度と勾配方向を求めます
その勾配強度に関しては
勾配強度をここを中心としたガウス窓の値で
重み付けして勾配強度を計算します
これはなぜこのようなガウス窓を使うのかというと
この端の方の特徴と真ん中の特徴ではどっちが重要かというと
やはり真ん中の方がより重要であるということが言えます
この端の方に大きな勾配強度が発生すると
その端の方の勾配強度がこの勾配方向ヒストグラムに加算されますので
その影響を受けてしまいます
そこでより真ん中の情報を使いましょうということで
各座標の勾配強度にガウス窓をかけて
重み付けした勾配強度をここの画素の持つシーターに対応する便に投票するという
これをすべての画素について行うと
このような勾配方向ヒストグラムを算出することができるようになります
続いてこの算出した勾配方向ヒストグラムから
代表的なオリエンテーションを算出します
どのようにするかと言いますと
今、横方向が勾配方向で現在は36個の便に分けられています
縦軸は先ほどの重み付けした勾配強度を各画素に加算したものです
この中からどのように算出するかというと
まず最大となるところを見つけます
この最大値の値より80%以上上にあるところをですね
正確には80%以上となるピークですね
ここがピークここですね一つだけですから
ここをここのキーポイントのオリエンテーションとして割り当てます
ここの便は何の方向かということがわかりますので
こちらの方向ですよというふうに割り当てを行います
例えばこのようなコーナー点がキーポイントとして検出された場合
その勾配方向ヒストグラムはどのようになるか考えてみましょう
当然x方向とy方向の勾配が2つ強く出ます
なのでこのx方向とy方向の勾配のところに対応する2つのピークが出てくるわけです
実際にどのように計算するかというと
まず最大値を求めます最大値はここだとします
この最大値の80%以上のところを取り出すと
緑のところも含まれてしまうわけですが
先ほど紹介したようにピークのところだけを取り出します
この緑のところはピークではありません
従ってこの場合はこのこことここの2つのオリエンテーションが割り当てられます
ということはどういうことになるかというと
この局所領域によるオリエンテーションはこういうふうに2つの方向があるという
ゆくゆくはこのオリエンテーションを使って正規化をして特徴領を算出します
従ってShiftの場合は面白いことに
同一点で2つ以上のオリエンテーションが検出されたら
オリエンテーションごとに正規化をして特徴領を記述することになるわけです
従ってこの点においては2種類の特徴領が記述されることになるわけです
では続いて特徴領の記述について説明します
このようにここまでの処理でキーポイントの座標とスケール
そして勾配方向ヒストグラムからオリエンテーションが算出されました
続いてこれらの値を正規化して特徴領を計算します
この128次元の特徴ベクトルがShiftの特徴領となるわけです
ではまずこの算出したキーポイント点とスケール
そしてオリエンテーションに対してどのように特徴を記述していくか説明します
まず記述するためのウィンドウを作ります
この記述するウィンドウはスケールの大きさに合わせた大きさとなっています
そしてさらにこのオリエンテーションの向きに合うように
特徴記述するウィンドウを回転するわけです
そしてこの特徴記述ウィンドウを順番に特徴領記述することで
スケール変化と回転に対して
普遍な特徴領を記述することができるようになるわけです
ではここではどのように具体的に特徴領を記述していくか説明します
まず先ほどのこの向きのオリエンテーションを上向きにした領域をこのように表示しました
ここから特徴領の記述をします
まずこの緑の四角形一つ一つが画素に対応します
この画素はこの場合は1,2,3,4,5,6,7,8掛け8画素の領域になっています
この8掛け8という大きさはスケールによって決定されます
なのでより大きなスケールが選ばれた場合は
例えば64x64だとかという風にサイズ大きさが
画像の切り出されたウィンドウの画素数が異なるわけです
そこでこの領域を4x4の16個の1個1個のこのようなブロックに分割をします
そしてこの分割した領域この場合は
ここの2x2の領域がここに対応しますね
したがってこの一つのブロックに対応するこの局所領域の勾配情報から
また新たに8方向の勾配方向ヒストグラムを計算します
これは具体的にはこのように8方向の先ほどの横軸が勾配方向ですね
これが8方向ですからすなわち45度間隔のここに対応する勾配情報から
勾配方向ヒストグラムを算出します
そしてこの一つ一つがその特徴領になるわけです
これがそれぞれの方向の勾配強度を表しているわけです
これを次のブロックに対応する領域から計算して
それぞれすべてで16ブロックごとに8方向の勾配方向ヒストグラムを計算する
そして順番にまず左上の1つ目2つ目3つ目という風に値を記述して
次2つ目のブロックの順番に特徴値を記述していくわけです
そうすることによって4×4ですから16ブロック
そして一つのブロックには8方向の値を持ちますから
結果シフトの特徴量は128次元の特徴量となるというわけです
ただしこの値は勾配の強度を使っていますので
明るいくらいによって勾配の大きさが変わります
そこで最終的にはこの128次元の各特徴ベクトルの長さは
そのベクトルの相和で正規化をします
これがシフトの特徴量になるわけです
ではここまでシフトアルゴリズムのまとめです
シフトの処理を大きく分けてキーポイント検出と特徴量給付に分けられました
キーポイント検出の中ではまずDOGを使ってスケールとキーポイントを検出します
この処理によってスケールに対する不変性を獲得します
続いてエッジだとかローコントラストな候補点を削除しました
それがキーポイントのローカライズでした
これによって対応点に向かない点をなくなるので
ノイズに対する断片性を獲得することができるようになったというわけです
2つ目の処理である特徴給付ではまず最初にオリエンテーションを算出しました
特徴量を給付する際にオリエンテーションに合わせて
給付する空間を回転させますので
これによって回転に対する不変性を得ることができるわけです
最後に特徴量を積化してあげます
これによって軌道変化に対する断片性を獲得したというわけです
それぞれの処理によってこのような効果を獲得したというわけです
シフトアルゴリズムはこれまでの画像処理のアルゴリズムに比べて
やや難しかったかもしれませんが
それぞれの処理でどういったことを目的に
そのアルゴリズムができているかということを理解できるといいと思います
では続いてこの128次元の特徴量
これを特徴ベクトルと呼びますが
この特徴ベクトルをどのように使うか
実際に対応点を求める処理について説明します
これをキーポイントマッチングと呼びます
今ある画像においてこのようなシフトの特徴点キーポイントが得られたとします
画像2においては1,2,3,4,5個点のシフトのキーポイントが得られたとします
それぞれのキーポイントごとに128次元の特徴ベクトルを持つわけです
そのシフトの特徴量をVで表していますね
Kは何番目のキーポイントであるかということを表します
さあではあるキーポイントとあるキーポイントの228次元の特徴ベクトルに対して
128次元ですからそれぞれの値の差をとって
自乗したものをサンメンション、相和をします
そしてルートをとっているわけですから
これは128次元の特徴量間のユークリット距離を計算していることになります
したがってまずある画像1のキーポイントと
画像2のすべてのキーポイントとこのユークリット距離を計算します
そしてユークリット距離の値がこのように求まりました
ここで対応点としては距離が小さければ
その特徴ベクトルが似ているということになりますので
距離を見て一番小さなこの点、すなわちこことここが対応点ですよというふうに判定しようと思います
しかし実はそうではなくて
もう一つ対応点を求めるために
一番小さい点と2番目に小さい点を使って決定します
これはなぜかというと
小さいのが12で2番目に小さいのが13だったとします
そうすると値は小さくても
こちらの画像の中に似たような点が複数あるということになってしまう
似たような点が複数あるときは
これはユニークな対応としてはあまり好ましくありません
そこで先ほど計算した距離をまず
小さい順にソーティングします
ソーティングした後に2番目に小さい点の距離の値を持ってきて
この式の条件に一番小さい値が成り立つかどうかということを確認します
ということかというと一番小さいのは12ですから
ここに12が入ります
続いて2番目に小さいのが47になるわけです
47に0.6をかけた値よりも
一番目に小さい距離が小さければ
この場合はこの一番目に小さいこの点とこの点は
多様点ですよという風にしましょう
もしこの条件が成り立たなければ
すなわち一番目に小さい点と2番目の小さい点の距離が
あまり変化がない値が近いというわけですよね
その場合はそこは似たような点で
ご対応である可能性があるので
対応点として採用しないというわけです
ちなみにではこの値を比率、この係数ですね
この係数をどのように決定するのかというわけです
例えば係数を0.6と小さく設定をすると
このような対応点が得られます
もちろん0.6と小さくすると
一番目の距離と2番目の距離が離れていないと
一番目の距離の点同士を多様点として求めることができませんので
対応点の数は当然ながら少なくなります
ただし、一つ一つの対応点を見ていただくと
より確からしい対応点になっていますので
すなわち誤った対応点が少ないという効果を得ることができます
これは一般に画像認識とかに使うときは
このシフトの特徴量を画像認識に使う場合に
このようなより小さな値で厳しく対応点をチェックして
もっと使うことがあります
じゃあ先ほどの係数の値を0.9と大きくしました
0.9とすると
1番目に小さい距離と2番目の距離が
あまり差がなくてもいいですよということになりますので
このように結果、対応点がたくさん増えます
こちらと比較すると対応点がたくさん増えているのがわかると思います
しかし、中には残念ながらこの点を見てわかるように
誤った対応点も含まれるということになるわけです
このような値をなるべく大きくして
より多くの対応点数を得るということは
どういったところで使うかというと
画像間のレジストレーションに使います
例えばイメージモザイクをやりました
イメージモザイクはある画像とある画像の
特徴点を対応付けして
変換行列を求めるというものでした
この変換行列を求めるときに
画像全体からこの対応が取れていないと
画像全体を表す変換行列を
うまく求めることができません
もし厳しくしてしまうと
部分的なところしか対応点が得られないので
より好ましい変換行列が求められないわけです
そこで画像のレジストレーションを
使うような用途の場合であれば
このように値を大きくして
より多くの画像全体から
対応点が求められるようにしておくというわけです
このように用途によって
この係数の値を設定します
では続いて実際にシフトが
どれくらい回転、スケールの大きさ、軌道変化、
遮影変化に効果があるか
といったことを調べてみましょう
今この画像に対して
それぞれの画像と対応点マッチングを行います
まず回転変化に対する
シフト特徴量の変化を見てみましょう
この画像でシフトのキーポイントを算出すると
この点でこの大きさで
このオリエンテーションのキーポイントが得られました
他にもたくさん出るわけなんですけども
この一つに注目をします
そしてこの画像が45度回転した画像に対しても
同様にシフトを算出します
そうすると同じ点で
だいたい同じような大きさで
ちょっと異なるオリエンテーションが
算出されたというわけです
この場合、上の元の画像においては
このようなスケールでこのオリエンテーション
下の45度回転した画像からは
このようなスケールでこのようなオリエンテーション
これを見てもらう上と下を比較してみると
ほとんど同じ大きさ
すなわちスケールがちゃんと得られているのは
わかると思います
ただしこのオリエンテーションの方向が
ちょっと異なるというのが
わかるんじゃないかなと思います
上の方向を下で描くとこんな感じですかね
なのでオリエンテーションの方向が
少し変わったということがわかると思います
実際にそれぞれで算出したスケールと
オリエンテーションで正規化して
特徴量を記述すると
このような特徴量になります
このような特徴量間の
ゆっくりと距離を計算したら
この場合は42.6という距離になりました
これは後で比較に使います
続いてスケール変化における距離です
こちらのシフト特徴量に対して
画像を2倍大きくしたところから
シフトキーポイントを算出すると
やはり同じような点が出ました
ただしそこの領域を見ると
こちらに対してやはり2倍大きくしているので
2倍ぐらい大きいスケールが算出されています
オリエンテーションは回転が起こっていませんので
大体同じような方向を向いていることがわかります
それぞれの領域からシフト特徴量を算出しました
形状が非常に似ているのがわかると思います
実際にゆっくりと距離を計算すると
27.3という値になります
続いて軌道変化です
これは画像が全体的に暗くなったときです
この場合は同じスケール
同じ向きのオリエンテーションが算出され
それぞれシフト特徴量がこのような形を持ちます
ほぼ同じような形状を持っていることがわかります
したがってゆっくりと距離は7.8と小さくなります
続いて遮影変化です
遮影変化というのはどういう変化かといいますと
皆さんのカメラに対して
こういうのは拡大縮小ですね
こちらが回転になりますね
遮影変化はこのような変化が起こることを
遮影変化というふうに呼びます
したがってこのような遮影変化が起こると
似たような点が検出されているのですが
このスケールの中に遮影変化によって
異なる上とは異なる領域が含まれることになります
したがって異なる領域が含まれるため

その領域の特徴量は当然、上とは異なる特徴量になってしまいます。
したがって距離は149ということで大きくなるというわけです。
ではこれまでの回転変化、スケール変化、軌道変化、遮影変化における
全体で対応点を求めた結果を表示しています。
回転はたくさんの対応点が得られています。
スケールもうまく対応点が求まれていることがわかると思います。
軌道変化も問題ありません。
遮影変化です。
遮影変化は先ほど説明したように、
遮影変化するとスケール領域に異なる領域が含まれてしまうので、
結果、対応点がうまく求めることができません。
シフトができるのは回転変化、スケール変化、軌道変化が生じても
似たような特徴量をちゃんと計算してくれるわけです。
遮影変化に対してはうまくいきません。
ゆっくりと距離を比較しましょう。
ゆっくりと距離を見ると、
軌道変化、スケール変化、回転変化は小さな値が出ていますし、
より多くの対応点を求めることができていることがわかります。
残念ながら、遮影変化においては距離が大きく、対応点が少ないことがわかります。
何度も言いますが、シフトは軌道変化、スケール変化、回転変化に
元件な特徴量を算出するアルゴリズムです。
遮影変化に対して元件な特徴量を算出するわけではありません。
その違いを理解しておきましょう。
ちなみに、遮影変化とアフィン変化の違いを説明します。
アフィン変化は、この3点で変形を表現できるものをアフィン変化と言います。
一方、遮影変化は、この4点の変化によって表現されるものを遮影変化と呼んでいます。
シフトでは残念ながら、この遮影変化に対応することはできていないというわけです。
では、シフトがどのように実際に画像認識に使われるか見てみましょう。
これは我々が作ったデモンストレーションの一つです。
大きな画像に対して、こちら小さな画像を入力しています。
小さな画像、パッチ画像ですけども、これが大きな画像のどこにあるかといったことを
多様点マッチングした結果から認識しています。
ジグゾーパズルではないんですけど、ジグゾーパズルみたいなことをしていると思ってみてください。
これをですね、見ていただくとよくわかるように回転してますね。
入力が回転していても一度も間違いません。
また、この画像が出てきてわかるようにミッキーマウスがたくさん映っています。
このような画像においても、どこであるかということを一切間違えることなく
シフト特徴領によって対応点を求め、それがどこにあるかということを見つけ出すことができるわけです。
これはこれまでこの講義でやったテンプレートマッチングを考えてみましょう。
これをテンプレートとしてだーっと探すということを考えると、
似たようなものがかなり中にいっぱい含まれていますから難しい問題ですね。
しかも回転も含まれています。
ですが、このシフトを使うとこれぐらい眼鏡に入力した画像が
この大きな画像のどこであるかということを正確に認識することができるというわけです。
その例になります。
このように今度は具体的にシフトを使った画像認識として
特定物体認識について紹介します。
特定物体認識というものは何かというと、テンプレートです。
ここではこのようなテンプレートがあって、
これらのテンプレートが入力画像のどこに写っているかということを知るということが特定物体認識です。
まず何をするかというと、
シフトの特徴点をそれぞれ計算して多様点マッチングを行います。
多様点マッチングをした後に、その多様点の関係からどのように変化しているか、
このパターンが多様点マッチングの多様点の関係から
どのように変形しているかといったことを求めます。
これはアフィン・パラメータになります。
そうすることによって、それぞれのテンプレートがこの画像中のどこに写っているかということを知ることができるわけです。
ここでこの方法の良い点は、このテンプレート画像は一部分が隠れていますよね。
隠れています。隠れちゃっているわけです。
隠れていてもこのようにちゃんとこの物体を認識することができるというわけです。
ではもう少し詳しく説明をします。
入力画像があると、まずテンプレート画像が一つ目のテンプレートと多様点マッチングを行います。
そして多様点マッチングした結果からこのアフィン・パラメータ、変換するアフィン・パラメータを求めることで、
この領域画像がどこにあるかということを知ることができる。
同様に二つ目のテンプレートに対しても多様点マッチングをしてアフィン・パラメータを求め、どこにあるかを認識します。
三つ目のテンプレートについても同様です。
このような処理を行うことで特定物体認識を実現することができます。
これは我々が共同研究である企業さんと共同研究でやった成果です。
このシフトを使って交通道路標識の認識を行いました。
これ、線がたくさん引っ張ってあると思いますけれども、これがシフトの多様点を表します。
いろんな標識と入力画像のシフトの特徴点の多様点を求め、
そしてその多様点が多く見つけられたものがその標識というふうに認識しているというわけです。
ただし標識は画像の中で小さくしか映りません。
なので先ほどの特定画像の特定物体の認識のように大きく映っていないので、
多様点の数が少ないという問題があります。
そこで我々が考えたアプローチはシフトの多様点を求めます。
シフトの多様点を求めた後に今度は1点の多様点の関係から
このテンプレート画像の基準点がどこにあるかということを投票したわけです。
この時スケールと回転というオリエンテーションがありますから、
1点の多様点が求められればこのスケールとオリエンテーションの関係から
基準点が画像中のどこにあるかを投票することができる。
1点1点の多様点から投票することができるので、
この画像中に多様点が少なくてもより投票数を稼ぐことができるわけです。
これによって先ほどのように物体が小さく映っていても
ちゃんと認識ができるようになったというわけです。
他にはどういったところで使われているかというと
類似画像の検索に使われています。
皆さんAmazonのアプリを使ったことありますか?
Amazonのアプリではカメラを使って本を検索することができます。
このような形です。
まずその本をカメラで写すと多様点を求め、
そしてその多様点からその本は何であるかということを
このように表示してくれます。
したがってその本がここの下に出てきているように
Amazonではいくらで売ってますよという情報が得られますので
そこでAmazonで注文することができるというアプリです。
こういったところにも画像の認識、
しかもこのようなシフトみたいなアルゴリズムが
実際に使われているというわけです。
他の例は前回紹介したようにイメージモザイクです。
このように画像をそれぞれバラバラに撮影します。
このバラバラに撮影した画像間で多様点を求めます。
そしてその変換行列を求め、画像を変換してつなげることで
1枚の大きな画像になるわけです。
これは実際に黒板を使って授業をやっていたときに撮影した例です。
このように1枚目、2枚目、3枚目の画像を撮影します。
当然そのときに部分的に重なりを持つように撮影してください。
そしてこのシフトを使って多様点を求めつなぎ合わせることによって
このような1枚のモザイク画像を作ることができます。
それぞれに僕が写っていますからこのように
ちょっと薄くなっていますけども
1枚の画像に同一人物が3人写ったような形で
1枚の画像として表現することができます。
これはぜひここのサイトに
AutoStitchというソフトウェアがダウンロードできます。
Windows、Macそれぞれで動きますので
ぜひ皆さん実際にやってみてください。
では今日はここまでです。
これまでシフトの特徴点検出と記述について説明をしました。
これまでの画像処理のアルゴリズムに比べて
かなり難しかったかもしれません。
しかしそれぞれでどういったことをしているのかということを
理解するようにしてください。
このシフトのアルゴリズムが
どういったことをするためにどういった処理があるのかということの
関係性が理解できれば
画像処理全般をですね
ちゃんと理解できたと思っていただいていいと思います。
それではまた。

みなさんこんにちは 今日はコンピュータービジョンについて紹介します
この講義の最初にロボットビジョン、コンピュータービジョンを実現するためには
まず画像を取得して処理をして認識をしてそのシーンを理解することで
ロボットビジョン、コンピュータービジョンが実現できるという話をしました
今日はこのここまで学んできた画像処理の次であるコンピュータービジョンという世界が
どういうものであるのかということを紹介したいと思います
まずコンピュータービジョンにおいてまず画像とは何かということを考えましょう
画像は何らかのこのような3次元のシーンを撮影し
人が見る色明るさを保存したものでした
これはこれまでの講義の中でやってきました
そこで画像からこの画像からこのシーンである風景の情報を獲得
もしくは推定する技術をコンピュータービジョンと呼びます
逆に何らかのシーンを想定した画像を作成、生成する技術をコンピューターグラフィックスというふうに呼びます
したがってコンピュータービジョンとコンピュータグラフィックスは逆のことをやっている関係になるというわけです
コンピュータービジョンの起源はいわゆる人工知能AIやロボットの視覚部分、目の部分ですね
この機能を扱う問題として研究が行われてきました
このコンピュータービジョンもいろんな研究があります
例えば認識という関係では、そこで映っているシーンがどういうものであるかというシーン認識、文字認識、パターン認識
もしくは動いているものを認識する移動体検出、物体認識などいろいろあります
認識とは異なって、そもそもそのシーンの3次元がどうなっているかということを知る
3次元計測、モデリング、キャリブレーション、スラム、ストラクチャーフロンモーションといった分野の研究もあります
その他には観測対象の物体の表面の反射特性を推定するという問題
例えばこういうスマートフォンだと硬いですよね
光が共鳴反射するようなテクスチャーだとか材質だとか
もしくはこのようにちょっと柔らかいもの、この材質を求めるためにこういった反射特性を推定するという研究
同じように環境の光源、テクスチャー解析とか、いろんなコンピュータービジョンという分野の中には研究が行われています
技術的、理論的には信号処理、コンピュータグラフィックス、VR、AR、AI、ロボット工学などと一部重複して関連しており
基本的に画像に関する技術はすべてこのコンピュータービジョンという分野に含まれるということになります
ロボットを見てもロボットが入力するセンサーはいろいろあるわけですけれども
やはり画像から得られる情報は非常に多いので
このコンピュータービジョン、すなわちロボットビジョンというものがロボットにおいても重要とされているわけです
このコンピュータービジョンのまず3次元の計測ということを紹介します
3次元を計測する技術自体もコンピュータービジョンの研究になります
その中にも能動的な方式と自動的な方式があります
能動的な方式の中にはタイムオブフライト、光切断法、モーションキャプチャーシステム、構造加工投影法、
回れパターン法、フォトメトリックステレオ法などいろんな方式が提案されています
最近、とても多くの方が注目しているものがタイムオブフライトと言われる方法です
このiPhoneのカメラのところに実はこの能動的なタイムオブフライト形式の3次元計測が実は使われたりしています
一方で、能動的でない自動的な方式がありまして、それらをShapeFromXと呼ばれます
このXにはシェーディング、フォーカス、リフォーカスといったいろんなものが当てはまります
後で紹介します
あとはステレオ法、ストラクチャーフローモーション、動きから形状を獲得するといったような
自動的なこういった方法が研究されているわけです
ではまずステレオ法について説明します
我々人間も左目右目を持っています
この時に左目と右目があることによって
左目と右目で見たシーン、画像が少し異なるわけです
この異なる、どれくらいずれているかといったことを
視差という言葉で表現します
その視差を計算することで
例えば右と左の同じところが視差が大きいとずれていると手前にあって
視差があまり少ないと遠くにあるということがわかるわけですね
よってこの2枚の画像からこのような結果を出力することを
ステレオ法を使ってこの結果画像を作ります
もう少し詳しく説明しましょう
ある左の画像と右の画像を取得します
この時にこの画像はそれぞれ3次元空間のある点を
こちらに写像したものがこの画像上で観測できるわけです
なのでこの点とこの点が同一の点ですよという対応点が求まれば
カメラがどこに設置してあって
どういう風に姿勢が向いているのかということがわかると
そこからステレオ法を通過して
伸ばした線とこことここを結ぶ直線が交わる点が
すなわちこの対応点の3次元座標という風に求めることができるわけです
この対応点のズレの量というものを
画像上のズレの量を視差という風に呼びますね
この場合2つに見ている視点が異なるので視差が発生し
その視差を使って3次元値を求めることができるわけです
ちなみにこれは2台のカメラで同時に撮影するということもありますし
ある時刻で撮影してそれからカメラを移動してまた撮影しても同じです
カメラを移動させた場合を運動視差と呼びます
このステレオ法を拡張した方法が今多く取り組まれています
ステレオ法は左のカメラと右のカメラで同時に
しかも固定した状態で撮影してそこから対応点を求め
ステレオ視によって3次元形状、3次元情報を獲得しています
一台のカメラを時系列に動かして撮影することをした場合
この2時刻間、ある時刻とある時刻のこの2つの時刻間で
対応点を求めていくことによってこの3次元形状と
このカメラがどのように移動したかを同時に復元することによって
形状とこのカメラの座標がどう動いたかということを知ることができる
こういった研究をストラクチャフログモーションという
動きからストラクチャを求めるという問題です
さらには異なる時刻でいろんな人が撮影したカメラから
その場合これらのカメラがどこでどう撮影したかが分かりません
だけど複数の人が同一対象を異なる時刻で異なる場所で撮影したものから
この3次元の情報を獲得する方法をバンドル調整と言われています
ただしこの問題はこちらのストラクチャフログモーションの
時系列がバラバラになった状態と同じですので
これも一種のストラクチャフログモーションというふうにも呼ばれています
このストラクチャフログモーションはいろんな原理理論
基本的にはこのストラクチャフログモーションは同じ原理理論なんですけども
分野とか目的によって実際にそのアプローチ
少し呼び方が異なったりします
例えばコンピュータービジョンという分野ではストラクチャフログモーションと呼ばれますし
我々のロボット分野であればビジュアルスラムというふうに呼ばれたりします
他にはAR、オーグメンティッドリアリティの分野ではマーカレスAIだとか
コンピュータービジョンの分野でもマッチムーブという技術として使われているのが
このストラクチャフログモーションという技術になります
一般的にはVスラム、ビジュアルスラムという呼び方が最近はですね
よく呼ばれたりしています
ではこのスラムをちょっと紹介しましょう
スラムというものはサイマルテイニアスローカライゼーション&マッピングです
サイマルテイニアスというのは同時にという意味です
すなわちローカライゼーションとマッピングを同時にします
すなわち自分がどこにいるかということと合わせて
そのマップを作っていくということを同時にしてくれます
ちょっと見てみましょう
これは今ロボットが観測している領域が見えています
その観測している領域では全体の地図は分かりません
だけどそのロボットが移動していくと見える範囲が変わっていくわけです
その時に以前のところで獲得した地図と今の観測した結果をですね
つなぎ合わせていくことで
自分の自己位置がどこにあるかということを知ることができる
なのでロボットがこうやって移動していくことによって
新たな場所で情報を獲得して
その情報を今までの地図情報につなぎ合わせていくことによって
広い地図を作ることができます
これをスラムというふうに呼びます
ちなみにこのスラムでは大きな問題があります
今観測した情報を前作った地図情報につなぎ合わせていくわけですね
なのでつなぎ合わせる時に誤差が生じると
どんどんそれに合わせていきますから
誤差がどんどんどんどん蓄積するという問題があります
そこでこういったスラムというものでは
ループクロージャーという技術を使います
どういうことかというと
つなぎ合わせていってまた同じところに戻ります
同じところに戻った時に本来であればズレがあるわけです
ここを見ていただくと
こことここは同じ廊下なんですけども
ずれているのがわかります
ただしずれているんですが
画像上で対応点を求めた時に
これは以前求めた廊下だということがわかれば
そこで全体的にもう一度最適化を行います
そうすることによってより正確な
今ですねこのようにより正確な
このような地図を求めることができるようになるというわけです
これもスラムというコンピュータビジョンの技術になります
このスラムを画像だけでいろんなセンサーを使うんですけども
例えば車の場合であればどういうものを使うかというと
こういったIMUというセンサーから得られる
車の速度とか加速度等を使って
自己位置がどのように動いているのかということをですね
使って使うこともあります
さらに画像のみからスラムをしましょうというのが
ビジュアルスラムになります
このビジュアルスラムをやるときに重要となるのが
前回やったShiftの対応点マッチングですね
Shiftの対応点マッチングをすることによって
異なる画像間の対応点を求めることができるわけです
この対応点を複数の画像間で求めることによって
画像を使って自分の位置と地図ですね
この場合地図というのは
3次元形状を求めていくことができるようになります
では実際にビジュアルスラム
画像によるスラムの例を見てみましょう
この例では車載カメラ
車に積んだカメラ映像画像のみから
このように特徴点を出して
時系列間で対応点を求めることによって
この下のように地図をどんどん更新していきます
したがって車にカメラを設置するだけで
フレーム間、時系列間の対応点から
車がどのように移動したのか
そしてその周りの対応点の
3次元情報を取得することができるわけです
当然1フレーム前と次のフレームの位置が分かれば
そこからステレオ使用することによって
このように自分の動きと
さらにその周りの情報を地図として
3次元地図として獲得することができる
このような技術は最近ですと
こういったHMDの中でも使われています
この例をちょっと見てみましょう
これはマイクロソフトが出している
HoloLensというヘッドマウントディスプレイです
ここには、見てもらったら分かるように
いろんなカメラとかセンサーが付いています
このセンサーから見えている
3次元形状をこのように獲得することによって
その3次元情報に今、手の形状を当てはめることができる
手がどのような姿勢になっているのか
どういう操作をしたいのかということを
このヘッドマウントディスプレイで認識することができる
それに合わせてこの周辺に
こういった3次元のものを
オーグメンティットリアリティとして
CGで見せることもできるし
擬似的に触って操作しているようなことが
こうやってできるようになってきた
ぜひこのHoloLensは
いろんなコンピュータビジョンの機能が詰まった
デバイスになっていますから
いろいろ試してみるといいと思います
面白いです
他には、VisualSlamの例としては
複数の人が撮影した画像から
どのように3次元形状を復元しているかという
研究に取り組んでいる
ワシントン大学の例を紹介します
この人はスティーブン・サイツと言いまして
今はワシントン大学の先生なんですけど
その前にカーニギメロン大学で研究者をやっていました
その当時、僕も知り合い
その時に知り合った研究者です
彼らは何をやっているかというと
こういう有名な観光地があります
そこでいろんな人が撮影した画像が
アップロードされています
なので、インターネットにアップロードされている画像を
集めてきて、その画像から
多様点マッチングをして
そしてそれぞれのカメラが
どこから撮影したものか
かつ、合わせてその3次元形状を復元するというものです
そうすると、このように
複数の画像をいろんな人が撮影しました
そして、その対応点から
それぞれの画像がどこで撮影したのか
かつ、その対応点が
3次元的にどこに位置するものか
ということを求めることによって
このように、それぞれのカメラ視点から見た
位置情報が分かるので
それぞれの位置から見た画像と
かつ、このような大規模な3次元の形状のモデリングができるわけです
不動意ですけども、この3次元は
そこ行って誰かがセンサーを使って計測したのではなくて
観光した人が撮影してアップロードした画像から
こういう広い範囲の3次元形状を
復元したというわけです
非常に面白い研究だと思います
さあ、もう少し具体的に役に立つ例も紹介しましょう
最近のGoogleマップの
ARナビゲーションというのがあります
これはGoogleストリートビューと
うまく連動したアプリ、サービスになっています
皆さんも例えば、どこか知らない土地に行った時に
地下鉄を使って
地下鉄から出たとします
その瞬間、ナビは地図上で
こっちに行けってあるんですけども
そもそも自分自身が地下鉄から出た時に
どっちの東西南北を抜いているかというのが分からないので
地図上でどっちに行けば分かっても
自分のいる、立っている、向きから
どっちに行くのかというのを
知ることがよくできないですよね
よく分からなくなることがあると思います
そういった時にGoogleは
Googleストリートビューと言いまして
世界中のストリートの画像を持っていますね
ですかつ、このカメラ、いわゆるスマートフォンで
今の自己位置はGPSで撮りますけど
どっちを向いているのかということで
自分の位置だけでなく、そのカメラを向けている
この向き、姿勢をカメラ画像から
Googleストリートビューのデータと
マッチングすることで位置情報を獲得しています
ちょっとこれも見てみましょう
このように地下鉄から出てきた時ってよく
あれ、右左どっち行くんだろうってのは分からないですよね
これは東西南北になっていますから大丈夫なんです
こういった時にこのGoogleの
GoogleマップAIナビゲーションではカメラを使うわけです
カメラを使うことでそのカメラ情報と
Googleストリートビューのデータと
マッチングすることでこっちに行ってくださいっていうのを
教えてくれる、これ結構便利な機能だと思います
わかりますか、こうやってカメラの向きを変えると
地図の向きも連動して変わっています
このようにコンピュータービジョンは
皆さんの生活の中でいろんなところで
今使われようとしている技術になるわけです
今日はですね、画像処理の次に行われる処理である
コンピュータービジョンという分野の
研究をザーッと紹介しました
コンピュータービジョンの分野の研究は
まだいろいろいっぱいなされています
ぜひこういったコンピュータービジョンの技術を
高めることによって
ロボットにとってより良い視覚機能を
実現していきましょう
それが僕の研究でもありますし
その研究をですね
今一生懸命取り組んでいるというわけです
今日はコンピュータービジョンについて紹介しました
それではまた

みなさんこんにちは。今日は我々の研究グループマシンパーセプション&ロボティックスグループの研究紹介ということで
アフィン不変なキーポイントマッチングについて紹介します。
今日の内容はNANAのところでやった特徴点検出・記述におけるシフトの問題点を解決するような研究です。
キーポイントマッチングにおけるそれぞれの画像間の変化においてどういったことを考えないといけないかといったことをまず振り返りましょう。
まず画像間に回転変化が生じた場合は特徴点として点を求めれば良いわけです。
続けて画像間に拡大縮小が起こった場合はこの場合は点だけではなく領域を求める必要があります。
シフトはこの回転変化とスケール変化に不変な特徴量を検出・記述することができます。
しかし遮影変化についてはアルゴリズム上対応できていませんでした。
そこでこの遮影変化に対応するにはどうすればいいかといったことを今日は扱います。
この遮影変化に対応するためにはこのように楕円領域を求め、その楕円領域を一旦深淵に正規化してから特徴量を記述するということが重要となります。
回転変化に対してはキーポイント検出、スケール変化に対してはキーポイントの座標と東方性のスケール推定を行っています。
そして遮影変化に対応するためにはキーポイント検出とさらにアフィン領域、ここでいう楕円領域を推定する必要があるというわけです。
この表はこれまでのいろんな研究がキーポイント検出に関わるいろんな研究を年表という形でまとめたものです。
古くはLOG、そしてSHIFTが1990年代の後半に提案されてからいろんな手法が出てきたというわけです。
まず一番下は回転変化に対応する特徴点のみを検出するアプローチです。
続いて真ん中の段は回転とスケール変化に対応するための東方性にスケールをどのように推定するか、
すなわちスケールをどのように求めるかというアプローチがこのようにいろんな研究がなされてきたというわけです。
そして遮影変化に対応するためにアフィン領域の推定としては2002年にMSER、2004年にヘシアンアフィンという2つの方法が提案されたのみだけであって、
残念ながら2004年以降にこのアフィン領域を推定するための手法という新しい手法が提案されていないということがわかります。
これが逆に言えばいかにここが難しい問題設定であるかということがわかると思います。
ではまず従来法であるこのヘシアンアフィンがどうやってアフィン領域を推定するかについて紹介をしたいと思います。
ヘシアンアフィンは1つのアフィン領域を探索します。
そのキーポイント周辺の二次モーメント行列より固有値を計算します。
この固有値を使って領域を深淵に、楕円を求めます。
その楕円を深淵となるように繰り返し処理していくことで楕円領域を推定していきます。
まずこのキーポイント点に対する東方星のスケール領域の中から二次モーメント行列より固有値を計算します。
そして楕円の領域を推定していくことをどんどん繰り返していくことで、最初は深淵だったのですが、
繰り返し処理によってこのような楕円領域の当てはめを行うことができます。
これはこれで非常に良い方法ですが、実際にこういったことが起こります。
問いプログラムという問題設定で、実際の画像ではなくて分かりやすい簡単な例で、
このヘシアン・アフィンの良いところ、悪いところを確認します。
このような二つの楕円の形状が重なったこのような画像が入力されたとしてください。
この時、この画像からアフィン領域を推定すると、このようなアフィン領域が求められます。
上と下は同じ画像ですが、X方向に1画素、Y方向に2画素ずれている画像、
すなわち最初に探索する初期値の深淵の位置が少しずれているというわけです。
見た目はほとんど変わりませんが、実はこの中心値がずれているわけです。
中心値がずれたものからスタートしてアフィン領域を推定すると、
残念ながら上と異なるパターンに収束してしまうということがわかります。
このペシアンアフィンでは、この当てはめを行った楕円領域から深淵にして、
そしてそこの領域からシフト特徴量を記述します。
このシフト特徴量間の距離を計算すると、残念ながら当然違う領域を捉えていますので、
距離は大きくなってしまう。すなわちこのパターンとこのパターンはほとんど見た目一緒なんですが、
このようなずれを生じているだけで別物ということになってしまう。
これが従来のアフィン領域推定であるペシアンアフィンの問題点になるわけです。
我々はこの問題点を見つけました。
そこでこの問題点を解決するようなアプローチというものを研究で取り組んだわけです。
これを複数のアフィン領域推定、Multiple Hypothesis Affine Region Detectorということで、
ICCV、2015年に開催されたInternational Conference on Computer Vision、
コンピュータービジョンの国際会議、これの中でもトップカンファレンスといって非常に難しい国際会議なんですけれども、
そこで採択されまして発表した内容です。
この方法はこれらの入力画像に対して複数のアフィン領域を推定するというアプローチになっています。
したがってそれぞれの推定したアフィン領域ごとにシフト特徴量を計算し、
これらの距離計算をすることで部分的に一致するものがあるので、
この上のパターンと下のパターンは距離が小さい、すなわち同一ですよというふうに判定することができるという考え方です。
では実際にどのようにこの複数のアフィン領域を探索するか紹介します。
ここで非東方性のログフィルターというのを使います。
皆さんログフィルターはX方向もY方向も同じ大きさ、それを東方性のログフィルターというふうに言います。
ここではXとYで大きさが違うような、しかも回転するような非東方性のログフィルターというものの畳み込みを行います。
このような入力画像が与えられたら、ここでいろんなパラメータにおける非東方性ログフィルターを作り、
この非東方性ログフィルターをこのキーポイント画像に畳み込むわけです。
そしてその時の応答値を計算し、応答値が最大となる非東方性ログフィルターのパラメータがアフィン領域になるというアプローチです。
もちろん他のところでもピークが出れば、これが2つ目のアフィン領域になるというわけです。
さあ、ここで問題は、この非東方性ログフィルターを作るためには、
sigmaXというX方向の分散と、sigmaYというY方向の分散、そして回転を表すθというパラメータがあります。
これらに対してそれぞれ非東方性ログフィルターを作りますので、実際はこのようにたくさんのログフィルターとの畳み込み処理を行う必要があります。
実際このようなパラメータで非東方性ログフィルターを作ると、全部で4913個のフィルターができます。
ということは、画像のある点に対して、この4913個のフィルターとの畳み込み処理を行わないといけないというわけです。
これは計算量が膨大になってしまう。
それではですね、非効率なので、そこで我々はどうしたかというと、特異値分解という方法を使って、
この4913個のログフィルターを、1、2、3、4、5、6、7、8、9、10、11、12、13、14という、
14個の主成分フィルターというものと係数で近似するといったことを考えたわけです。
特異値分解についてはですね、説明を省略しますが、
これらの情報を、この少ない14個の主成分フィルターというものに分解してくれるアプローチというわけです。
ではどのように分解するかを簡単にだけ紹介します。
まず、非東方性ログフィルターをたくさん集めた行列を表現します。
この行列の横には、1つの非東方性ログフィルターの左上から右下までを並べた値が入っていると思っている。
したがって、1つ目、2つ目ということで、全部で4913個のログフィルターを
1次元ベクトルに展開して、それを並べたものが、スタックしたものが、このダーブルという行列です。
このダーブルという行列に特異値分解を適用すると、
U、S、Vの点値という行列に分解することができます。
このとき、このSという行列の対角成分は、このように固有値を持っています。
この固有値のどこまであるかが、この元々の行列のランクというものを表しています。
実際に調べてみると、14個の特異値を使うことで、全体の96%を表現できていることがわかります。
すなわち、ここを全部使う必要がなくて、14個に制限することができますよというわけです。
したがって、14個だけを採用して、そして行列をもう一度元に戻してあげると、
U、Sという行列とVの点値という行列になるわけです。
こちらのVというものの、ここの一つが縦に14個あるわけですけども、
一つ一つが固有フィルターと言われるものになります。
そして、こちらのUとSの行列が固有関数というもので、
この固有フィルター、ここでいうV1、V2、V14というのは固有フィルターで、
この固有フィルターにかける係数をロー1、ロー2というふうに言いますが、
この係数がこちらで表現される固有関数として求めることができるようになるわけです。
まず、固有フィルターを見てみましょう。
非統合性ログフィルターの行列を特一分解してできたときの固有フィルターの14個は、
このようないろんな面白いパターンを持つ固有フィルターが生成されています。
そして、それぞれの固有フィルターに対応した固有関数はこのような形状を持ちます。
シーターを変えていきます。
シーターによってこの固有関数の形状が変わります。
特にこのような点対称のものは当然シーターを変えても変わりません。
しかし、このようにシーター方向に対してパターンが異なるような固有フィルターにおいては、
固有関数もシーターに合わせて変化するということがわかります。
少し式を追って説明をします。
今ここまでできたことは何かというと、
シグマX、シグマY、シーターというパラメータを持つ非等法性ログフィルターがありました。
我々はこれを特異値分解を適用することで、
14個の固有フィルターと固有関数で表現される係数の掛け算でこれらを表現する、
近似するといったことを実現しました。
ここをよく見ると、この固有関数は最初にシグマXとシグマYとシーターを理算的にパラメタイズしていますので、
それに合わせた値を出力することができるだけです。
もしシグマXが1.62とかという、
ここで設定していない値におけるログフィルターを求めようとすると、
この場合はパラメータは理算的なので求めることができない、
すなわち近似することができないということになります。
そこで我々はこの理算関数をこのような連続関数に使ってフィッティングを行っています。
連続関数をフィッティングすることで、
この理算的な点以外の場所におけるパラメータを近似することができるようになります。
このような連続関数でこちらの理算的なものを連続関数で表現し直すということです。
これで連続関数のフィッティングにより、
2の連続パラメータのログフィルター、最初41900個と言いましたけど、
それ以上のログフィルターの畳み込んだ応答値を計算することができるようになるわけです。
ではもう少し細かく見ましょう。
ログフィルターに対して、実際にログフィルターに対して画像を畳み込む処理を考えます。
iが画像です。
vが固有フィルター。
そしてこのρが固有関数でした。
この計算は、まず固有関数から出てくる係数と固有フィルターを掛け算して、
そして画像に畳み込むという処理をします。
よくここの中身を見てみると、この式は順番を、ここは画像ですね。
なのでこの順番を入れ替えることができる。
iを後ろに持ってくることでどのようにできるかというと、
まず最初に固有フィルターと画像を畳み込んでから、後で係数を掛ければいいということになるわけです。
したがって、このあらかじめキーポイント画像が与えられたら、
このパラメータのσx、σy、σθの係数は置いておいて、
まず固有フィルターとパッチ画像の掛け算をして取っておき、
そしてこの2のσx、σy、σθを与えて、
この固有関数に与えて、その係数を掛けていけばいいという。
こうすることによって計算の効率化を行うことができるわけです。
さあでは実際に近似計算の流れを見てみましょう。
キーポイント画像が入力されました。
そしてそこにまず14個の固有フィルターを畳み込んでおきます。
そしてその後に固有関数にパラメータを入力して、
それに該当する係数を求め、
それを総合することで近似計算値が出ます。
この上は実際に連続的な非等方正ログフィルターを計算したときの真値で、
下が14個の固有フィルターを使って近似計算したときの結果です。
ほぼ値が一緒であるということがわかります。
では実際にこのσx、σy、σθのパラメータの値を入れ替えてみましょう。
このようにパラメータを変えたときは、
この係数、固有関数が出てくる係数だけが変わるだけなので、
こちらの計算はもう事前に終わっています。
したがって高速にこのように近似計算を行うことができるというわけです。
ではこの方法によって我々はσx、σy、σθという空間における
すべての応答値を効率的に計算することができた。
では今度この中からどこにそのピークがあるかということを調べます。
σx、σy、σθという3つがパラメータがあります。
このある回転した同じ同一σにおいて複数のアフィン領域が出るということは、
ちょっとこれはシフトにおける小さなスケールから探索して
そこで出たらストップするという考え方からも1つで十分だということがわかります。
したがって各シーターごとにまずピークの点をこうやって見つけ出します。
これを並べてみるとこのような形状になります。
この時に最大値となるところから80%以上となるピークのみを取り出します。
まず1つ目のピークのパラメータはσxが6.4、σyが12.8、σθが15というこのようなアフィン領域を表しています。
もう1つのピークはσxが6.4、σyが12.0、σθが156度というこのようなアフィン領域を示しているわけです。
したがってここでは2つのアフィン領域が検出されたということになるわけです。
では従来法とそのアフィン領域推定結果を見てみましょう。
従来法のヘシアンアフィン、そしてMSERという方法に対してアフィン領域の推定結果ですが、
このように複数の団員が含まれるような形状ですと、毎回初期値によって異なる領域が求められてしまっているというのがヘシアンアフィンでした。
一方MSERは軌道値の変動を見ますので、全体を含むような領域を取り出します。
なのでこれらのパターンに対しては同じ領域を求めることができていますが、
グラデーションがあるようなものですと色の軌道値の変化を見ているので、
このようにグラデーションがかかったパターンにおいては領域が変わってしまいます。
一方我々の提案手法はすべての4913種類の非統合性のログフィルターと同じように探索した結果を効率的に求めることができています。
さらに複数の団員が含まれるパターンにおいては、複数のアフィン領域がこのように求められていることができていることがわかります。
実際に画像にこのアフィン領域検出をしてみました。
このような色んなアフィン領域が計算され、視点Bの画像においても同じように色んなアフィン領域が推定されています。
少しわかりづらいので一箇所拡大しましょう。
この領域、2つの領域のアフィン領域が推定されています。
同じように同じ中心点に対して2種類のアフィン領域が推定できています。
当然見え方が変わっていますが、それに合わせたアフィン領域が推定できているというわけでは、
この我々の方法がどれくらい効果的であるかといったことをデータセットを使って評価しました。
まず、Oxford Matching Datasetの中にある遮影変化を含むGraftyというデータです。
横が1と2という画像間でのマッチング精度、次が1と3、1と4、1と5、1と6です。
当然1から6になればなるほど遮影変化がきつくなっていますから、
当然性能が落ちていくということがわかります。
その時にまず従来手法であるヘッシアンアフィンを見てみましょう。
ヘッシアンアフィン、ちなみにDOGはシフトの方法ですね。
まずシフトのDOGを見てみると、当然遮影変化が少ないときは通常のスケール推定で求まるので、
60%くらいできているんですが、遮影変化がきつくなっていくとどんどんできなくなってしまうというのがわかります。
同様にヘッシアンアフィンも最初はうまくいくんですけども、ある程度のところからうまくいかなくなってしまいます。
MSERはその点非常によくできるんですが、我々の提案手法はそのMSERを上回るような性能を獲得することができています。
続けて遮影変化だけではなく、回転スケール、ブラー、軌道変化、JPEG圧縮が生じた画像感におけるアフィン領域推定結果を見ましょう。
それぞれの結果をグラフで表します。赤色が提案手法です。
赤色のグラフが一番カーブが上にあります。
すなわち、我々の提案手法が回転スケール、ブラー、軌道変化、JPEG圧縮においても効果的であるということがこの実験結果がわかります。
では実際に今度はどれだけ計算効率が良くなっているか、実際に求める計算時間の比較をしてみました。
まず従来のシアンアフィンですと1枚の画像に対して大体4秒ぐらいの時間を必要とします。
オリジナルの非統合性のログフィルター4913種類を畳み込んで、すべて畳み込んでしらみつぶしに探索した場合は当然時間が198秒かかるというわけで、
我々はシアンアフィンより性能が良く、かつオリジナルログフィルターと同じ精度を保ったまま計算速度を2秒ということですから、
かなり早くすることができたというのがこれを見てわかると思います。
実際にこのシアンアフィンからは大体2倍ぐらい、さらにオリジナルのログからは87.2倍の高速化を実現したというわけです。
これがキーポイント検出におけるアフィン領域推定として、我々はマルチプルアフィンリージョンディテクターという方法を考えました。

考えて提案しました。では続けてキーポイントマッチングの特徴記述についても見ていきましょう。特徴記述においても年表でそれぞれのアプローチをまとめていました。
まず軌道ベースの回転スケール変化に対応するような特徴量というのは2010年頃からいろんな方法が出てきました。
勾配ベース、シフトでは勾配を計算して勾配方向筆グラムを出しました。最初に提案されたのが1999年でした。それからもいろんな方法が提案されています。
一方、アフィン領域、アフィン変形に対応した記述方法というのは少なく、これを視点合成ベースというふうに呼びますが、
Aシフト、ASRという方法が提案されているだけで、それ以外にはなかなか研究が取り組まれていない、すなわち非常に難しい問題であるということがわかります。
特にこの視点合成に基づく特徴量記述を効率的に求めるということは、研究が少ないということからも困難であるということがわかります。
ではまずシフトと従来法であるAシフトという方法がどれくらい違うかということを見てみましょう。
この2画像間においてシフトの特徴量を使ってマッチングしたのですが、マッチングが一つもできていない、対応点が取れていないということがわかります。
一方、Aシフトはこれぐらい遮蔽変化が起こっても、これだけの対応点を求めることができていることがわかります。
いかにAシフトがシフトよりも、当然ながら遮蔽変化に対応した方法ですから、より良くできているということがわかります。
ではこのAシフト、どうやって実現しているか想像してみましょう。
このAシフトは画像の視点合成に基づく特徴量記述を行います。
実は検出したパッチ画像に対して、いろんなパラメータ、フィンパラメータでそれぞれ変換して、
いろんなパラメータに対応する視点合成をした画像を作ります。
そしてそれぞれから、この1個1個からシフトの特徴量を記述して、これらをマッチングするということをしているわけです。
よく考えれば、素直なアプローチではあるんですが、計算量が大幅に多いということが想像できると思います。
そうですよね。一つのパッチ画像に対して、数百種類というフィン変換をして、それぞれで特徴量、128次元の特徴量を出して、
それをすべてしらみつぶしに距離計算をして対応点を求めるというわけです。
これを式で表現すると、どういったことかというと、
i というパッチ画像が入力されたら、あるアフィン変換パラメータ p で変換したものから特徴量記述をしたものということになります。
まずここの視点合成は、i という画像が与えられたら p というパラメータで変換をしました。
その変換した画像に対して特徴記述を行い、そして特徴量を使ってマッチングするというわけです。
問題はここですね。この一つの一枚のパッチ画像に対して、すべてのアフィンパラメータで視点合成をしないといけないわけです。
これを入力画像を与えられて、オンラインでアフィン変換をしようとすると、明らかに計算コストが高いということがわかります。
そこで我々はどういったことを考えたかというと、画像をアフィン変換するのではなくて、特徴量を記述するためにフィルターを設計し、
そのフィルターを事前にアフィン変換しておけばいいんじゃないかということを考えたわけです。
このように画像をアフィン変換するのではなくて、下にあるようにフィルターをアフィン変換してから画像を畳み込みましょうというわけです。
そこで従来の特徴記述書を記述する方法に ORB という方法があります。
ORB を見てみると、1枚のパッチ画像の赤の点から青の点の差分を計算し、その差分の値を特徴量にするという方法です。
これが1次元目。2次元目は異なるペアにおける差分を計算します。
これが例えば 256 個あることで 256 次元の特徴量になるというわけです。
この差分計算をフィルターで表すことを考えてみると、この領域にはプラスの値、この領域にはマイナスの値、それ以外はゼロという値を持つフィルターをこの画像に畳み込むことは、この計算をすることと同じであることがわかります。
我々はこのような特徴記述を、こういうフィルターをそれぞれ設計し、このフィルターの畳み込み処理によって特徴記述をするということにしたわけです。
そしてこの特徴記述するためのフィルター、これを視点合成、すなわちアフィン・パラメータによって変形させましょうというわけです。
この時に実際にこのパターンをいろんな視点から見ます。
ここではこのtというパラメータとphiというパラメータでいろんな視点からのこのフィルターを作るということをするわけです。
実際にどうすればいいかというと、この特徴記述をするためのフィルターをまずあらかじめアフィン・パラメータで合成をします。
それぞれ合成をします。そしてこれを使って多視点の特徴量をAシフトと同様に、だけどAシフトよりも効率的に算出しようというわけです。
しかしこのアフィン変換済みの特徴記述フィルターというものはこれも大量にあります。
大量の特徴記述フィルターの計算が必要になるわけなんですが、これは先ほど実際にこの記述フィルターは256種類あって、
それを576視点になりますので、合計でなんと14万7456枚の特徴記述フィルターになります。
1枚のパッチ画像に14万7456回のフィルター処理をしないといけないということになります。大変です。
そこでこれも前のアフィン領域の検出と同じように特一分解を使って、この大きな特徴記述フィルター群をより少ない固有フィルターで禁止しようというわけです。
この後アプローチは全く前回と一緒です。
まずダブルという行列、すなわちアフィン変換した特徴記述フィルターを並べスタックした行列を求め、これを実際は縦の方向に対しては14万7456、
こちらの横軸は一つのフィルターのサイズですから、60×65の4225というふうになります。
そしてこのダブルという行列を特一分解により、UとSとVの点値に分解し、同じように特一を見ています。
ここでは上位250個の特一のみを使います。
そうして求めると、USという行列とVの点値という行列になり、こちらは固有フィルター、そしてUSは固有関数を表すことになります。
この固有関数は前回と同様に連続関数でフィッティングをしておきます。
そしてこのように行列計算をしていけば、各1次元目の特徴量、2次元目の特徴量という形で、各次元の特徴量を行列計算によって求めることができるようになります。
さらにマッチングをする際に、視点Aの特徴点と視点Bの特徴点をマッチングする際は、
本来はこのAの赤の点に対して、緑のいろんなアフィン・パラメータを変えたときの特徴量と一番近いものを探すという方法。
これは一体、片一方はアフィン・パラメータで変えたので、他のユークリット距離を算出します。
さらに叩いたという形で、こちらのキーポイントに対しても各アフィン・パラメータの特徴量、
そしてもう一方の画像における各キーポイント点に対しても、それぞれアフィン・パラメータに対する特徴量を記述して、叩いたでこの中で一番近くなるところを探すということもできます。
この場合、我々の方法は連続関数でフィッティングしていますから、パラメータを細かくこのようにすることができるわけです。
そうすると、より良いところでのマッチングができるようになります。
さらには、この特徴給付空間を遮蔽して部分空間に遮蔽することで、ここの全体同士での距離計算もすることになるわけです。
こうすることによって、より高精度化することができます。
実際に実験結果です。青色がAシフトです。
Aシフトに対して、我々の赤色、オレンジ色と赤色が我々の提案手法になりますけれども、
この従来手法を上回る精度を達成することができていることがわかります。
実際にキーポイントマッチング例ですが、このように非常に難しいパターンにおいても、
提案手法は従来法と比べ、青色がご対応ですから、ご対応が少なくマッチング率精度が高いことがわかります。
これは別の例です。
実際にこちらも計算時間を比較してみましょう。
Aシフトは特徴量を記述するために、画像をいっぱいアフィン変換しないといけません。
なので、計算時間がかなりかかっているということがわかります。
このAシフトの計算時間を100%としたときに、提案手法がどれくらいか見てみると、
この結果を見てわかるように、Aシフトと比べて約6.6倍高速化を実現することができました。
この方法は、この特徴点、キーポイント、画像間のマッチングだけではなく、
ロボットの物体を掴むというところにも展開することができます。
例えば、ロボットがこのような物体を挟んで掴むときにはどうすればいいかというと、
ハンドの形状であるこういったテンプレートと画像の畳み込みをします。
ただし、細かくハンドのパラメータを回転角、開き幅を変えると、
当然ハンドテンプレートが324枚という大量の画像になってしまいますから、
早くどこを掴めばいいかということはわかりません。
そこで同様に特位値分解をして、ハンドテンプレートを効率よく表現します。
こちらは従来法の計算。
一方、その特位値分解を使った我々の提案手法では、
より早く舵位置がこちらよりも検出することができているということがわかると思います。
このように画像処理のテクニックを提案したんですけども、
こういったロボットへの適用もできるアプローチになっています。
提案手法はもう終了しています。
いまだに従来法は探索をしているということがわかると思います。
今日はですね、アフィン不変なキーポイントマッチング、
非常にシフトでは解決できていない、難しいとされてきた問題に対して、
我々が研究として取り組んだアプローチを紹介しました。
キーポイントにおける複数のアフィン領域を推定する方法と、
視点合成による特徴量給付の効率的な算数方法というものを、
両者ともに特位値分解という方法をうまく使って、
法制度化とかつ法律化を同時に実現したアプローチになっています。
最後に、僕の師匠であるカーネギメロン大学の金谷先生の言葉を紹介したいと思います。
僕はカーネギメロン大学に2回行っています。
これは2回目に2005年から2006年の1年間行ってた時にですね、
最後帰る時に、また日本に戻ってくる時に、
金谷先生からいただいた言葉を紹介します。
金谷先生からいただいた言葉としては、
尺眼対極、着手消極、大きな視点を持って捉えながら、
一つ一つ小さなことから手をつけてやっていきましょうということだと思います。
そしてもう一つが、素人発想、苦労と実行です。
これ言葉で言うと非常に絶やすいんですが、
実は研究者になると、どんどんどんどんいろんなことを知っていますので、
なかなかこういった柔軟な素人的な発想ができません。
なので、素人的な発想で、だけど素人の実行ではなく苦労との実行しましょうというのが、
より良い研究になるというわけです。
そういうアドバイスをいただきました。
非常に良い言葉なんですけれども、
この言葉に合うような研究をするということは、
並大抵できるもの、その簡単にはできるものではありません。
これ実際いただいたのが2006年です。
2006年からこの言葉をいただいて、
いつかこういう研究をしたいと思って頑張ってやってきたわけなんですけれども、
ようやく今回の先ほど紹介した研究、
10年ぐらい経ってでしょうか、
できるようになってきたかなというふうにちょっと思えるようになったというわけです。
先ほどの着眼対極、着手消極という観点では、
従来のヘシアンアフィンはこのような領域に対して、
局所領域だけを見てフィッティングをしてしまいますので、
良いところを探せずに終わってしまう。
すなわち、スインクローカリー、アクトローカリー、
局所領域だけを見ていろいろ処理をしちゃっているので問題があったんです。
一方、我々はスインクグローバリー、
全アフィンパラメータ、アフィン領域における応答値を計算して、
そしてこういったピークを取り出すので、
ちゃんと最適な解を求めて、
全体を見て最適な解をちゃんと求めることができるようになります。
そしてこの素人発想、クロート実装においては、
もし非統合性のログフィルター4913枚を全て畳み込むということを考えるということは、
これすごく素人発想ですよね。素直な考え方だと思います。
だけどこれを実際に4913枚を全て畳み込んでしまったら、
これクロート実装ではなくて素人実装になってしまいます。
そこで我々はクロート実装ということで、
特異値分解という手法をうまく適用して、
効率よくクロート的な実装をした。
これによって精度向上と高速化という効率化を同時に獲得したという意味で、
ようやく先ほど金田先生からいただいた言葉に対応するような研究ができたかなと思っているところです。
最後の今日の話においては研究の話です。
いろいろな研究が提案されていて、
研究というものはいきなりポンとできるものではなくて、
いろいろな従来の研究があって、
その中から切磋琢磨するような形で新しい方法を考えて提案するということです。
その時に先ほどもあったように、
素人発想、クロート実装というような考え方で研究ができると、
より良い研究になるというのが、
僕の金田先生がいただいた言葉であり、
僕の研究モットーでもある。
今日は、アフィン不変に対応するキーポイントマッチングとして、
我々の研究グループの研究について紹介しました。
我々の研究グループ、機械知覚ロボティクス研究グループは、
私と情報工学部の山下先生、
そして特任授業の平川先生の3名の先生と学生たちで、
研究グループという形でこういった研究、
最近では特に人工知能といわれるディープラーニングの研究に取り組んでいます。
今日の内容はちょっと難しかったかもしれませんが、
どういう観点で、どういうふうに研究をしているかということを知っていただければ嬉しいです。
では、今日はこれで終わりです。
それではまた。

みなさんこんにちは 今日からロボットビジョンを始めます
さあロボットビジョンというものは まず何かという話なんですけども
ロボットが目の情報、視覚情報をもとに 何かこう動作をできるようにするために
必要となる技術となります
まずロボットビジョン自体を実現するためには
まず我々のこの世界であるシーンの画像を カメラを通して取り込みます
そしてその画像の中をですね 認識しやすいようにまず処理をします
これが画像処理といいます
そしてその画像を処理した後に 画像からものを認識したりします
例えばロボットがぶつからないように動くためには 障害物を認識しないといけます
そしてさらにロボットが動くためには 2次元的なこの認識結果だけじゃなくて
3次元のシーンがどのようになっているのか ということを理解する必要があります
そしてようやくロボットは目の情報、視覚情報から 自立して動くことができるようになるというわけです
この講義ではロボットビジョンという タイトルなんですけども
まずその最初の画像処理について 講義を始めていきます
まずこの講義ロボットビジョン 画像処理のテキストがこちらの本になります
皆さん手元にありますか まだもしかしたらない人もいるかもしれません
ちなみにこの本非常に分厚い本です
しかもなんとこの本いいところがありまして 全ページフルカラーになっています
なので非常に見やすい 実際の例が豊富に入っています
画像処理もかなりいろんな技術がたくさんありますので
そのすべての技術を非常に網羅した良い本になります
もう一つこの本のいい点がありまして ここにありますように
画像処理エンジニア検定エキスパート対応書籍 というふうに書いてあるのがわかると思います
これ何かと言いますと CGアーツ協会という財団がありまして
そこの財団がやっている検定試験に沿った対応の書籍となっています
ちなみにこの画像処理エンジニア検定には ベーシックとエキスパートという2種類あります
このエキスパートは大学院相当になります
この本全体ですから 残念ながらこの授業では全部はできません
あともう一つベーシックってなります
ベーシックは3年生の皆さんでも この画像処理のロボットビジョンの授業を受けた後に
ぜひ興味のあった人は受けてもらうと受かるぐらいだと思います
ぜひ興味のあったらこういう検定も受けるといいかなと思います
さあこの本のもう一ついい点は 実は後ろの著者のところを見ていただくとわかるように
日本の中の画像認識処理に非常に有名な著名な先生が執筆をしています
その中に一応僕も入っていまして
11章、12章、13章というのを担当して書いています
自分が書いたからこれを勧めているわけだけではなくて
この本は社会人の方に非常に人気があります
社会に出てこういった画像処理に携わっている業務の人は
必ずオフィスの本棚にこれを置いておいて
なんかちょっとあれどうだったかなという時に
この本を見て思い出したりするのにも使えたりします
非常に実用的でいい本ですのでぜひ購入をしてください
実際に本、物理的な本だけでなくて
キンドル版も出ています
キンドル版の方がちょっと安いです
キンドル版でも問題ありませんので
興味が出たら購入をしてみてください
このロボットビジョンという講義の中では
ここにあるような1から9の内容をやっていきます
今日はまず第1回目ということですから
この1にあたる画像の標本化、量子化
そして濃淡画像の処理について説明をしていきます
次回以降は2から順番にやっていきます
この講義の目的は
いろんな国画像集のアルゴリズムというもの
どういうふうにできているかという概念をちゃんと理解した上で
そして最後にここの7番にあります
シフトと呼ばれる特徴点検出記述をするためのアルゴリズムがあります
これはちょっと難しいアルゴリズムなんですけれども
このアルゴリズムが最終的に理解できれば
この画像処理全般をちゃんと理解できたというふうになると思います
そこまでにいろんな国画像処理のアルゴリズムの説明をします
そして8ではコンピュータービジョン
画像処理の次に認識をするための技術を
そうしてコンピュータービジョンもしくはロボットビジョンといいます
コンピュータービジョンとロボットビジョンは基本的には同じものなんですけれども
このコンピュータービジョンという分野でどういう研究が
どういったことがなされているかということを紹介します
ちなみに僕はこのコンピュータービジョンの研究者です
我々の研究室はここにあるように
マシンパーセプション機械知覚ですね
andロボティクスグループということでMPRGというふうに呼んでいます
我々の研究グループではこういったコンピュータービジョンだとか
AIを使った画像認識の研究に取り組んでいます
最後にこの研究紹介として
アフィン不変なキーポイントマッチングという方法について
我々の研究グループの研究についても紹介したいと思います
ではまずアナログ画像をデジタル画像にします
アナログ画像をデジタル画像にするためには標本化と量子化を行います
これがレンズです
ここにこういうあるシーンがあるわけですね
このシーンをレンズを介して
まず最初にカメラのCCDというのに取り組みます
CCDでは何をしているかというと
ここにあるように標本化を行います
まず最初にやることが標本化というわけです
標本化は空間的配置を理算的な座標に射増します
連続的な空間を細かく理算的な座標に射増するというわけです
そして標本化した後にAD変換器によって量子化を行います
量子化、クォンタイゼーションとも呼ばれます
これは何をすることかというと
明るさ、各画素と言いますけど
標本化した各画素の明るさの濃淡的を理算的な値
いわゆる整数値に変換をします
これが量子化をします
その結果画像、デジタル画像になるわけです
おさらいです
アナログ画像が入力されます
アナログ画像をまず空間的配置を標本化して
そして色明るさを量子化することによってデジタル画像になるわけです
デジタル画像になることによって
いろんな画像処理のアルゴリズムを
コンピューター上で実現することができるようになるわけです
ではまず標本化です
標本化、これはアナログ信号から
理算的な位置におけるアナログ値を取り出す処理になります
まず1次元の信号波形で確認しましょう
デジタル信号処理でもやったと思いますけども
こういった1次元のアナログ連続信号が入力され
これに対してx、これ時間だと思ってください
時間方向に理算的に区切ります
こんな感じですね
これが標本化です
その標本化点に対する値をアナログ値ですから
こちらの同じ値のアナログ値を取り出してきたものが
取り出すことを標本化という
では2次元の画像ではどうなるか見てみましょう
x方向とy方向の2次元です
上から見ると普通のこういった画像ですね
これどういう画像か
これを縦軸に画素値、明るさを表すと
xとyと明るさ画素値になります
そうするとこのような形状になって
これをまずどうするかというと
x方向とy方向に対しても標本化を行い
その各標本点に対するアナログ値を取り出してきます
これが標本化のやることです
ちなみに標本化の後に量子化を行います
量子化は何をしているのかというと
今度はこの縦方向を整数にするという感じですね
そうすると縦方向にガタガタという形になるわけですね
これが2次元で表すとこのようなパターンです
これによってアナログ画像から標本化して
標本化した後に量子化することで
デジタル画像になるというわけです
実際の例を見てみましょう
この画像はコンピューター上で表示していますので
本来であればデジタル画像です
ですがここではアナログ画像だと思ってください
下にあるのが下のバーが
明るさが連続的に変わってますよということを示して
この画像をまず標本化します
標本化感覚をこれぐらいのこの1個のですね
この1個の四角形を標本点として標本化すると
こんな画像になります
その標本点におけるアナログ値を取り出してきた画像が
こんな形です
当然標本化感覚が大きいと
元の画像の情報がかなり失われているということが分かると
ではこの標本化感覚を少し小さくしていきましょう
少し小さくしましたというぐらいですね
ちょっと元の画像の何が映っているのか
少し見えてきたかもしれませんが
でもまだまだよく分かりません
じゃあもっと細かくしますかなり小さくします
さらにもっと小さくしましょう
そうすると元のアナログ画像に含まれていた情報が
ちゃんと残っているような形で
画像が標本化できるようになっていくということは分かると思います
これをですね標本化定理と言います
この標本化定理はデジタル信号処理でも出てきたかと思います
どういうことかというと
元々の標本化するアナログ信号が
そもそも周波数Fmaxを持つときに
その2倍以上である2Fmax以上で
標本化周波数で標本化すれば
アナログ信号に含まれる情報を失わないということになります
では今から3つの例をちょっと見てみましょう
まず一番最初の例です
これは元の信号アナログ画像が
このような信号だと思っています
このような信号が入力されたときに
7.4倍の周期で標本化したときどうなるかという場合
これをですねちょっとつないでいきます
この標本点が0,1,2,3というところですね
そこの交差するところ
この黒い点が標本化した後のアナログ値になります
それと選ばれたところをですね
結んでいくとこんな波形になるわけです
どうでしょう?元の黒いアナログ波形と
標本化した後の赤色の線というものが
ほぼ一致しているのがわかります
すなわち元のアナログ信号の周波数よりも
かなり大きなこの場合7.4倍ですね
の標本化周波数で標本化しているので
元のアナログ信号の情報がほとんど失われていない
劣化していないということがわかると思う
続いてこの2つ目です
2つ目の入力アナログ信号がこんな信号です
結構先ほどに比べると高周波な信号になっているのがわかる
これを2.3倍の標本化間隔
周波数でサンプリングをします
そうしたのがこの黒の点になります
黒の点を結んでいきます
という赤色のような波形になったわけです
黒色の元のアナログ波形と
赤色の標本化した後の波形を比べてみると
高いところ低いところの位置は
ある程度表現はできているけれども
元々の正弦波の滑らかな変化が
表現できていないというのがわかります
もちろん元々の持つ周波数の2倍以上で
標本化しているのですが
十分に元の波形を表現できているとは言えない
というのがわかると思う
なので2倍以上だからといって
2倍でサンプリングするのではなくて
標本化するのではなくて
大きな2倍以上の標本化周波数で
標本化しないといけないということがわかると思います
続いて最後の例は
こんな非常に高周波な信号ですね
この信号に対して1.1倍です
そもそも一番最初の標本化定理を
満足していない状態で
標本化するとどうなるかというのが黒い点です
これも同じように結んでいきます
そうするとこのような赤色の波形が
標本化した後の波形となります
元々の黒色のアナログ信号を
全く表現できていないのがわかります
むしろ違う信号となって
見えているというのが取られているというのが
わかると思います
これが標本化定理です
2倍以上の標本化周波数で
標本化することによって
元のアナログ信号に含まれる情報が
失われないということです
続いて量子化です
量子化はそもそも標本したところの値を
有限分解量の数値に変換する処理です
ちなみに画像の素子
CCDの出力というものが
一つ一つが電圧信号に変わります
これをAD変換器によって
アナログ信号からデジタル信号に変換します
その時に電圧の信号のレベルというものを
0から255の256段階にしたとします
これを8ビット量子化
2の8乗は256ですから
8ビット量子化というふうに呼ばれます
この量子化レベルというものを
かなり増やせば増やすほど
例えば256段階から512とかですね
どんどん増やしていくことによって
量子化レベルが増えれば増えるほど
当然ながら量子化誤差も小さくなるというわけです
ではこれも同じように実際に例を見ていきましょう
まず今この画像が
元のアナログ信号の状態だと思ってください
では量子化数を32にしてみました
違い分かりますか
この動画の中ではちょっと見づらいと思いますから
PDFの方で確認しておいてください
元のアナログ信号に対して量子化数を32にすると
あんまり変化が分からないかもしれませんが
よく見るとこういうですね
グラデーションのあるところが
段階的に変わっているというのは分かると思う
続いて16です
量子化数が16になるとほとんど
こういった非常に差が分かると思います
さらに8です
8にすると顔のところにもかなり違和感がありますね
続けて4そして2となるとこんな感じになっていきます
こうやってデジタル画像を標本化と量子化によって
アナログ画像からデジタル画像に変換しました
続いてデジタル画像をどうやって表現するかということなんですが
このような画像が入力されたところ
ここの領域を見ると
こういった画素値というもので構成されているのが分かると思います
この1個1個のことをですね
この1つ1つのこと
これ1個1個のことを画素と呼びます
もしくはピクセルとも呼ばれます
この1個1個の画素の値を画素値と言います
画素値というのは明るさを表していまして
一般的に範囲は256段階
すなわち0から255の値を持つということになります
したがってこの1枚の画像のサイズというのは
横幅×高さ×1画素を8ビットとすると
それがそのままバイトという単位のファイルサイズになります
ちなみに画素値についてはですね
濃淡、濃度もしくは輝度とも呼ばれたりします
ではデジタル画像の表現について見ていきましょう
デジタル画像を表す表現する1つの例に
解像度というものがあります
解像度というものは画素の密度を示す数値です
いろんな表現があるんですけども
例えば皆さん聞いたことあるようなものに
DPIというのを聞いたことあるんじゃないかなと思います
DPIというのはドットパーインチ
1インチの長さに何画素、何ドットを存在しているかということを表します
ちなみに1インチというのは2.5センチです
皆さんの自宅にスキャナー付きのプリンターがありませんか?
そのスキャナーが何DPIだったかわかる人いますか?
ぜひこの動画を見た後調べてみてください
ちなみにここでは5DPIの画像を作ってみたいと思います
5DPIの画像を作るため、量子架数は4とします
そのためにどうすればいいかというと
まず元の画像の1インチ、すなわち2.5センチを
5DPIですから、1、2、3、4、5という形で5つに区切ります
そしてその1つ1つに対して量子化した結果がこのような形になります
今度はその1つ1つの値に対して量子架数4ですから
黒、濃いグレー、明るいグレー、白という4つの値に変換した結果
この画像が最終的に解像度5DPI、そして量子架数4のデジタル画像になるわけです
ちなみにスキャナーはいろいろあるんですよ
600DPIとかそれぐらいのものがあったりします
600DPIということはたった2.5センチの長さを600に分割して
その1個1個の値を取り出してきてくれているわけです
スキャナーは非常に我々人間にとって気の遠くなるような作業ですね
皆さんの代わりにしてくれているというのがわかると思います
ちなみにカラー画像はどうやって表現されるかということなんですが
カラー画像は可放混色という方法によって色が表現されます
R、赤の成分、G、緑の成分、B、ブルー、青の成分
光の3原色を合成することによってこのカラー画像が表現できる
ちなみに実際にファイルを取り込んだアナログ画像から
スキャナーによって取り込んだ画像をデジタル画像を
ファイルとしてセーブしておく必要があります
その時にその画像自体をどのような形式で表現するかということが重要となります
実はOSだとかコンピューター環境、アプリケーション、使用目的に応じて
いろんなフォーマットが提案されています
JPEGだとか聞いたことあるかもしれませんが
一番単純な画像の形式を今日は紹介します
それがPGMというフォーマットとPPMというフォーマットです
PGMというフォーマットはポータブルグレーマップファイルフォーマットといいまして
グレースケールの画像を扱います
PPMはポータブルピックスマップファイルフォーマットといいまして
RGBのカラー画像を扱います
じゃあまずPGMの方を見てみましょう
PGMは基本的にここのヘッダーと呼ばれるところと
画像データの羅列の2種類で表現されます
ヘッダー部分にはまずP2というマジックナンバーがあります
これは何かというとこの後ろで表現されている画像データが
ASCIIテキスト形式なのかバイナリー形式なのかということを指します
その後に4.4とあります
これは画像の横幅と縦幅を表します
今4×4の画素ですよということ
そして次に255
これは量子化した後の画素値明るさの最大値を記入しておいて
その後に4×4ですから16個の値がこうやって並んでいるわけです
このそれぞれ0、255、255、0というものが
0が黒、255が白というふうに
こういう対応した形で画像が表現できるという
ではカラー画像を見てみましょう
こちらはPGMフォーマットです
こちらも同じようにヘッダー部分と画像データの羅列部分があります
最初にマジックナンバーでp3だとテキストであるASCIIデータ
p6だとバイナリーを表します
ここでも4×4ですので4.4と書いておきます
そして最大値255を書きます
この後画像データが羅列します
まず最初のところはこの3つですね
この3つでここの最初の値を表現します
順番にRGBという値が並んでいます
RGBすべてが0ですからそこの画素は黒色になる
なのでこの3つずつで値が入ってきます
ここはすべて255ですから
ここが見てもらうとわかるように白色を表します
ここは255、0、0ですから赤色
0、255、0ですから緑色を表します
このような形でデジタル画像の形式が決まっているわけです
これテキストデータで自分で入力して
それをファイル拡張子をPPMだとかPGMという風にしてセーブすれば
そのファイルをですね
画像を表示するビューワーで見ることができるようになります
では画像を今度はコンピューターで表現するときに
プログラムをするときにどういう風に考えるかという話です
用途の方法によってデジタル画像が得られました

このデジタル画像はコンピュータ上では2次元配列として扱います。
例えば、iという配列で2次元配列にします。
2と3という要素を指定すれば、ここの画素値を参照することができます。
もちろん皆さんご存知のように、コンピュータプログラム上ではこういう2次元の配列として表現しますが、
実際のコンピュータ上は、こういったメモリ空間、1次元のメモリ空間に各要素のところが、
2次元配列の要素の値がある各メモリに配置されるということになります。
擬似的なプログラムで書くと、xとyという変数を二重ループの中で、
この場合縦が8、横も0から7の8ありますので、
8、8ということにして二重ループをして、その中にこの一つの画素に対応する処理をするというわけです。

さあではここで画質について考えてみましょう
皆さんよく画像を見た時にこの画像の画質が良い悪いといったことを言うと思います
そもそも画質が良い悪いといったのはどこから来るんでしょうか
これはもともと画像が持つ情報を考えてみましょう
画像が持つ情報は濃淡情報と濃淡の空間分布から表現情報が含まれていることになります
すなわちこの2つが画質に影響を与えるというわけです
これアナログ画像をデジタル画像に変換する時にまず空間的配置を標本化しました
その後に色明るさを量子化しています
したがってこの空間的配置から選ばれる空間分布と色明るさを量子化した時に得られる濃淡情報
この2つに画質の良し悪しが決まってくるというわけです
ここではこの下の濃淡の上の濃淡情報に着目して画質というものを見ていきましょう
これ左と右の画像があります
どちらの画像の方が画質が良いでしょう
左の方が良いと思う人や右の方が良いという人ですね
色々ですね
じゃあ後でまた見てみます
次今度はこの左の画像と右の画像どちらが良いですか
では今度はこの画像です
これも左と右の画像どちらが良いですか
こちらの画像の方がおそらく良いという人が多いと思います
さあこれちょっとよく見てみましょう
この画像だけ見て何となく感覚的に画質が良い悪いということは言えるんですけども
やっぱりコンピューター上で扱っていろんな処理をするためには
その画質の良し悪しというものを何らか数値で表現する必要があるわけです
そのためにはまず画像の情報を何らか変換をしないといけない
ここではまず濃淡変化だけに注目をします
すなわち濃淡の空間分布の情報が必要ないというわけです
なのでこういう二次元的な情報を一旦ですね
濃淡ヒストグラムと呼ばれる各画素、軌道、明るさの値を持つ画素が
画像中にいくつあるかを数えたヒストグラムのような形で表現をします
これ横が濃淡画素値を表します
例えばこの画像を順番に見ます
一番最初見ると11って値です
11のところに1画素ありました
投票します
次145のところに投票します
次は85ですから85に投票します
というふうに対応する
この各画素の画素値に対応するところに投票していくことによって
各画素値が画像中にいくつあるかというのをこうやって数えることができます
それを度数として縦軸に表現したものが濃淡ヒストグラムになります
これプログラムで二次的に書くとこのような下のようなプログラムになります
よく間違えるところとしてここですね
これは1次元の配列で表現できますので
ヒストという1次元の配列で256個の要素としておきます
これを画像の画素値を参照して
その画素値のところに比の値をインクリメントしてあげればいいわけです
これインクリメント処理をするときには
必ずその1次元配列の初期化ゼロをしておく必要があります
気をつけましょう
さあではこの画像に対する濃淡ヒストグラムを表現してみました
これ見るとわかるように暗いところの値から明るいところの値まで幅広く存在しているというのがわかります
そしてこのあたりの色明るさとこのあたりの明るさが
この画像の中に多く含まれているということもわかります
これが濃淡ヒストグラムというもの
では画質が悪いと思われる画像の濃淡ヒストグラムを見てみます
この画像の濃淡ヒストグラムを見るとほとんどが暗いところに分布しているのがわかります
一方明るいところの画像は少ないほとんどないということもわかります
なのでこの画像は画質が悪いんじゃないかということが
濃淡ヒストグラムにすることによってより客観的に確認することができるようになります
ではこの元の画像に対して今度はこんな画像です
この画像は暗いところと明るいところが全くなくて
この範囲のみに明るさが分布していることがわかります
当然はっきりしないような画像になっているんですね
これもいわゆる画質が悪いという画像になるわけです
この画質というものをですね
さらに濃淡ヒストグラムを求めた後に客観的な数値で表現するための
画像のいろんな統計量というのがあります
例えば濃淡ヒストグラムを計算した後に
一番小さな値を出したものが最小値になります
当然その濃淡ヒストグラムの最大の値を最大値と言います
濃淡ヒストグラムの平均値は画素値の合計からを画像サイズで割ってあげれば
平均値は計算できます
あと中央値というのがあります
中央値というものは画像サイズ割る2番目の画素値です
これ全部で画素が含まれるわけですけど
その半分のところを見るとここの値になりました
これを中央値と言います
再品値は一番この値が大きいところですね
これが再品値となります
あと分散も当然計算します
分散は画素値を平均で引いた後に
自乗したものを相和をとります
ちなみに分散の計算は一般的に
各値を平均で引き算してから自乗しますけども
式変形していくとこのような表現ができます
この各画素値を自乗して平均を自乗したもので
引くということでも分散は実は計算できたりします
覚えておきましょう
このような画像の統計量
最小値と最大値を濃淡ヒストグラムから
まず計算します
ここですね
計算しました
そしてこの値を使って画質を表す客観的な数値である
コントラストというのを計算することができます
例えばこのコントラストというものは何かというと
画像のヒストグラムの分布の広がりを表したものになります
要は画像の明暗を表したものになるわけです
このコントラストの式はCで表しますけども
分母がiマックスとiミニマムを足します
最大値最小値です
こちらがiマックスこちらがiミニマムになります
この値を使って分母はiマックスプラスiミニマム
そして分子はiマックスからiミニマムを引くということになるわけです
では実際にこのような濃淡ヒストグラムが得られたときの
コントラストを計算してみましょう
これはどういう風になるかというと
最大値は250ですから
250プラス5の250-5ということになります
すなわち255分の245となるわけですね
実際にこれの計算をしてみるといくつになるかというと
245割る255ですから
0.96という値になります
これコントラストが1という時はどうなるかというと
分かるようにiマックスが255iミニマムが0だとして
すると255プラス0分の255-0になりますから
255分の255すなわち1になるわけです
すなわちコントラストが1が一番大きな値となるわけです
したがってこのようなヒストグラムを得られたとき
0.96というコントラストになっていますから
コントラストが高いという状況であるということが分かると思います
一方下は計算してみましょう
191-64で191-64になりますから
分母は足し算をすればいいわけですから
191-64をするとこちらは当然255
一方191-64は127となります
なので127÷255は0.49となります
すなわちかなりコントラストが低いということが分かります
これでまず画像から濃淡ヒストグラムを計算して
濃淡ヒストグラムから得られる画像の統計量である
最小値と最大値を使うことによって
最終的にコントラストを出すことができたわけです
したがって画像が与えられたときに
この画像のコントラストは0.96ですから画質がいいですよ
だとかこの画像はコントラストが0.49ですから画質が悪いですよ
ということで客観的な数値でその画像の画質を
表すことができるようになるというわけです
では次に画質を調整してみましょう
画質の悪い画像をより良くするわけです
そのためにはいろんな方法がありますが
まず線形変換について見てみます
こちらのFが入力の濃淡ヒストグラム
こちらのGが出力画像の濃淡ヒストグラムを表します
この入力画像の画素値Fの値を入力したときに
S6画像の画素値Gをどのように計算するかというわけです
その式がこの式になります
この式は何を計算しているかというと
Imax-Imin分のDmax-Dminです
Imax Iminはまず入力画像のIminと
最大値であるImaxがIminとImaxです
ここでいうとここの値とこの値ですね
その後にS6画像の最小値と最大値を決めます
これはどれくらいコントラストを高くしたいかによって
皆さんが設定する値となります
ここではあるDminとDmax値を設定します
そうするとここの項が計算できます
これは何を表しているかというと
この傾きを表しているわけですね
この傾きによってFが入力されると
最小値から引いてその傾きに合わせて計算して
Dmin分を足すということで
幅が狭いヒストグラムを
DminからDmaxまでに広げることができるというわけです
実際にこのような入力画像に先ほどの
線形変換を施すことによって
このようなS6画像が得られます
コントラストが低い画像から
高い画像に変えることができたというのがわかると思います
この画質を調査するために
いろんな変換があります
例えば区分変換と呼ばれる方法では
ある範囲はこれぐらいの傾き
あるi2からi3の値はこのような急な傾き
i3からi4の間はまたなだらかな傾き
というものを区分ごとに傾きを変えて
変換するといったことができる
これもいい方法であるのですが
この各区分ごとにどれぐらいの傾きにするか
ということを決定するための
D1,D2,D3,D4のそれぞれ値を設定しないといけません
ちょっとめんどくさいです
そこでこのγ補正という方法があります
これはγこのような式で表現できるものなんですけど
ここのここにγがあります
このγの値を
いろいろ設定することによって
いろんな変換ができるようになるという
γの値によって
このトンカーブの形状
この形をトンカーブと言いますけども
γの値が1より大きいときは
上に凸のトンカーブになります
γの値が1より小さいときは下に凸のトンカーブになります
上に凸のトンカーブは
暗い値をかなり明るくしてあげます
一方下に凸のトンカーブ
この波線の場合は
暗いところはより暗くして
明るいところはより明るくするということになる
このγ補正の面白いところは
1つの式でこのγの値を変えることによって
いろんな効果を得ることができるようになるという
実際に見てみましょう
暗いところにたくさんの画素が分布しているのがわかります
なのでこの辺りはちょっと見づらいですね
こういったところを見やすくするために
例えば葉っぱの形状とかは非常に見づらい状態です
このようなγ2というトンカーブを使って変換すると
このような画像になります
先ほど非常に見づらかったこの辺の草の
いろんな形状がよりはっきりとわかるようになったと思います
このようなトンカーブは
暗いところを押し上げますから
暗いところが非常に幅広く分布するように
変換された画像になっているわけです
これがγ補正になります
他にはヒストグラムの平坦化という方法もあります
先ほどはこのようなトンカーブを
γ2というのをγ2にするのか
γ3にするのかγ0.2にするのか
我々が決めなくちゃいけませんでした
その値を設定しなくて
自動的によりはっきりとするような画像に
変換しようと
一方、ヒストグラムの平坦化を行うと
このような画像に変わります
これをどうやって実現するかというと
頻度の値が大きい、たくさんあったり少ないというのがありますが
なるべくこれを一定になるようにしようというわけです
ぴったり一定にはならないのですが
ビンというのを決めます
ビンというのは元々の画像サイズを
どれくらいの種類に分けるかというわけです
その一つ一つがビンと呼ばれて
その一つ何種類か
ビンが大きいとその種類が少なくなるわけですが
一つのビンのサイズをまず決めます
そして小さな値から頻度を計算して
その頻度にたどり着いたら
例えば10というふうに決めたら
小さいところを数えて10個到達する後の
10個の値を一つに割り当てます
そして次の10個をまた次のところに割り当てます
というふうにすると基本的には
ビンに足したら画素値を割り当てることによって
このようなヒストグラムに変わる
つまり各画素値が比較的なるべく均等に表現されるので
細かいところのテクスチャがよりはっきりと
わかるような画像にヒストグラムの平坦化を行うことによって
変換することができます
ではこのような入力画像に対して
このようなスローク画像をスロークしたい
この時どのようなトーンカーブにすればいいか
ということを考えてみましょう
この画像からこのような画像をスロークするための
トーンカーブはよく見ると
暗いところは白く
白いところは暗くなっています
ということは反転しているわけです
すなわち暗いところは白く
白いところは暗くなっていますから
こんなようなトーンカーブですね
反転するというのはこのようなトーンカーブになります
では今度は下に見てみましょう
下は白いところは白い、黒いところは黒い
だけど白と黒の2種類しかないわけです
デジタル画像は白と黒しかないわけです
なのでどうなるかというと
暗いところは暗いまま
そして白いところは白いところですから
このようなトーンカーブになる
写真家の方でマンレイという写真家がいます
この人はまだデジタル画像がなくて
何をしたかというと
写真を実際にアナログのカメラで撮影して
写真にするときに現像をします
現像はわかりますか?
暗室、真っ暗なところで現像するわけですけど
そのときに現像するときに光を当ててしまうと
これがうまくいかなくなっちゃう
ちゃんとしたフィルムがダメになっちゃうんですけど
逆にそのことを逆手にとって
このような作品を
アナログのカメラで撮影して
このような作品をカメラで写真で撮影しています
これちょっと面白いのは
輪郭のところがちょっと黒くなっていますよね
これ何かというと
現像するときに濃い光をフィルムに当てることによって
このような効果を得たわけです
このような効果のことをソラリゼーションと言います
このソラリゼーションも実際に
デジタル画像では実現できます
ちなみにこのマンレーは
このソラリゼーションという技術は
現像をしているときにたまたま暗室の扉を開けてしまった
ということによって
そういった偶然によってこのような効果を実現したというわけです
さあトンカーブはどんなようになるのか考えてみましょう
このような画像からこのような画像にしてみたいというわけです
あるところはそのまま出ていますよね
でもあるところは反転したような形になっています
これ実はどうなっているかというと
トンカーブはあるところはまっすぐ右肩上がりになって
あるところはネガで反転して
またあるところはこうやって上がる
このようなトンカーブを設計すると
実際にソラリゼーションと同じような効果を
実現することができます
今日は画像の標本化、量子化
そして濃淡画像集について行いました
次回は画像の空間フィルタリングについて説明したいと思います
それではまた

みなさんこんにちは。今日は第2回目です。空間フィルタリングについて説明します。
今日は画像をですね、いろんな処理、フィルタリングという処理を行っていきます。
まず最初に画像処理の演算について説明します。
これは数学と一緒で単行演算という画像処理のモデル、演算方法があります。
まずこの fij これは画素を表します。
i、j で指定された座標、場所の画素を表します。これを fij とします。
画像はこの画素の集合なので大文字の f で表します。これは集合を表しています。
このような画素の集合である入力画像を入力して何らかの処理の演算を行います。
そして出力画像が g となります。当然出力画像の g も画素の集合となっています。
ではこの演算に、この単行演算の演算の種類にどういったものがあるかということですが、
実は前回やった線形変換やガンマ変換といった濃淡変換は入力画像をスロック画像に演算する単行演算の一種と言えます。
今日はですね、この平滑化という演算について後でやります。他にはサドゥンだとか
強調だとかいろんな演算がこの単行演算に含まれます。
さらにですね、二項演算という処理も可能です。二項演算、単行演算の場合は入力が f 画像1枚を表していましたが、
二項演算の場合は f 1と f 2という2つの画像を入力します。 f 1、f 2、それぞれの画素の集合である f 1と f 2という2枚の画像を入力して何らか演算を行い
一枚のスロック画像を計算します。ではこの二項演算に用いる演算としてどんな種類があるかということですけれども、ある画像とある画像を加算したり減算したり、もしくはブレンディングしたりするという時にこういった二項演算を行います。
今日はこの単行演算における平滑化を説明します。ではまず演算形態として画素単位における演算について説明します。
画素単位における演算においてもですね、2種類の演算方法があります。まず点演算です。
名前の通りなんですけども、入力画像のある画素に注目します。出力画像のあるこの画素の画素値を決定するために入力画像の同じ、基本的には同じ座標の一つのこの画素値に注目して何らか演算をしてここに代入する。
そしてこれをすべての画素で行うことで出力画像が出力することができる。これは前回やった線形変換やガンマ変換は画素ごとに濃淡変換を計算して出力値を計算するというわけですから、点演算を行っていたということになります。
さあ、今日後で平滑化という処理をします。平滑化という処理は画像の周辺である近傍の値を使って計算し、一つの画素に対応する画素の出力値を計算するというものです。
この出力画像における画素と同じ座標を注目画素とします。その注目画素の隣接したこの集合3×3の領域をこの場合は近傍というふうに呼びます。
したがって出力画像のある画素値を決定するために入力画像の複数の画素値である近傍に注目する演算ということになります。さあ、この近傍演算を数式で表現してみましょう。
今、この注目画像のここの画素に対して同じ入力画像の画素がこのちょうど100といったところになります。ここですね。ここを中心とした近傍がこの3×3の値です。
それぞれ上から50、50、50、50、100、100、120、120、120という画素値を持っているというわけです。この周りの近傍の画素値の値を使って何らか演算を行い、この出力画像における画素値の値を計算します。
例えば積和計算を行います。このフィルターというものをあらかじめ用意しておきます。このフィルターはこの近傍と同じサイズで3×3となっています。
それぞれ左上をA、B、C、D、E、F、G、H、Iとします。ここにいろんな値を落ちます。ここでは一般式と表現するため、AからIのアルファベットで表現しています。では積和計算はどのように行うかというと、Aと左上の50の値を持つ
F、I-1、J-1と積をとります。続いてBの位置に対応するここの50ですから、BとF、I-1、Jとの積を計算します。
Aはこちらと、Bは隣と、Cはここと、Dはここと、Eは真ん中と、Fはここと、Gはここ、Hはこの画素、Iはこの画素とのそれぞれ
A、B、C、D、E、F、G、H、Iということで、それぞれ積を計算し、その積の和をとる相和処理が積和演算といいます。一般的にフィルタリングと言われる処理はこのような処理計算方法になります。
では、このAからIの値をどのように設計するかによって、その画像をどのような処理をしたいかといったことを決めることになります。そこでまず今日は平滑化という処理を実現したいと思います。さて平滑化とは何でしょう。平滑化は画像上の濃淡変動を滑らかにする処理です。
平滑化の目的は一つは、例えばノイズ、画像に何らか高周波のノイズが乗っていたとします。そうするとこのようなノイズの低減を図る場合に平滑化を行います。もう一つは、画像をぼかすような効果を得たい場合にも用います。
例えば、パワーポイントの背景にカメラで撮った写真を貼り付けます。その上に文字を書きます。背景が非常に複雑な画像、複雑なパターンを持つ画像ですと、前の上に表示した文字が読みづらいですね。なので、例えば背景をなるべくぼかしましょうというような効果を得たいときに用いたりします。
この平滑化の方法なんですが、いろんな方法があります。ここでは代表的な3つを紹介します。まず、移動平均フィルターです。移動平均フィルターは、局所領域のすべての画素値に対して同じ係数で平滑化する方法です。
次に、荷重平均フィルターです。荷重平均フィルターは注目画素に対して周辺はその規模が小さいとするフィルターです。要は、真ん中注目画素の重みは高くして周りは小さくしましょうというようなフィルターを使って設計して平滑化するという方法です。
続いて、ランクフィルターです。このランクフィルターはノイズを除去しながら平滑化するときに使われます。では順番に各フィルターがどのようにできているのか説明します。
まず移動平均フィルターです。移動平均フィルターを数式で表すとこのような式になっています。移動平均フィルターは注目画素を中心とする画素の局所領域の平均値を注目画素の数録とします。平均を計算しますので、F画素をFiプラスK、JプラスLというふうに表現します。
この時にKとLをそれぞれこの3メンションとこの3メンションで操作をします。Kは-MからM、Lは-NからNです。ここでMイコール1とするとKは-1、0、1となることがわかります。
さらにもう一方のNを同じように1と設定するとLは-Nですから-1、0、1というふうになるのがわかると思います。
従ってまずKが1の時にLが-1、0、1というふうになります。続いて今度Kが0の時にLが-1、0、1となります。
Kがある値の時に3つ行いますので、3×3のすべてで9つの画素値に対応する相和を計算するということになります。
その後、こちらの式で計算するとMは今1、5は1でした。同じようにNも1ですから2×1たす1ですから2たす1で3になります。
同じように2×1たす1ですから同じように3になります。すなわち3×3ですから、結果この式はすべての1、2、3、4、5、6、7、8、9つの画素の相和を計算した後、9の値で割っていることと同じになります。
この式が上の式になるわけですけれども、この式のこの9で割っている9分の1をそれぞれの画素に分配してあげます。
当然下のような式を変換することができます。そうすると9分の1の画素値というふうにこのような式で書くことができるわけですね。
この時、それぞれこちらは画素を各画素を表しています。その各画素にかかる係数が9分の1、9分の1、9分の1、9分の1、すべて9分の1ということになるわけですね。
なので先ほどのここで紹介したこのフィルターの各係数の値がすべてこの場合は9分の1になりますよということになるわけです。
ではこちらが入力画像です。この入力画像に3×3の移動平均フィルターを施した時の出力画像がこちらになります。
さあ見てみてどうですか。よく見るとこの目のあたりちょっとぼけているのが少しわかるんじゃないかな。
あと髪の毛のこの辺かな。この辺を比較してみると違いがよくわかるかもしれません。
動画上ではわかりづらければ配布したpdfを確認してみてください。
さあ今度はこの移動平均したこの金棒のサイズを大きくします。今はmが1、nが1としていました。
なので3×3になりました。今度はmを2、nを2とします。
そうするとここに2を代入しますので2×2プラス1なので5。
nにも2を代入しますので2×2たす1ですから5。すなわち5×5のフィルターになります。
5×5の場合は係数は1Ⅶという係数となります。
この入力画像に5×5の移動平均を施すとこのような出力画像になります。
ここまでフィルターのサイズを大きくするとより平滑化の効果が出ているということがわかると思います。
さあではもうさらに大きくしましょう。今度はmを5、nを5としました。
そうするとそのフィルターのサイズは11×11になります。
実は値は1つ1つが0.008という係数を持ちます。
そうするとこの入力画像に対して11×11の移動平均を施すとこのような平滑化画像になるというわけです。
先ほどの5×5の時の結果と見てみると11×11にするとさらにボケの度合いが大きくなっているということがわかると思います。
これは移動平均する範囲を広くしていくので結果ローパスフィルターの遮断周波数がどんどんどんどん低くなるわけです。
そうなるとよりローパス、低周波の成分だけ残るような状態になっているという。
今の考え方は一次元のデジタル信号処理でも全く同じです。
さあまず移動平均フィルターをやりました。移動平均フィルターによって近傍領域の平均をとることで平滑化ができました。
ただし移動平均する近傍領域の大きさを大きくしていくとかなりボケてしまいます。
そこで平滑化はするんだけどもできるだけ元の画像の情報を残すように平滑化したいというわけです。
注目画素も周りも全く同じ係数でした。
いやそうではなくて注目画素はより出力されて、でも周辺の周りの値をうまく使いながら平滑化しようという方法がこの可重平均フィルターになります。
可重平均フィルターは注目画素に対して周辺がその規模が小さいとするフィルターによる平滑化になります。
このように3×3の場合の平滑化の係数というものを設計します。
どのように設計するか。注目画素が真ん中ですから注目画素だけこの場合係数を2という風にします。
周りはすべて1にします。分子を見ていただくとわかると思います。
今回この場合は分子の値をすべて足した値が分母の値になります。
すなわち1たす1たす1たす1たす2たす1たす1たす1たす1ですから10ですね。
ですからこの場合は分母は10で注目画素だけ2それ以外は1となります。
例えば上下左右と斜めの場所では係数を変えてみましょう。
例えば一番端、遠いところは1とします。上下左右は2とします。
じゃあ真ん中は4にしました。この黒い値が黒くなればなるほどこの四角形の色がグレースケールの色が黒いければ黒いほど重みが大きいということを表しています。
このような場合は分母の値は1たす2たす1たす2たす4たす2たす1たす2たす1で16になります。
なので分母はすべて16になります。
さあでは今度は5かけ5の値を考えてみましょう。
5かけ5ですから真ん中上下左右斜めさらに上という形で値が変わってきます。
この分子の値を見ると1、4、6、16、24、36というような形で係数分子の値が重みが変わっています。
この場合の分母の値は分子の値をすべて計算すると256になりますので分母は256というわけです。
こうするとより広い範囲だけどかつ注目画像に注目したような形での過重平均フィルターを実現することができるというわけです。
ではみなさん今度は3かける3、5かける5が出ましたので7かける7のフィルターを設計してみましょう。
7かける7ということは49個の係数を何らかの方法で決定しないといけません。
だんだん大変になってしまいます。
そこでどうするかということなんですが、こうやって今まで過重平均フィルターのように係数を我々人が設計していくと大変です。
これぐらい小さいところであればいいんですけども、7かける7とか9かける9になると大変です。
そこでこの係数、重みの値を何らかの分布に従って自動的に計算しようという方法がこちらのガウシアンフィルターというものです。
ガウシアンフィルターのガウシアンとはガウス分布のことですね。正規分布と同じです。
このような形状を持つもの。これの2次元のガウシアン分布はこのエクスポネンシャルのこの式で表現できます。
iとjは中心からの距離で、真ん中が0、0という値です。
なので離れれば中心が一番大きくて、中心から離れれば離れるほど値が小さくなるような値をするのがこのエクスポネンシャルというものになります。
この値を各このフィルターの係数として採用しようというのがガウシアンフィルターです。
ガウシアンフィルターにはここにシグマという値、パラメータを設定しないといけません。
シグマはこのガウシアン分布の傾き度合いを表すものです。
次の資料で説明します。例えばシグマが3というふうに設定すると、このような分布の形状を持つ分布がガウシアン分布となります。
この各格子の値をフィルターの係数としてこの入力画像にガウシアンフィルターの平滑化をすると、このような平滑化画像が計算できます。
先ほどの移動平均フィルターによる平滑化と比較すると、よりはっきりと平滑化されていることがわかると思います。
例えば髪の毛のところだとか、こういったセーターのところのガタガタしたところがかなりボケていると思います。
だけど全体的にはっきりと映っているような画像にすることができるというのがわかる。
今度はシグマを大きくしました。シグマを大きくするとこの傾きが滑らかになります。
シグマ3のとき、シグマ5、このような変わります。
シグマ5で平滑化すると、より広い範囲に対して影響を受けるような形で平滑化をしますので、
この平滑化度合いが大きくなったというのがわかります。
もう一回シグマ3がこんな形。
次にシグマ5にすると、かなりボケの度合いが強くなっているのがわかる。
当然傾きが広くなっていきますから、周りの情報より広く含んだ形で平滑化を行うので、
平滑化度合いの高い、よりボケた画像になるということがわかります。
さらにシグマを10とするとだんだん平らになっていきます。
平らになればなるほど移動平均に近くなりますね。
なので従ってこの場合は、より平滑化度合いの大きい、こういったボケた画像になるというわけです。
最後に平滑化の最後としまして、ランクフィルターについて説明します。
ランクフィルターは背景成分の局所特徴をできるだけ保存しながら平滑化しようという方法です。
この方法は、まず近傍領域における画素値を並び替え、ソーティングします。
小さい順にその画素値で並び替えをするわけです。
その並び替えした後に最大値を取れば最大値フィルター、中央値を取れば中央値フィルター、メディアンフィルター、
そして最小値を取れば最小値フィルターとなります。
例えば近傍の値がこのような画素値を持つとし、
200、210、220、150、200、100、200、210、230、これをまずソーティングします。
9つの値があります。小さい順に並べていきますので、
まず100、150、200、200、200、210、210、220、230という形で9つの値が小さい順に並びました。
今9つの値がありますので、真ん中である中央は5番目になります。
したがって1、2、3、4、5の5番目の200と値を採用しますよという時は、
いわゆるこの中央値フィルター、中央値の値ですから、メディアンフィルターという方法になります。
この方法の良い点は、もしこの値がとんでもなく間違った値が入ったとし、
例えば0という値が入ったとし、この時0の値が入って全体の平均値を計算すると、
この間違った値であるノイズとしましょう。
このノイズの影響を平均値フィルターの場合は大きく影響を受けてしまいます。
一方、この中央値フィルターの場合は、ここの値が100という値が0となったとしても、
中央値の値は変わりませんね。
ということで、このランクフィルターはできるだけ局所特徴を保存しながら、
実際にフィルターを作ることができます。

平滑化するといったことができる。では実際に処理結果の画像を見ながら各手法の特徴を見てみましょう。
まずこれが入力画像です。この入力画像に移動平均フィルターを施しました。移動平均フィルターを施すと全体に非常にこのベーッとしたような平滑化が行われるということがわかると思います。
次にガオシアンフィルターです。ガオシアンフィルターは先ほどの移動平均フィルターに比べると元の画像の情報をなるべく残しつつ、かつ平滑化しているといったことがわかります。
続いてメディアンフィルターです。メディアンフィルターを施すとややちょっと違和感のある画像になりますが、とはいえ何らか平滑化が行われているということがわかるかと思います。
あとでメディアンフィルターの良い点については説明したいと思います。
続いて違う入力画像におけるそれぞれの平滑化手法の比較を行います。
今度は入力画像にこのようなゴマ仕様ノイズが付与されたとします。
このような画像に対して移動平均フィルターをかけます。そうするとそれなりに平滑化がうまくいっていると思います。
ノイズの影響がこういった入力画像にすると非常にゴマ仕様的な細かいノイズが乗っているのがよくわかるんですけれども、
移動平均フィルターをかけることで比較的その影響は受けないようになるということがわかると思います。
続いてガオシアンフィルターです。ガオシアンフィルターは注目画像に重みをつけて平滑化をしますので、
残念ながらノイズが大きく乗ったところにはそのノイズが注目画像であるとそのノイズをそのまま残すような形で平滑化を行ってしまいますので、
背景のところを見てわかるようにもともとのノイズの影響がそのまま出てくるといったデメリットがあります。
続いてメディアンフィルターです。メディアンフィルターも残念ながらあまりノイズ除去、
ごま塩ノイズに関してはノイズ除去という観点ではうまくいっていないことがわかります。
これはなぜかというと、近傍領域である3×3、すなわち9つの画像のうち5個以上にノイズが乗ってしまうと、
その中央値である値にもノイズが乗っていることになります。
したがってこのようなごま塩ノイズ、非常に広い範囲に多くの画素にノイズが乗るような場合であると、
残念ながらメディアンフィルターでもメディアンフィルターはうまく働かないということがわかります。
では最後に平滑化手法の比較です。今度はこのようなスパイク状のノイズが画像に付与されたとします。
例えば、最近はこのような例は少ないと思いますけど、何らか通信をしている画像を通信して送っている途中にノイズが入って、
そのノイズがあく影響としてこのようなスパイクノイズになったと思ってください。
このような画像に移動平均フィルターをかけるとどうなるでしょうか。
皆さん一度まず想像してみましょう。答えはこの形です。
移動平均フィルターですから、そのまま局所領域における平均値を計算しますから、
スパイクノイズの影響をこのように受けたような結果になってしまいます。
続いてガオシアンフィルターです。ガオシアンフィルターは先ほども説明したように、
注目画像に重みを大きくして平滑化を行いますから、スパイクノイズのところがより強調された形で平滑化してしまいますので、
このノイズの影響をするまま受けた状態となってしまいます。
ではランク中央値、メディアンフィルターはどうでしょうか。
メディアンフィルターは局所領域における中央値を取り出しますので、
スパイクノイズが中央値から離れたどこかに存在していてもその影響を受けないというメリットがあるわけです。
同じ平滑化においてもそれぞれ移動平均フィルター、ガオシアンフィルター、メディアンフィルターによってそれぞれ効果がこのように違うというわけです。
なので各平滑化手法の仕組みをちゃんと理解して、その問題に合わせて適応していく必要があります。
ではここまでのまとめです。
それぞれの平滑化効果について皆さんの考えを記入してください。
動画は一旦ここで一時停止しましょう。
一時停止して、このそれぞれに対して効果を代であれば20、効果がなければ×といった形でそれぞれのところに記入をしてみてください。
では始めてください。
では平滑化の効果をまとめていきましょう。
まずノイズない画像に対して移動平均フィルターは確かに効果ありました。なので○でしょう。
ガオシアンフィルターはとてもいい感じに平滑化できました。
メディアンフィルターはちょっと変な感じがしたのでやや効果ありというぐらいでしょうか。
さあゴマ塩ノイズをした場合移動平均フィルターはそれなりに効果ありましたよというわけです。
ガオシアンフィルターをゴマ塩ノイズにすると残念ながらあまり効果がありません。
同じようにメディアンフィルターをゴマ塩ノイズの画像にノイズ除去をしてみるとこれも効果がありませんでした。
じゃあ続いて今度はスパイクノイズです。
スパイクノイズは移動平均の場合は残念ながらこれも効果はありませんでした。
ガオシアンフィルターも強調しますので効果がなし。
一方スパイクノイズに対してメディアンフィルターは効果がありますよというような形。
なのでこのように各手法によって得意不得意とするところが異なるわけです。
これらの性質をちゃんと理解してどういうような画像に対してどのような効果を得たいのか。
平滑化のどのような効果を得たいのかということを理解した上でちゃんと適切なフィルターを適用しましょう。

では続いてエッジ抽出について説明をします まずエッジというのはどういったところでしょうか
エッジというのは濃淡が画素値が急激に変動するようなところを エッジと呼びます
画像中で明るさが急に変化するようなエッジを 何らか画像処理によって抽出をしたいというわけです
このような変化を捉えるためには微分ということを行います
さあ画像における微分というものを考えなくてはいけません まず一般に x に関する変微分という式を出します
f x y が画像です この x と y は i と j ではなくてアナログ画像
連続的な値を持つものとして表現しています いわゆる連続関数です
なのでこれはアナログ画像です アナログ画像における x 方向 x に関する変微分はこれはよく微分積分で出てきたようなこのような式で表現できて
まあこれをそのまま解ければいいんですが この f x y は残念ながら連続関数です
我々が扱う画像はデジタル画像です デジタル画像はそもそもこの delta x を 0 に近づけるということができません
なぜかというと画素というものはもう離散化されていますので整数値になっています 0 に近づけるといったことができないわけです
そこで画素の最小単位は 1 画素になりますのでこの delta x を 1 というふうに置くわけです
そうするとこの分母が 1 このここの delta x も 1 になりますので結果式は f x y の x に関する微分は
x プラス 1 の画素値から f x の 1 の画素値を引き算
すなわち差を計算するということになります したがって画像の微分というものは差分近似で表現することができます
くどいですけどもデジタル画像における微分は結果 隣り合う画素値の差として表現することができるわけです
それでは先ほど計算した微分を用いてエッジ抽出を行います これを微分フィルターと呼びます
まず横方向に微分を計算します 横方向に微分を計算するとそれを delta x と呼びますが縦値が計算することができます
この横方向に微分を取った値を勾配と言います 続いて今度は縦方向に差分を計算します
y 方向の差分です 結果 y 方向に勾配を計算すると横エッジが出ますよというわけです
この時それぞれの勾配 delta x と delta y には正の値とか負の値を持ちます 当然ながら差分を計算しますので
不正が存在するわけです そこでこの横方向の勾配と縦方向の勾配を合成して勾配を求めます
どうするかというと x 方向の勾配を自乗して y 方向の勾配を自乗して足したものをルートを取ります
それが各画素における勾配の大きさということになります それを示したのはこの画像となります
さらに各画素に対して x 方向と y 方向の勾配がありますので x 方向の勾配分の y 方向の勾配を計算して
アークタンジェントを取ることでこの勾配の傾き どちらの向きに勾配がすなわちエッジが発生しているのかということも知ることができます
これが微分フィルターです 他にもですねいろんなフィルターがあります
クリューイットフィルターというフィルターはなるべくノイズを抑えながらエッジを抽出しようというものです
ノイズを除去する効果としては平滑化がありました 平滑化は局所領域における平均値を計算するというようなものでした
なので微分は隣り合う画素の差分を計算するわけですね なので微分と平滑化を
2つの効果を持つようなフィルターを作ろう その場合は微分のフィルターを表すマスクパターンと平滑化を表すマスクパターンを
合成して一つにするわけです これとこれを合成するとこのようなプリウイットフィルターができます
これは横方向に差分をとりますので横方向にはそれぞれ差分をとっていますが縦方向に見ると1、1、1
ここも-1、-1、-1 同じ係数がかかっていますから縦方向には平滑化をして横方向には差分をとるというような方法
これによってノイズを抑えながらエッジを注視することができるようになるわけです このプリウイットフィルターをもう少し拡張したのがソーベルフィルターとなります
平滑化の中で出てきたように移動平均に対して荷重平均 いわゆる重み付きフィルターというのがありました
重み付きフィルターは収穫画素により重みを大きくするというものです したがって微分は先ほどと同じです
平滑化のところ先ほどのプリウイットフィルターはすべて同じ値だったんですけども ソーベルフィルターでは真ん中を2、上と下を係数を1、1にします
そうすると分子の値を足すと4になりますから1Ⅳ、2Ⅳ、1Ⅳという縦方向に対して真ん中に重みを置きながら平滑化するというわけです
この重み付き平滑化フィルターと微分フィルターを合成してできたのがソーベルフィルターというものです
どのような係数になっているかというと横方向には差分を計算します 縦方向には平滑化を行うんですが収穫画素に近い真ん中の値に重み付けをするという
このようにして平滑化と微分フィルターを合成してエッジを抽出しようという方法です
同様に縦方向の勾配を計算するためには横方向に平滑化して縦方向にそれぞれ差分を計算するというものです
では実際の画像を見ながらどのような効果があるかを見てみましょう まず入力画像です
この入力画像にプリウィットフィルターを施します エッジのところがかなりはっきりと表現されている
続いてソーベルフィルターです ソーベルフィルターは先ほどのプリウィットフィルターよりより
注目画素が強調したような形でエッジを抽出していることがわかると思います では続いてエッジ抽出の中にラプラシアンフィルターという方法があります
ラプラシアンというのは二次微分を表します 二次微分とは一次微分したものをまた微分する
もう一度微分するというもので得られたものが二次微分と言います この二次微分を使ってフィルターに用いたものをラプラシアンフィルターと言います
式ではこのような式で書けます 何を言っているかというと画像に対して x 方向に2回微分をする
画像に対して y 方向に2回微分する それぞれの方向で2回微分したものを二次微分とったものを
足し算しますよという じゃあこのフィルターをどうやって設計するかというのを説明します
まずこちらの x 方向の二次微分の計算を見てみましょう まず
x 方向すなわち横方向の一次微分を計算します 一次微分は
i 隣り合うガスで差分を計算しますので こことここの差分を計算します
もう一つ1個隣にずれてここからここを引くというような一時微分差分を計算します それぞれ一時微分を計算しました
一時微分を計算するフィルターになっています さらに今度は二次微分をしないといけませんから
一時微分したフィルターとさらに一時微分したフィルターの差を取ることで 二次微分フィルターを作ることができる
じゃあ実際にどうやって計算するか見てみましょう ここの画素値はフィルターの値は1-0ですから1になる
続いて-1-1ですから-2になります 続いて0--1ですからマイナスとマイナス1がかかって1になります
すなわちこの二次微分フィルターが x 方向横方向の二次微分を取るためのフィルターになるわけです
同様に y 方向に対しての一時微分をまず計算します そしてこちらからこちらの一時微分フィルターから一時微分フィルターの差を取って
二次微分フィルターを計算します これも同様に1-2-1という値を持つ二次微分フィルターが計算できます
最後はこちらがこちらでした 最後は足し算が残っています
なので足し算をしましょうというわけです 足し算をすると0たす0は0 0たす1は1 0たす0は0 1たす0は1
-2たす-2は-4 1たす0は0 0たす0は0 0たす1は1 0たす0は0ということで最終的にこのような係数を持つ
二次微分であるラプラシアンフィルターを設計することができたというわけです さあこのラプラシアンフィルターを使うときに平滑化というものと組み合わせて使おうというのが
だんだんちょっと複雑になってきましたがラプラシアンオブガオシアンという方法があります これは何かというとラプラシアンオブガオシアン
ガオシアンの二次微分をログフィルター 略してlogフィルターというふうに呼びます
ガオシアンフィルターで平滑化しさらにラプラシアンフィルターを適用するような フィルターを作ろうというわけです
これログフィルターの形状はこのような形状になります ガオシアンの形状は二次微分をとりますからこのような形状になります
このログフィルターの面白い点はシグマの値がありますので このシグマの値を変えることによってそのエッジの抽出できる効果が変わります
ガオシアンフィルターのシグマを変えると平滑化する度合いが変わりました したがってシグマを2としてログフィルターを計算するとこのような形になります
シグマを4としてログフィルターを計算するとこのようなスロックになり シグマを6としてログフィルターのスロック値を計算するとこのような形になります
このログフィルターのスロックに対して 正から負もしくは負から正に切り替わるところのゼロ交差する場所を抜き出すと
このようにエッジの抽出をすることができます 面白いのは同じ入力画像に対してシグマを変えることでこのシグマに合わせたサイズの
スケールと言いますけどもシグマに合わせたスケールのエッジ シグマの値が小さいとより細かいエッジを
シグマの値が大きいとより大まかなエッジのみを抽出することができるようになります これをログフィルターと呼びます
さあ先ほど出てきたラプラシアンフィルターを使うと画像を鮮明化することができます 画像の鮮明化というのはどういうことかと言いますと
少しぼけた画像が入力されると少しシャキッとした引き締まったような 画像を作るようなことを鮮明化と言います
鮮明化は何をしているかというとなだらかに変化しているようなところをですねより エッジをですね強調するようにする
エッジを強調するということは斜めの傾きだったものを傾きが大きくなるようにきつく なるようにするということによって画像の鮮明化
はっきりくっきり写ったような画像にすることができる どうやって実現するかというと注目画像から注目画素から
ラプラシアンフィルターを引いてあげればその鮮明化が実現できます 注目画素だけを取り出すためには周り0で真ん中だけ1といったフィルターを作れば
okです このフィルターからラプラシアンフィルターを3分を取ります そうするとこのようなマイナス1マイナス1マイナス1マイナス1
真ん中だけ5といった鮮明化フィルターが計算できます この鮮明化フィルターを画像に演算します
フィルタリングを行います これが入力画像です この入力画像に対して鮮明化を行うとこのような画像になります
もう一度元に戻します こちらが入力画像 鮮明化した画像がこのような形 このセーターのですねパターンを見ていただくと
あんまりはっきりわかってないんです 最初の入力画像の時ははっきりしてないんですけども
鮮明化するとくっきりとですねこういうラインが見えるようになっているのがわかると さあこれどういうふうに実現できているのか実際にもう少し詳しく見てみましょう
元の画像におけるこのラインですね ここのライン上の画素値を縦軸に横軸はここのライン上の値を表示しました
そうするとこのようなプロファイルが見られます このあたりは背景ですからぼやーっとしてますね
ここセーターのところは黒になったり白になったり黒になったり白になったりしてますから こういうギザギザとしたところがこのセーターのところになると思い
ここ二つ値が急に落ち込んでますね それは多分ここの服の色のところですねパンツの服のところが黒くなっていますから
値が小さくなっているのがわかる これは入力画像のこの1ラインにおける画素値の変動を表したもの
では繊維化した後の同じラインにおける画素値の変動を見てみましょう そうすると先ほどはこれぐらいの範囲で濃淡の変動が起こってたものが
繊維化を行うことでより大きく離れるように変化したというのはわかると思う
すなわち傾きがよりきつく大きくなったというわけです この繊維化の効果というのがこういったところを見るとよくわかると思います
さあでは二次微分を用いた繊維化の原理というものを復習してみましょう まず入力画像に対して
一次微分をとります この破線のこのデータが入力データだと思ってください
この入力データにまず一次微分を計算します 皆さんもですねぜひノートか何かに書き込んでやってみましょう
まず一次微分を計算するとここまで変化がないところはゼロです ここから値が右肩上がりに大きくなりますから大きくなってここで最大となってまた大きさが
減っていきますここまたゼロですね傾きがありません 今度は右下がりになってますから下に今度はマイナスにこうなってこのような感じになる
この赤色で書いたのが一次微分した結果になります 次に二次微分を計算します
二次微分はこの一次微分をとったものをまた微分をとるわけです 今度はゼロの位置がここに変わっていますからそこ気をつけて書きましょう
変化がないとこはゼロのままここで上に上がって今度ここの頂点のところは傾きがないというわけですけどここはゼロになる
そしてそこから今度は右下がりになっていきますからマイナスの方向に行ってこうなるわけですね これでここのとゼロになってここは次下に行ってますから下に下がってから上に上がって
こうなる形になります これが二次微分をとったものになります 最後原画像からこの二次微分を引きます差をとりますそうするとここはずっとゼロだからこうなんですがここからここは正の値を差をとりますから負に変わりますけどちょっとここからだんだんこうなってここがゼロになってこうなってまたゼロになってちょっと上がってこんな形でしょうか
これが原画像から二次微分を計算したものです 元の画像のこの傾きに対して原画像から二次微分を引いたものはよりきつい傾きになっているのはわかると思う
これによって千円化を実現したということになるわけです 今日は空間フィルタリングとして平滑化とエッジ抽出について説明をしました
次回は画像の気化変化について説明をします それではまた

皆さんこんにちは 今日は画像の気化変換とイメージモザイクについて説明したいと思います
まず画像の気化変換ということですけれども これまでの画像処理は画像の濃淡に注目して
その濃淡を近傍領域の例えば赤和演算を行って なんらか平滑化だとか
園地抽出といったことをしてきました 今日はですねそういう処理ではなくて画像そのものを
気化的に回転したりだとか拡大縮小したりする 変換処理について説明をします
この画像の気化変換は非常に面白いところです この画像処理というですねこの分野っていうのは
非常にですねなんでしょう 工学的アプローチによる解決が重要となっています
どういうことかというと数学は当然重要なんですけども 数学の世界だけでは残念ながら画像処理はうまくいきません
その数学的基礎知識を使って かつ工学的なエンジニアリングといったアプローチをうまく組み合わせることで
より良く画像処理を実現するといったことになります そこで今日はですねまず最初に画像の
気化変換をするために座標変換について説明をします そしてその後工学的なエンジニアリングによる解決方法として
画像の再配列と画像データの内装について紹介します ではまず画像の座標変換をするわけですけども
その座標変換をするときにはどのような 比較的な変換をするかによってその変換式が異なっていく
座標を計算するための変換式が異なっていく 座標変換はどのように行われるかというと
x と y が変換前の座標になります そして x' y' が変換後の座標になります
この式を見ていただくと変換前の座標である x と y に a b c d という何らかの変換パラメータがあって
その変換パラメータを x と y にかけることで 変換後の座標である x' と y' を計算します
この場合 x' は a x プラス b y y' は c x プラス d y というふうに表現できる
これは気化変換の一般的な式表現になります それでは具体的に一つ一つのいろんな気化変換についてその座標変換の数式を見てみましょう
まずは平行移動です 平行移動は英語では translation というふうに呼ばれます
このとき移動量をまずパラメータとしてあらかじめ決めておきます この tx こちらは x 方向の移動量を表します
一方この ty というものはこれは y 方向の移動量を表します
このようにまず tx と ty をあらかじめ皆さんがどのように変換したいかということで値を決めます
そしてそのパラメータ tx, ty を使って変換前の座標である x, y を x, y にそれぞれ tx と ty を足すことによって
平行移動の変換後である移動後である x' と y' を求めるという
下の例はこのような入力画像に対して x 方向にプラス100 y 方向にプラス50したときの平行移動した後の画像はこのようになります
それでは続いて拡大縮小です 拡大縮小は英語でスケーリングと呼ばれます
ここも同じようにあらかじめ皆さんがどのようなパラメータで拡大をするか縮小するかといった値を決めておく必要があります
拡大縮小においては x 方向の拡大縮小率である sx と y 方向の拡大縮小率である sy というパラメータがあります
この sx と sy の値が1より小さければ当然ながら縮小 そして1より大きければ拡大を実現します
さあこの sx sy を使って拡大縮小後の座標を x' y' とするとどのように表現できるかというと
x に対しては sx を乗算し y に対しては sy を乗算するということになります
この式を一つの行列で表現すると sx 0 0 sy といった行列で式で表現することができるのがわかります
このような入力画像に対して x 方向 y 方向ともに sx sy が 1.5 とした場合はこのように拡大することができるわけです
続いて回転であるローテーションです ローテーションはこれは高校生の時の数学でやったと思いますけれども
cosθ-sinθ sinθ cosθ というのが回転をするための行列となります
この式の動質に関しては今日はやりません 高校数学を見返していただくと出てきているはずです
この r のことを回転行列というふうに呼びます この回転行列は変換前の座標である x,y にかけることになります
したがって変換後の x' は cosθx-sinθy が x の変換後の座標
そして y 座標は sinθx たす cosθy になってこちらが y 変換後の y 座標となります
このような入力画像に対して半時計方向に15度回転するとこのような画像が得られるわけです
続いてスキューという変換です このスキューは長方形を横に水平方向にギュッと変更した変形に変形するような処理を言います
このスキューは水平方向のスキューと垂直方向のスキューがあります まず水平方向のスキューですけれども
どのようにするかというと y 座標はここ y 座標に対しては横方向に見ると同じですね
同じ座標になっていますから y 変換後の y 座標に対しては変換後も同じになります
それに対して x の座標だけを変化していく どのように変化していくかというと x に y タンジェントシーターを足します
この y タンジェントシーターですけれども これは基本的には x 方向に対する移動量になるわけです
その移動量が最初 y の値が 0 ですよね 0 からだんだん下に行くほど大きくなるわけです
したがって最初 0 の時はここが 0 になって x' は x のそのままということですけどここになるわけです
これが y の値が下になればなるほど大きくなるにしたがってこの平行移動量がどんどん増えていくというのがこの水平方向のスキューになります
垂直方向のスキューは先ほどと逆です x に関しては全く同じ値です
一方 y については x の値が増加するにつれて y の移動量が増えるということになる
そうすることによってこのような垂直方向のスキューといったものを実現することができます
それでは実際に座標変換を計算してみましょう
今 x1 y1 x2 y2 x3 y3 の3点が与えられています
この3点の各座標を順番に座標変換をしていきます
まず最初に平行移動を行います x 方向にマイナス30 y 方向にマイナス30ですからその一般式はどのようになるかというと
x-30 y-30となります
ではこの一般式を使って x1 y1の変換後の座標をこちらに x2 y2の変換後の座標をこちらに
x3 y3の変換後の座標をここに記入してください計算してください
そしてその計算した座標を使ってこのグラフ上にプロットをしてください
では続いて先ほど平行移動した後の座標に対して今度は xy 共に0.5倍に拡大してみましょう
まず一般式は0.5倍ですから0.5x 下も同様に0.5yというのが一般式になります
では先ほど変換した後の x1 y1 x2 y2 x3 y3に対してこの式を使って変換後の座標を求め
そしてこのグラフ上に3点をプロットするようにしてください
続いて回転を行います
まず回転の一般式はcosθxプラスマイナスマイナスsinθy sinθxプラスcosθyでした
この式に45度ですからsinθを45度として計算をしてください
そして同じように x1 y1 x2 y2 x3 y3の各座標の変換後の座標を計算し
そしてこのグラフ上にプロットをしてください
続けて最後にもう一度 x 方向に30 y 方向に30 平行移動します
先ほどの回転した座標にこの今度は平行移動を適用します
最終的にこの x1 y1 x2 y2 x3 y3がどのように変換されたかというのは
課題の方で提出をしてもらいます
さあここまで座標変換を説明しました
座標変換ができればこの画像の基下変換ができたかというと
残念ながらそうではありません
数学として座標変換をするということはできるわけなんですけども
残念ながら画像はですねデジタル画像を扱います
デジタル画像というのはどういう性質があったかというと
そもそも座標が整数系の座標です
小数点がありませんとですね
なので実はこういった問題が起こります
例えばどういうことかというといろんな変換を組み合わせてやっていきます
じゃあ実際に今から変換をしていきます
こちらの入力画像を x 方向に-320
y 方向に-240して変換をするとこのような画像が得られます
ここは大きな問題ありません
続いて次は変換2として先ほどの平行移動した画像に対して
今度は0.5倍に縮小します
そうするとこちらの画像がこのような画像に縮小されます
ここまでも見た感じ問題ありません
さあじゃあ次は今度は先ほどの縮小した画像に対して
45度の回転を施します
そうするとこのような画像です
45度回転しました
そしてこの45度回転した画像にもう一度320、240ということで
真ん中に移動します
そうするとどうなるかというとこんな画像になってしまいます
これって本当はここをもっとこうあるわけですよね
ですけども元の変換前の画像と変換後の画像は
例えば同じ大きさで用意する画像ですね
2次元配列として用意していくと
はみ出たところはマイナスという値をとります
そもそも2次元配列を扱うときに
マイナスのところをアクセスすることができませんね
なので用意したところだけには値を置いておくことができるんですけども
外れたところはなくなっちゃう
なのでこのようにもう一度平行移動して拡大縮小をして
そして回転して平行移動すると
外にはみ出たところがなくなってしまったような画像となってしまう
これでは本来我々が意図した変換ではないわけです
そこでこの問題を解決するためにどうするかというわけなんですけども
行列を使いましょうということです
先ほどのまず変換するためのパラメータをそれぞれ行列で表現しました
平行移動の場合はTx、Tyが縦に
拡大縮小もSx、Syが縦に
そして回転行列は2×2の行列という風になってました
これらのいろんな変換をですね
まず一つの行列で表現できるようにしようということをします
この時3×3の行列で全ての変換を表現するようにします
3×3にするということは変換後の座標がx'、y'
そしてもう一つ出てきますのでここに1というのを入れます
こういったのを同時座標という風に呼びます
この1という同時座標を導入することによって
この変換行列を3×3で表現することができる
ただしこの一番下に関しては
この一般式はまず上から順番に行くと
x'イコールaxプラスbyプラスc×1ですからこのような式になります
次y'に関してはdxプラスbyプラスfということになります
さらにもう一つ式が出てきますね
1イコール0×xプラス0×yプラス1×1ですから
最後の式は1イコール1という式が表現されていることになります
この一番下のところを追加することによって
上のこのaからfの値にそれぞれ拡大縮小回転平行移動といった
パラメータを当てはめることで
それぞれ行列の形や違うものを
全て3行3列の行列で表現することができるようになるわけです
順番に見てみましょう
平行移動はTxTyというものでした
x方向の移動量がTx、y方向の移動量がTyでした
この場合、平行移動をするためには
このような3行3列を表現します
実際に見てみると
x'変換後の座標x'イコール1×xプラス0×yプラスTx×1ですから
xプラスTxがx'になることがわかります
同じようにy'イコール0×xプラス1×yプラスTy×1ですから
yプラスTyというのがy'になることがわかります
例えば、x方向に30ピクセル、すなわちTx
y方向に30ピクセル、Tyを30とすると
こことここにそれぞれの30、30を代入してあげれば
3行3列で平行移動、x方向に30、y方向に30移動する
平行移動を表す行列が表現できるようになりました
続いて拡大縮小です
拡大縮小率であるxx、syは
このxとyという変換前の座標に直接乗算することになります
なのでx'イコールsx×xプラス0×yプラス0×1となりますから
残るのはsxxです
同様にy'イコール0×xプラスsy×yプラス0×1ですからsyyとなります
例えば、x方向に0.5倍、y方向に0.5倍縮小する場合は
sxに0.5、syに0.5を入れたこの3行3列が拡大縮小を表す行列式となります
続いて回転です
回転行列はコサインシーター、マイナスサインシーター、サインシーター、コサインシーターでした
それがここのa、b、c、d、e、fのここの2行2列のところに代入されます
したがって90度回転する場合はコサインシーター、サインシーターが90ですから1になります
マイナスがつきますからマイナス1、コサインシーターは1のままですね
コサインシーターの90を代入した場合は0、0となりますので
x方向、ごめんなさい、90度回転する場合はこのような式になります
ここまでそれぞれの変換行列を3行3列で表現することができました
これをこの3行3列の行列を使って何ができるかというと
今までは平行移動の行列、拡大縮小の行列、回転の行列がありました
どうやって計算したかというと
まず変換前の座標に対して平行移動の行列をかけて計算して
平行移動した後の座標を計算しました
そしてその変換した後の座標に次はこのHSという行列、すなわち拡大縮小を表す行列をかけます
そしてさらにその変換した座標に回転する行列をかけるわけです
このようにまずこれを計算して、次これを計算して、次これを計算してということを順番にやって
変換後の座標を求めました
これはこれでいいわけなんですけども
この変換後の座標と変換するための行列というものを順番にこの座標にかけていくと
残念ながら計算するときに例えばコサインシーターとかですと小数点がいっぱい並ぶわけです
そういったときにどこで桁数を区切ってしまうと誤差が出ます
その誤差をまた変換座標にはその誤差が含まれており
それをまた変換してまた変換していくと
どんどん誤差がですね変換による誤差が蓄積されるという問題があります
またこの変換行列が例えばいろんな変換をたくさん繰り返していくと
毎回毎回座標を変換するので計算がですね
その変換の数分だけしないといけないという問題がある
これ非常に時間がかかってしまって
そこでどうするかというと
このまず平行移動するための行列と拡大縮小するための行列は
それぞれ産業算列で表現していますから
この行列のまず掛け算を先にして求めます
そうするとこの2つの行列の積がこのような行列になりまして
今度はこの行列と残っている回転の行列との積をとって一つにまとめます
要は座標を順番に変換していくのではなくて
あらかじめこの変換行列をこのように一つにまとめておくことをするわけです
この変換行列をですね
例えばプログラムであればダブルみたいな倍精度で精度をよく求めておいて
そして変換前の座標xyにこの変換行列を計算して
変換後のx'y'を1回の計算で求めるようにすれば
非常に計算時間がかからずに
そして高精度な誤差が乗らないような状態で変換をすることができるわけです
このような方法を使うことによって変換行列を一つにまとめて
その一つにまとめた変換行列を1回座標変換するだけで
もともとであれば途中で外はみ出たところがなくなっていたものが
この変換行列を作ってからちゃんと一度の変換計算によって
好ましい画像をすることができるようになるわけです
はいではですねぜひ皆さん最初にですね座標変換したものを
一つの行列にまとめてみてください
これはぜひ一度手で計算してみてください
答えは皆さんの回答はですね課題の方で提出してもらいます

それでは続いて座標変換をした後に 画像を実際に作っていきましょう
例えば順方向、これまで説明してきた変換では
入力変換前の画像座標に対して拡大1.5倍します
そうすると変換後の座標を計算します
この時にこの変換前の座標が持つ画素値を 変換後の座標に割り当てます
続いてどうするかというと 入力画像の隣の画素に対して変換後の座標を計算して
ここの画素が持つ画素値を代入します
これを順番に入力画像の1画素ずつ変換座標を求めて 値を代入していくわけです
そうするとこのような拡大した画像が得られるわけなんですが
よく見ていただくと特に1.5倍という形で拡大をすると この間抜けてしまったところが出てきます
なのでこのように白い値を持たない画素が あるような画像として画像になってしまいます
これは好ましい変換とは言えません
ではこの問題をどうやって解決するか
実は逆方向の変換ということを行います
逆方向の変換というのはどういうことかというと
先ほどは入力画像から変換後の座標を計算しましたが
そうではなくて変換後の座標が 入力画像のどこから来るかということを調べます
そしてその入力画像の座標が持つ画素値を代入するということをします
そしてこの出力する画像の座標を次隣のところを見て
この座標に対応したところの入力画像はどこかを探して
その入力画像の画素が持つ画素値を代入します
このように出力画像の1画素1画素ごとにこの処理を行うことで
穴が開くような画像ではなく綺麗な拡大をすることができるようになるというわけです
これが画像の再配列というアプローチで逆変換に基づく手法です
これは実際にどうやって計算するかということなんですが
今まで変換する行列というものを求めました
この変換する行列はここでは h で表していますが
変換前の座標から変換後の座標を計算しました
この h この逆を求めないといけません
変換後の座標から変換前の座標がどこであるかということを調べないといけません
そのためにどうすればいいかというと
この式にそれぞれこの変換行列である h に
逆行列をそれぞれ左辺と右辺にかけてあげて計算すると
ここは単一行列になりますので結果はこのような式になります
すなわちどういうことかというと
変換後の座標である x ダッシュ y ダッシュに対して
もともとの変換行列の逆行列をかけてあげれば
変換前の座標である x と y が求まるという
なのでまず変換行列を求め
その変換行列の逆行列を計算して
変換後の座標が変換前の座標のどこに対応するかを調べ
そしてそこの座標の画素値をもとの変換後の座標の画素に
画素値として代入すればいいわけです
これで穴が開かないような画像を
機械変換してもできるようになりました
最後に内装について説明します
内装というのはどういうことかということなんですが
先ほどの逆変換に基づく方法
こちらが変換後の出力
こちらが変換前の入力画像だと思ってください
出力画像の各画素が
入力画像のどこに対応するかというのを調べます
この時にそもそも入力画像はデジタル画像ですから
座標が理算的な整数形になっています
なのでこのような小数点を持つというようなところの
画素が存在しません
そこでどうするかというと
まず一番単純な方法としては
最近傍方、ニュアレストネイバー法というものがあります
変換後の画像の座標から逆行列を求めて
変換前の画像の座標を計算しました
この時赤色のこの点だとしてください
これは小数点を持つ座標なので
ここから一番近いところ
周り4点の整数度を持つ画像の
一番近いところを見つけて
この一番近いところの値を代入しましょうというわけです
これがニュアレストネイバーという方法で
一番近いところを使いますよという方法です
これを実現するためにはどうすればいいかというと
非常に単純です
四捨五入をすればいいだけ
四捨五入をすれば
小数点以下がなくなって一番近いところになります
これはプログラムでは
xに0.5を足して
yに0.5を出して
それで整数化してあげれば
いわゆる四捨五入ができるというわけです
さらにですね
よりきれいな気化的内装変換をするためには
一番近いところだけの画素値を取り出して当てはめると
少し拡大した時とか
ガタガタとしたような画像になってしまう
より滑らかな画像を作るために
どうするかというと
バイリニア保管という方法があります
S6の座標から
入力画像の座標を止めて
この点xyに対して
周りの4点の画素値の値を使って
内装しましょうという方法です
これは近ければ近い画素
例えばここがfa,fb,fc,fdという
それぞれの画素値を持つとしてください
もしこのxyという値が
aに近い座標であれば
aの値はより大きく
それ以降b,c,tの値は小さくなるように
4つの周りの4点の値を使って
画素値を内装するという方法になります
これをバイリニア保管という風に言います
一方、バイキュービック保管といって
周りのより16点を使って
変換、内装するという方法があります
この周りの16点を使うと
より周りの滑らかな画素値が
その領域に対してどのように変動しているか
ということを見つけることができます
例えばバイリニア保管の場合は
x方向だけに関して考えてみると
ここがfa,ここがfbとすると
例えば今この値が
faの値がこれぐらい
fbの値がこれぐらいだとします
この時今この値がxとして
観測されたとすると
この値を計算することになります
すなわち周りの2点を使って
内装したというわけです
一方、バイキュービックは
このような周りの4点を使います
その時に例えばこのxが観測されたとして
同じようにこれぐらいの値と
これぐらいの値で
この時にこの周りの値が
もしこのような値だとすると
ここはこういう感じになっている可能性があります
したがってこの値が
内装されることになります
なのでこれを見てわかるように
バイリニア保管の場合は
周りの2点を使って
直線、実際は2次元空間ですから
平面としてこうやって内装します
一方、バイキュービック保管は
周りの点をこうやって使いますので
こういうような曲線上で
どこになっているかといったことを
求めますので
よりバイキュービックの方が
きれいな内装をすることができます
この結果については
テキストの方に画像がありますので
確認をしておいてください
では実際に画像の
機械変換した時の
内装による違いを紹介します
まず、細菌防法です
細菌防法を画像を拡大すると
やはりちょっとガタガタとしたような
形になっているのはわかります
続いて、バイリニ保管です
バイリニ保管することで
より先ほどの内装に比べて
よりきれいな保管ができて
拡大してもよりきれいな
画像になるというわけです
では続いて、機械変換を
利用した処理として
イメージモザイクについて紹介します
イメージモザイクは
非常に面白い処理でして
例えばどういったところに
使われるものかというと
航空写真でこのような画像を
いっぱい撮影します
これらの画像から
1枚の大きな画像を
作ったりするときに使われます
例えばGoogleのストリートビュー
これも複数のカメラで
撮影したものを
1枚の画像のように見せています
こういったところに
このイメージモザイクという処理が使われます
この処理手順は
まず画像上で特徴点というものを見つけて
その特徴点と特徴点が
画像間でどこが一致しているか
といったところを探します
これをマッチングと呼びます
そのマッチングした後に
変換するための行列
気化変換の行列を推定します
そして推定した変換行列を使って
画像の気化変換と合成を行います
ではこのイメージモザイクの
処理について説明します
まず最初に多様点を求めます
今3枚の画像A、画像B、画像Cがあるとします
それでは具体的に
イメージモザイクの処理手順を説明します
まず最初に多様点を求めます
多様点というものは
Aの画像の座標が
Bの画像のどの座標と
対応しているかという座標を
それぞれペアを求めます
これを4点求めます
4点求まると
4つの座標のペアから
3行3列の変換行列を
連想として解くことによって求めます
このHBAは
Bの座標系から
Aの座標系に
変換するための行列となります
この時気をつけなくちゃいけないのは
この対応点なんですが
この対応点は
同一3次元空間の中において
同一平面上にある点を与えます
これは3次元的な
この空間の位置合わせをするものではなくて
あくまでも画像から画像
すなわち2次元同士の
対応を求めるだけです
したがって
例えば床面と
それとは異なる平面である
この机のような点を
対応点として与えて
変換行列が求まるかというと
残念ながら求まりません
なのでこのような平面上の
同一平面上の対応点を与えて
その同一平面がどのように変換されるかという
機械変換できるかという
この行列を求めます
そして先ほどの
画像BからAへの
変換行列が求まりましたので
実際に画像Bを変換します
まず画像Aを置いておきます
この画像Aの座標形に
HBAという画像BからAへの
変換行列を使って
画像Bを変換すると
このような画像になります
そして先ほどの元々のAの画像と
Bの画像をAの座標形に変換した
この画像を合成して内装することで
このような一枚の画像になります
そしてじゃあ次どうするかというと
今度はこの変換した座標形
画像における点と
画像Cにおける同一の点の
対応点を与えます
ここではスライドでは
2点しか与えていませんけれども
実際に4点以上与えて
そして画像Cから
画像Aの座標形への
変換行列HCAを
連立方程式により求め
そして同じように一回変換をします
そうするとCの画像が
このように変換されました
そして先ほどの画像と
合わせることで
このように一枚の画像が
作ることができるようになるというわけです
この処理をイメージモザイというふうに呼びます
実際には先ほどは
対応点を手で与えましたけれども
この対応点を自動的に取るためには
特徴点というものを
牽引する必要があります
特徴点は何かというと
画像中の中で特徴的な点を言います
例えばコーナーみたいな点だとか
テクスチャーが非常に多いような所といったのを
自動的に見つけます
この特徴点を見つける方法には
Harrisのコーナー検出器だとか
あとはシフトだとか
サーフとかいろんな方法があります
この手法については
後ろの方の講義で紹介したいと思います
そのような検出した特徴点から
どの特徴点とどの特徴点が対応しているかを求める
対応点マッチングを施して
このように対応点を求めれば
その対応点から変換行列を求めて
気化変換をすることができるようになります
この特徴点を牽引する方法に
より優れた方法として
シフトというアルゴリズムがあります
これはスケールインバリアントフィーチャートランスフォームといいます
スケールというのは拡大縮小ですね
それに対してインバリアント
普遍という意味です
すなわちこの方法は
スケール変化に普遍な特徴変換器というものです
どういうものかといいますと
このような画像とこのような画像
画像と画像の間に拡大と回転が含まれています
こういうときどの点とどの点が対応しているかを調べるのは
非常に難しいのですが
このシフトという方法を使うと
特徴点ごとに大きさと方向を取り出してくれるので
その大きさを一定のサイズに
かつ方向を同じ方向に揃えてあげることで
この点とこの点は一致していますよということを
見つけることができるようになります
そうするとこのように
一枚の画像と拡大した回転した画像間においても
どの点とどの点が対応しているのかということを
マッチングすることがこのシフトによってできるようになります
今日はですね
画像の気化変換ということで
画像の気化変換における各処理手順と
その応用としてイメージモザイクについて説明しました
次回は画像から直線を牽引する
ハフ変換について紹介します
それではまた

みなさんこんにちは。今日は4回目です。今日はハフ変換について説明します。
まずですね、ハフ変換を説明する前に、僕はですね、中部大学で働く前、
赴任する前にいたカーネギーメロン大学での研究についてちょっと紹介したいと思います。
ご存知の人多いかもしれませんが、僕は皆さんと同じ中部大学を卒業しました。
中部大学で学部大学院の修士課程、そして博士講義課程を進んで、中部大学で博士号を取得し、
その後アメリカのピッツバグという町にあるカーネギーメロン大学という大学で研究員を3年間やってました。
ちょうど1997年から2000年です。今から20年くらい前ですね。
そのカーネギーメロン大学というのはロボット工学研究所というのがありまして、非常にロボットの研究では有名な研究所です。
ですから僕は中部大学を卒業した後ですね、ロボットで非常に有名な研究所でですね、3年間そのロボットに関する研究をすることができました。
そして今ですね、皆さんの前でロボットのビジョンの話をしているというわけです。
さあ、その時ですね、当時僕がやってた研究の一つにですね、ヒューマノイドビジョンプロジェクトというのがありました。
当時は1990年後半なんですけども、ヒューマノイドロボットがちょうど世の中に出てきたところです。
皆さん、ホンダが作るアシモ、ご存知ですか?
アシモの前にP2、P3というヒューマノイドロボットをホンダが研究開発をしていました。
それが最初に発表されたのがだいたい1997年ぐらいですね。
その頃に、ホンダが作った、世界で初めて作った2足歩行ができるヒューマノイドロボットに目の機能、視覚機能、ビジョン機能をつけようということで、
金木メロン大学のロボットコア研究所は視覚機能、ロボットビジョンの研究でも非常に有名な研究所でしたので、
そこでホンダとCMUで共同研究が始まったんです。
ちなみに僕はそのヒューマノイドビジョンプロジェクトの研究を担当していました。
今からお見せする動画は、当時研究をやっていた内容を紹介しているビデオになります。
ヒューマノイドロボットを使って、ヒューマノイドロボットが階段だとかドアを認識するような研究をやっていました。
残念ながら当時は輸出規制があって、ロボットをアメリカに送ることができなかったので、
実際のヒューマノイドロボットで動かすことができなかったんですけども、
そのヒューマノイドロボットを見立てて、カメラを実は3台つけて、
そのカメラから取得した画像からドアを認識したり、さらにドアが開いてる、開いてないを認識します。
例えばこのシーンでは赤色のドアは閉めてあるというわけです。
そしてこっちの方では複数のドアを認識していることがわかります。
ドアを認識するだけではなくて、ドアが我々が住んでいる3次元空間の中でどこにあるかということも理解しています。
これは次は階段の例です。階段の近くまで近づくと階段を認識するようになります。
例えば、ちなみに今ドアを開けていただいたのは本田から来ていたお定田さんです。
このように階段の形状を認識して、ロボットから見てどれぐらいの位置にどの大きさの階段があるかということをちゃんと理解できるわけです。
そうすることによってようやくヒューマノイドロボットがこのような階段を歩いていくことができるようになるというわけです。
この処理、僕は先ほどの研究の中で画像からドアを認識するというところを研究を担当していました。
当時やってた処理を簡単に紹介します。どういうことをやっているかというと、まずロボットに積んであるカメラから画像を取得します。
そしてその画像から今この講義で学んでいる画像処理の技術を使って、この場合はまず最初に地を抽出します。
エッジを抽出した後に今度は直線を検出してあげます。この直線はもうたくさんあります。直線の数だけ言うと500ぐらいから1000ぐらいの線が検出されます。
そしてそこからこの後にパターン認識、画像分類という技術を使ってドアらしい形状の組み合わせとなる線分だけを取り出すようなことをしていきます。
そうするとこの画像からどこにドアがあるかということをロボットは認識することができたというわけです。
さあ、じゃあこれでこのヒューマノイドロボットはここにあるドアのところまで歩いていくことができるのでしょうか。残念ながらできません。
今このドアを認識したというのは、この二次元画像上でどこにあるかということを理解しただけです。
残念ながらロボットは二次元座標からそのまま歩くことはできません。我々が今いるこの空間は三次元空間です。
ですのでロボットが立っている位置、ここからドアがどのような先にどれぐらい先にどこにあるのかということを向きも合わせて知ることができないとロボットはその中を歩いてアプローチすることができない。
この例ですと、二次元の画像から2枚のカメラ、先ほどの動画の中でカメラが複数だったのがわかるかと思います。
人間も目が左目、右目、左目であるようにステローシーをします。
ステローシーの技術によってこのような認識結果、ドアがそれぞれのカメラでどこにあるかわかれば、
結果、ロボットから見て1メートル、2メートル、3メートル、そして左に1メートル行ったところにドアがこのようにありますよということを理解することができるようになるというわけです。
さあ、今日はここです。画像からこの直線を検出するというアルゴリズムについて説明します。
さあ、皆さんご存知のように直線の式は y イコール ax プラス b です。この式の a が傾きを表していて、
b が y 軸の切片を表します。今、横軸が x、縦軸が y で、この直線式はこのような線です。
この時、角ですけど、傾きが a、切片が b ということになるわけです。さあ、この直線式をですね、軸が今は x と y という空間なわけなんですけども、
この式の中に出てくる a と b これをパラメータと言います。その a という軸と b という軸で空間を張ります。
これを a b パラメータ空間と呼びます。この x y 画像空間における直線 l は a b パラメータ空間においては一つの点になるということがわかります。
当然そうですよね。2の直線を決定するためには a と b はある値になっているわけです。
なので、くどいですが、この直線を a b パラメータ空間で表現すると、ある a と b という値の点で表現されるということになります。
さあ、では画像中の直線は実際はデジタル画像であれば点が並んでいるだけです。そうですね、エッジの点がたくさん並んでいるわけです。
なので、観測するものは x y 画像空間においてはエッジ上のある点になるわけです。ここではこの点を x 1 y 1 とします。
ではこの x 1 y 1 という画像空間、x y 画像空間における点を先ほどの y イコール a x プラス b の y と x にそれぞれ x 1 と y 1 を代入します。
そうすると今度はこの点の値を代入すると b i イコールマイナス x i a プラス y j という式が導出されます。
こちらの式は、例えばここが x がそうですね、例えば 3y が 5 だったとすると x がここが 3y が 5 ですから b イコールマイナス 3a プラス 5 というような式になるわけです。
これは見ていただいてわかるように、今度は a と b という a と b というパラメータ空間で直線になるというわけです。
当たり前ですよね。先ほどは x y 画像空間で直線だったものはその直線を表す a b が a b パラメータ空間で1点です。
今度今ここで説明しているのは、そもそもこの画像空間はデジタル画像なので点の集まり、画素の集まりです。
なので直線上に乗っている点を、1点を a b パラメータ空間に写像してあげると、今度は直線になりますよというわけです。
じゃあ次もう1点追加します。消しておきますね。じゃあ次は2点目が観測されました。
2点目の直線上の点 x 2 y 2 が観測されましたので、この点を a b パラメータ空間に写像します。
そうすると新たなもう一つ直線が観測できます。さらにもう1点 x 3 y 3 という点を同じように a b パラメータ空間に写像します。
そうするとまたもう一つ新たな直線が引かれるわけです。さあこの時こちらの a b パラメータ空間を見てみるとわかるように
交点があります。この交点というものは何かというと x 1 y 1 x 2 y 2 x 3 y 3 は同一直線上に乗っています。
ということはこの同一直線を表現する a と b のところでこの直線は交わるということになる。
すなわち画像上で点を観測できれば、この点を a と b のパラメータ空間に写像して、写像した空間における交点を求めることで元の直線のパラメータである a と b を知ることができるわけです。
さあここまでは数学の世界です。残念ながらこの今我々が使うデジタル画像では簡単にはうまくいきません。
前回もお話ししたかもしれませんが、この画像処理というのはいろんなアイディアが詰まってできています。
特にこの幅変換も数学だけでは解けないので、そこをどうやって工学的センス、エンジニアリング的な要素、エンジニアリング的な解決でうまく実現するかというところが非常に面白いところかと思います。
さあではデジタル画像ですとどういうことになるかということなんですが、本来この直線上にこのエッジが表現されて各点がですね、存在していればいいんですけども、黒いですけども、この x と y は連続的な空間ではありません。
理算的な整数ですよね。 なっています。なのでぴったりこの直線上にこの点が観測、そもそもされないわけです。
そうなるとどうなるかというと、それぞれの点を a、b パラメータ空間に写像すると、残念ながらこのように1点にさらわりません。
すなわち連立方程式で解けないということになるわけです。 さあじゃあそこでどうしようかということです。そこで考えた方法が1点に混ざらないけど、よく見るとそれなりに近いところに集まっている感は集まっているというのがわかります。
そこでこの a、b パラメータ空間をこのようにセルというものに分割します。 a と b は本来は連続値なんですけども、この a と b のパラメータ空間も
理算化しようというわけです。そしてこのセルというところに通過したところに投票をしていきます。例えば1つ目の直線に通過したところに正の字を書いていきます。
ここ通過して、ここ通過して、ここ通過、通過、通過、ここ、ここ、ここ。続けてこの2つ目の直線を投票していきます。
ここは同じところですが2票目が入りました。
こんな感じですね。次3票目投票していきます。3つ目の直線ですね。
という感じになります。これよく見るとここだけ投票数が3票となっているのがわかると思います。
すなわちこの紫で表示したここのセルですね。ここのセルはこの3つの直線がそのセル上を通過したということになるわけです。
すなわちこのセルに対応した a と b をこの直線とパラメータとして求めることになるわけです。
それではこのハフ変換をプログラムで実現するときのことを考えてみたいと思います。
先ほど説明したように、実際の画像中の座標は直線上にありません。そのためコーテが1位に定まらないので、どうするかというと、
a b パラメータ空間をセルに分割して直線上のセルに投票することをします。
このパラメータ空間に分割して投票するという処理は、2次元配列をプログラムで作ろうとすると、
2次元配列を用意して直線が通過する要素のところをインクリメントしてあげればプログラムとしてできるわけです。
c 言語のプログラムを作るときに最初にやることは、この配列の大きさを決めないといけません。
すなわち直線のパラメータである a と b がいくつからいくつの値の範囲を取り得るかということを考えて、
それを2次元配列の大きさとしてする必要があるわけです。
さあ、じゃあ y コード x プラス b の a と b の取り得る範囲を考えてみましょう。
直線はいろんな傾きがあります。切片もいろんなところがあります。
そうなんです。 a の取り得る値はマイナス無限大からプラス無限大。
同じように b も、b は画像の縦幅の範囲内ということが多いんですが、
実際に取り得る範囲としてはマイナス無限大からプラス無限大まで取り得る可能性があるわけです。
ということは、このような2次元配列を作ることができるかということです。
これはできませんね。ということで、残念ながら a b パラメータ空間をセルに分割して、
そのセルを直線上のセルに投票していくという方法はプログラムで実現できないということがわかります。
じゃあ、そこでどうすればいいかというと、この範囲がある程度の大きさの中に収まるようになればいいわけです。
すなわちパラメータ空間の大きさを限定してあげましょうということで、
そこでこの下の式、ここで出てきた row イコール x コサインシータープラス y サインシーターという式を使って投票を行います。
これどういう意味かというと、row は原点から直線までのこの距離を表します。
シーターはこの角度を表します。それで実際に波幅変換をしてみます。
先ほど直線上にある点が観測されました。
その各点 x i y i の座標を先ほどの row イコール x コサインシーター y サインシーターという式の x と y に代入します。
そうするとこの式は横軸がシーター、縦軸が row になります。
シーターを 0 から 2π まで変化したときにこの式を計算した結果、
row の値がこうなって角度ができる。
すなわち一つの直線に対してこの1個のサインカーブが、この2つのやつに対しては2つ目が、
そしてこの3つ目には3つ目のそれぞれのこの正弦波が表現できます。
そしてこの時この直線上に乗っている点であれば、
ロー、シーター、パラメータ空間においても交点が観測され、
その交点となるローハット、シーターハットがその直線の同一線上に乗っているということになります。
このローとシーターを求めてから直線式に変換してあげることで、
直線のパラメータである a と b を知ることができます。
もちろんこのロー、シーター、パラメータ空間においても一点にぴったり定まりません。
これはなぜかというと、xy 画像空間が理算的な空間、整数座標形だからですね。
なのでこの場合も同様に下に書いてあるように投票空間をセルに区切って投票します。
この場合、ローとシーター、シーターは2π、実際はここはπまででokです。
π以降、180°以降は反転しているだけなので、交点は1点だけ見つければいいのでπまでで大丈夫です。
じゃあローに関して考えると、ローはxcosθ、ysinθですから、
もし画像が640×480とすると、最大値はこのxの640cosθが0度のときは1になって640になりますね。
なので、すなわちこのローの最大値は画像の大きさによって決定することができる。
すなわち、ローとシーターともにある範囲内の値にちゃんと収まっていますから、
これを二次元配列として表現して投票していくことができるようになる。
もう一度では、ハフ変換の直線検出の流れについて説明します。
まず最初、一番です。画像から直線の候補座標となるエッジを検出します。
このエッジを検出する方法には、ソーベルフィルターなどを使います。
そして、各検出したエッジの点の座標について、傾きシーターにおける距離ローをこの式で算出します。
ここでは、xiとyiを代入します。ある座標というのはxyiを表します。
そして傾くシーターに対する距離ローに対するセルに投票していく。
この処理ですね。この2番から3番という処理をすべてのエッジの点に対して2と3を繰り返します。
ただし、エッジの強さが非常に小さいところは、そもそも直線である可能性が少ないので、
そういった点は投票に使いません。そして2と3を全画像中で投票した後、
投票が一定数以上あった傾きのシーターとローを求めます。
そしてそのシーターとローから、今度はxとyにこの式を使って逆変換すれば、直線のパラメータとなるというわけです。
ではですね、実際に処理例を見ていきましょう。
この画像わかりますか?これはiPodです。初期のiPodです。これはちなみに僕の私物です。
ちなみにこのiPodはですね、初期型、一番最初のものなので、
なんと中身はですね、SSD、メモリーではなくてハードディスクです。
物理的にくるくる回るハードディスクで入っていた時代のiPodです。
このiPodも脱線していますけども、iPodは非常に画期的な製品で、
今それまではですね、音楽、例えば自分が持っているCDですよね。
CDの音楽をすべて外に持ち出すなんてできなかったわけです。
それまではCDプレイヤー、CDとCDプレイヤーを持って行っていくので、
家にある大量のですね、CD、音楽CDを外に持っていくことって普通はできなかったわけですね。
だけどこのiPodによって、すべて持っている、僕が持っている音楽をiPodに入れて、
それでどこでも聴けるっていうのは衝撃的な体験でした。
それぐらい非常に思い入れのある初期のiPodです。
それは良いとしても、今は普通ですけどね、iPhoneとかでも普通にできますけども、
さあ、それは置いておいて、半変換をやります。
まずこの入力画像に対して、ソーベルフィルターで字を抽出します。
そしてエッジの強度が大きいところだけを抜き出します。
こういうのを2値化と言います。
ある、例えば100以上の強度を持つエッジのところだけを取り出すと、
そのエッジの点だけを黒、それ以外を白にすると、
こういう2値化といってエッジの点だけを取り出すことができます。
さあ、ではこのエッジ上の点1点1点をローシータパラメータ空間に写像するわけです。
そうするとこの点はかなりたくさんありますよね。
なのでそれに合わせていろんな曲線が描けるわけです。
このローシータパラメータ空間を拡大してみると、
こういったセルに1個を区切ります。
例えばこれぐらいの大きさの

セルに区切って投票すると、投票数が多いところが
1、2、3、4、5、6、7、8、9、9点出てきます。
結果、この9点に対応した直線を元の画像で描くと
こういった直線になるわけです。
これ、複数の直線が観測されているのが分かると思いますが
まあでも、iPodのこの4つの直線を
ちゃんとそれぞれ1、2、3、4含むようなことができて
ただし、ここの線上も複数の直線が
ここ分かりますかね、これ複数の直線が
検出されていることが分かります。
これはなぜかというと、先ほどセルに区切ったときに
このような近いところに複数投票があるわけですね。
そこで、じゃあどうするかというと
セルを少し大きめにして投票をします。
そうすると1、2、3、4と4つが
投票が多いセルが観測されて
これを元の画像で直線を表現すると
このような1、2、3、4本だけになるというわけです。
ただし、残念ながらセルを大きくすると
どうなるかというと直線の当てはめが
少し精度が悪くなります。
そうですね、ぴったりと、こういうふうにぴったりと
本当はあってほしいんですけども
セルを大きくしているので
残念ながら直線の精度が悪くなるということがあります。
これは求める対象としている画像において
どのような直線を検出したいか
たくさんの直線を検出したいのか
少しぐらいパラメータが違うのも
別の直線として検出したいのか
そういう問題に合わせて
先ほどのセル空間をどれぐらいのサイズに区切るか
ということを設定してください。
はい、ではですね、ここで皆さんに質問です。
今までは直線をどのように検出するか
といったことを説明しました。
ではこのような画像から円を検出しましょう。
どうすれば円が検出できるか考えてみてください。
では今から一旦動画を一時停止して考えて
その考えたアイデア
こういうふうにできるだろうというのを
ここにですね、記入しておいてください。
はい、では円をどうやって検出するか続いて説明します。
円、先ほど直線を検出するときには
まず直線の式を考えました。
同じように円の式を考えてみましょう。
円の式はここにあるように
x-aの次乗プラスy-bの次乗イコールrの次乗になります。
よく知っているかと思いますが
aは中心座標のx座標
bは中心座標の円の中心のy座標を表します。
そしてrは半径を表します。
ということはこのような画像空間において
円上の点が4つもし観測されたとしますね。
その時にではこの1つの点に対して
同様にこの場合パラメータは
aとbとrとあります。
この3つですね。3つありますので
本来はこのパラメータ空間は
aとbとrなんですけれども
ここでは一旦rを10と固定にします。
そうした状態でaとbとパラメータ空間において
この点を斜像します。
この場合1個の点がaに与えられると
その点半径、この場合rは10というふうに設定していますので
この点を中心とした半径10のところが
こういった斜像した円になるわけです。
点が円になるわけです。
この点を中心に半径10で描いた円です。
今度は2つ目の点を中心に描いた円。
3つ目の点を中心に描いた円。
4つ目の点を中心に描いた円になります。
この時見てわかるようにここですね。
同じ半径が10
もし元々観測している円が10であれば
ここが1つ交点として交わるわけです。
すなわちこの交点の位置が半径のパラメータ
ごめんなさい、円のパラメータであるaとbを表すということになります。
さあ画像を入力して
実際にどうやって円を検出するかという処理の流れです。
まず画像を入力します。
この画像をよく見てみると
ちなみにこれは1セント
これは10セントかな
この1セントとこの10セントこれも僕の私物です。
日本円にすると1円と10円ぐらいですか。
これよく見るとここを見てください。
この1セント、これ欠けてますよね。
綺麗な円じゃありません。
日課した例を見るとよくわかりますね。
このような状態でちゃんと円が検出できるかということを試してみます。
まずソーベルフィルターをかけてエッジを抽出して
エッジの強度の強いところだけを抜き出しました。
そして投票していきます。
これがまず投票する前のエッジの各点です。
各点を中心に円を描いていくわけですね。
まず半径を二重として投票した結果です。
こんな感じですね。
まだ1点に定まっていません。
では次半径を大きくしてまた投票します。
今度半径が30です。
まだまだですね。
今度は半径を40です。
これもまだまだであるというのはわかります。
続いて半径が54。
実際は半径は細かく変えていきます。
ここは半径を54としました。
そうするとこちら側を見ていただくとわかるように
1点に投票が集中していることがわかります。
続いて半径を大きくします。
半径は60です。
60にすると今度はこちらが1点に集まりました。
という形で今度は半径70。
どんどんどんどん大きくしていきます。
そうすると1点に集まっていたのがまた離れていってしまいます。
半径80です。
先ほどの各半径ごとに最大投票数をプロットすると
こんな感じになります。
これを見るとわかるようにピークというのは
こういうふうに大きく尖ったところですね。
そこを調べるとまず1つ目のピークがここ。
2つ目のピークがここになります。
1つ目のピークは54。
ちなみにここの値が54でしたので
この54というところですね。
ここですね。
ここが一番最大投票数が多かった座標になりますので
この点を中心に半径54の円を描くと
こんな感じになるわけです。
こちらの円の中心と半径がちゃんと合っていることがわかります。
しかも欠けたところがあっても大丈夫です。
なぜかというと他の円周上の点がちょうどぴったり合っていれば
投票が1点に集まることです。
もちろん欠けたところの投票は違う中心に投票してしまいますが
より多くの円周上の点があれば
投票によってちゃんと求めることができるというわけです。
じゃあもう1点。
こちらの半径60の方ですね。
ここの座標を中心に半径60の円を描くとこんな感じになります。
こちらもちゃんと中心座標と
それがちゃんと合った半径60というのを
計算することが求めることができたというわけです。
このように円圏位数のパラメータは
円圏位数におけるパラメータはA、B、R
直線の場合はAとBだけでした。
パラメータ数は増えていますが
実は円の方が比較的楽ですね。
なぜかというとAとBとRが取りうる値というのは
必ず画像の大きさの中に入っています。
そうですよね。
画像の中心は必ず画像の中に見えているとして
半径が画像よりも大きいとはみ出ちゃうから
そもそも円が観測されません。
なので円を検出する場合は
実はA、B、Rというパラメータが
ある範囲の中に収まっているので
直線の時のようにローシーターといった
異なるパラメータ空間を使わなくても
直接うまくいくというわけです。
これが円圏位数の仕組みです。
まず、ハフ変換として
直線の圏位数と円の圏位数について説明しました。

はい、では続いて画像のフーリエ変換について説明します。
そもそもフーリエ変換はデジタル信号処理の中でも出てきたものだと思います。
フーリエ変換はいろんなところで使われます。
もちろん画像処理においてもこのフーリエ変換を使うことがあります。
まずどういったところで使うかということなんですけども、
フーリエ変換することによってできることは、
周波数成分、その画像の中に含まれる周波数成分を
解析することができます。
したがっていろんなテクスチャーを解析するのに使われます。
例えば、木目みたいなテクスチャーであるだとか、
タイル状のテクスチャーであるだとか、
いろんなこういったテクスチャーの違いを見つけるときに
周波数成分を使って観測することがあります。
では画像ですね。どのように周波数成分、
画像から周波数成分を獲得するかということなんですが、
まずここでは最初から2次元で考えるのではなくて、
1次元の信号におけるフーリエ変換を説明します。
フーリエ変換を説明する前に、
まずフーリエ変換の原理として、
2位の周期関数は三角関数の和で表されるということを説明します。
どういうことかというと、最終的にこのような波形があったとします。
このような波形は、まず1周期における1Hzにおけるこのような三角関数と、
次は1周期、2周期、3周期、
大体3周期強の制限波と、
1周期、2周期、3周期、4周期、5周期弱の制限波と、
さらには1、2、3、4、5、6、7、7周期の7Hzの三角関数があります。
ここで見ておくべきものはこの高さです。
この高さがここの高さと一致しています。
続いてここの幅がここの幅、ここの幅がこの幅、この幅がここの幅になるというわけです。
この波形とこの波形を組み合わせたものがこのような波形になって、
さらにこの波形にこの周波数を組み合わせるとこのような波形になって、
この波形にこの制限波を組み合わせるとこのような波形になります。
要はこの波形というものは周波数、この1の振幅がこの幅の制限波の成分と、
この周波数のこの高さの成分と、
この周波数のこの高さの成分と、
この周波数のこの高さの成分の制限波が和としてこのような複雑な波形が表現できているわけです。
なので2の周期関数、必ず周期性を持たないといけませんが、
周期性のある関数であればこのように三角関数の和として表現できる。
この時各周波数ごとのその成分の大きさといったものを
縦軸に振幅として表現したものがいわゆるスペクトルといわれるものです。
フリエ変換はこのような波形が観測された時に、
このような波形の中に各三角関数の成分がどれぐらい入っているかということを求めることになるわけです。
ではこれを式で書きます。
フリエ変換の原理として周期関数をFTという式で書くとこのようになります。
これは何かと言いますと、まずA0と言います。
A0はここにもあるように直流を表します。直流性です。
その後、A1、A2、A3といった係数がかかった
コサイン4πt、コサイン6πtということで周波数が2πtに対して2倍、3倍という風に変わっていくコサイン波が
それぞれの係数A1、A2、A3、A4という風に続くというわけです。
すなわち周期t分の1の整数倍、2に対してこれ2倍してますよね。
今度これに対して3倍になってます。ということで4、5、6という風に続くというわけです。
これがコサイン成分の成分を表していて、もう一方B1、B2、B3というのがあります。
これもよく見るとわかるようにサイン波、サイン成分でこの周期t分の1のものに対して
同じようにここは2πtですから2倍、ここは3倍、ここも同じように4、5、6という風に続くということになるわけです。
このように2の周期関数はこのような式で表現できますよということを言っているわけです。
このA1からたくさんあるAnとB1からだーっとあるBnをちょっと隠れてしまってますが、
風利恵係数というわけです。風利恵変換をやることは何かというと、
このAからAn、Bnという風利恵係数を元の入力された波形の中から風利恵係数を求めることが風利恵変換のやることになる。
なのでそれぞれ直流成分A0、コサイン波の成分An、サイン波の成分Bn、風利恵係数を求めましょうということをします。
これはどうやって求めるかというと直流成分は単純ですね。
入力されたftという信号を0からtまで積分して、積分ということは面積を求めるものですから、それをtで割ってあげれば平均、すなわち直流成分を計算することができます。
一方、こちらのAとBですけれども、Aに関してはA1の場合は、もしA1を求めるということであれば、
A1の場合はt分の1、0からtのftのコサイン2πt分のt、dtとなります。
これはこのftとコサイン2πtですね、この波形ですよね。
この波形とこのftの相関を求めていることです。
こことここの相関を求めていることになります。
この相関というのは何を求めているかというと、似ていれば似ているほど値が大きくなる。
なので、もしこのftに含まれている成分の中に1Hzのコサイン波が入っていれば、この掛け算した値は非常に大きな値になるという。
これでこのAの値を1から次に2にして、ここが4πt、ここを3にすると6πtということで、
元の波形とその周波数のコサイン波との相関を見て、その相関がどれくらい大きいかということがそれぞれのAnを計算することになります。
同様にBnはサイン波で、周波数1、周波数2、周波数3のサイン波との同様に相関を計算して、相関が高いということは、
そのft元の波形の中にその周波数成分のコサイン波、サイン波が多く含まれているということを表しているという。
これが風流変化になります。
ぜひプログラムを作ってみるとわかりやすいと思います。
このプログラムはまずこのような入力信号を作りました。これがftです。
このftを入力したときに出てきた風形係数がAnとPnになります。
そもそもこの入力はどうやって計算しているかというと、
これはわざと小さくて見づらいかもしれませんが、皆さんはpdfの方でちょっと見ておいてください。
これ見ていただくとわかるように、周波数1と2と4の波形を組み合わせて入れています。
なのでこのAn、Bnを見ると、しかもコサイン成分だけですから、
ここのサイン波のところを見てみるとほとんどゼロだということがわかります。
一方、コサイン波を見ると、ここの1のところと2のところと4のところに非常に大きな値になっていると思います。
係数を見てみるとわかるように、1、2、4という係数が変わっていますから、
8、16、32ですから、ちょうど2倍、4倍という風に振幅が変わっているのが確認できます。
同じこのような波形を生成して、それをここの風理変換によって求めたところ、
AnとBnにはこういう値が出力されたという風理変換の原理ということになります。
画像における風理変換なんですけども、非常に単純で、先ほどの風理変換を画像のx方向とy方向に合わせてやることが
二次元風理変換ということになります。なので画像はxとyがあります。
この画像空間を二次元風理変換すると、あるUとVの周波数、横方向の周波数成分と縦方向の周波数成分における
UとVにおける周波数領域の成分として表現できます。
ちなみに風理変換は逆変換ができます。
当然、二次元風理変換においても、この周波数成分から元の二次元風理へ逆変換をすることによって、
元の画像空間に戻すことができます。
では実際に二次元風理変換の例を見てみましょう。
この上の画像は顔のいろんなパーツが含まれています。
特に髪の毛ですね。細かい成分が入っているのがわかります。
細かい成分とはどういうことかというと、
例えば横方向に見ると強弱、振幅が激しいわけですよね。激しく変化している。
ということは周波数成分が比較的高いものが含まれているということがわかります。
実際にこの辺りですかね。
比較的高い成分が白く映っているということは、高い周波数成分が含まれていますよということがわかります。
一方、下の例を見てみますと、この下の例はかなりぼけた状態のものです。
周波数成分が高いということは、急激な変動が起こっているものが含まれているということですが、
この場合は平滑化されているような状態に近いわけですから、急激に変動があまりありません。
なので上の画像と比べると、周波数成分を見ると、
より低い周波数、これ真ん中に行けば行くほど0,0、低周波になりますから、
低い領域にですね、上の方は広く分布しているに対して、下の画像は狭い範囲にしか周波数成分がない。
すなわち低い周波数成分だけであるということが、このフーリエ変換した画像の結果から、
この2つの画像の性質を分けるというか、知ることができるわけです。
では他の画像を見てみましょう。これは指紋画像です。
ちなみに僕はですね、大学4年生の時に、この指紋画像の称号の研究をやっていました。
この指紋画像は非常に面白いんですね。
この指紋画像を2次元フーリエ変換すると、このようなリング状の周波数成分が観測できます。
なぜか、横方向に見ても白黒白黒とかパターンが変わりますよね。
斜め方向に見ても同じように白黒白黒とかパターンが変わります。
縦方向に見てもまた白黒白黒とかパターンが変わります。
すなわち、ある一定の周波数のところに、いろんな向きの、
すべての方向に対してある一定の周波数領域のところに、
この辺ですね、こういったある一定のところに、
この指紋の情報が載っているということがわかりますね。
なので、このフーリエ変換のパターンを見ることによって、
元の原画像がどのような構造を持っているのかということも比較的知ることができます。
下の例は、絨毯の例ですけども、これは細かいパターンが含まれていますので、
このようなパターンになります。
さあ、これは木目のパターンです。
この木目のパターン、まず上を見てみましょう。
上は横方向に見ると木目が縦に発生していますので、
かなり変動が激しくなるのがわかります。
すなわち、横方向に対しては広い範囲に対して、
低い周波数から高い周波数まで成分が含まれていますよというわけです。
一方、この画像を90度回転しました。
90度回転した画像の二次元フーリエ変換をすると、周波数成分もこのように変わります。
横方向に見ていくと、あまり色の変化がありませんね。
だから横方向には周波数成分が、
低周波はありますけど、高い周波数成分はないということがわかります。
今度は縦に見てみましょう。
縦に見ると、先ほどの横に見た時と同じようなことになりますから、
変動が激しくなります。
明るく暗くなったり、明るく暗くなったりします。
なので、その成分がこういうふうに出てきているというわけです。
なので、この木目の向きに合わせて周波数成分がどういうふうに表現されているかというと、
向きに合わせてフーリエ変換のパターンもこのように変わってくるということがわかります。
今度は画像を一旦フーリエ変換して、
その周波数成分である成分をなくした状態をまた使って、
今度は逆変換して元の画像に変換するとどうなるかということをやってみたいと思います。
まずこの入力画像を2次元フーリエ変換するとこのような周波数成分が得られます。
ここで真ん中のカットオフ10以上のところをカットしました。
ここだけを残して、すなわち低周波成分だけを残して、
もう一回逆フーリエ変換してあげるとこんな画像に変わります。
当然ながら高い周波数成分がなくなっちゃいます。
先ほど多分この辺にいろいろありましたね。
こういった高周波の成分がなくなってしまうので、
この領域だけを使って復元した画像はこのように受けたような画像になってしまうというわけです。
続いて今度はカットオフする周波数をちょっと広くしました。
すなわちこれぐらいまで大きくしました。
そうするとやっぱり先ほどより少し高い周波数成分が含まれるようになっていますので、
それに合わせて顔の画像も少し細かいパターンが見えるようになってきたのがわかるかと思います。
続けて今度カットオフを30にします。これぐらいです。
かなり多く含まれるようにしました。
そうするとほとんど見た目があまり変わらないということがわかります。
すなわちこの顔画像の周波数成分は30以下の中に含まれているということがわかるわけです。
次は40です。40にするともうほとんどこのような形で変化がありません。
こうやって風利変換をした結果、周波数領域である成分をなくしてあげて、
それを逆風利変換することでいろんな画像の変換ができるというわけです。
さあでは今度は逆をやってみましょう。
先ほどは低い周波数成分を残して高い周波数成分をカットしました。
今度は逆です。低い周波数成分をカットして高い周波数成分だけでどうなるかということで
ハイパスフィルターをやってみたいと思います。
指紋画像を2次元風利変換すると先ほども説明したように
ある周波数のところにリング状にこういう周波数成分を観測することができます。
ここで低周波のこの部分をカットします。ここをカットしちゃいました。
そうするとこちらの逆風利変換した画像はどのようになると思いますか。
一度想像してみましょう。この真ん中は直流成分でした。
ちなみに画像は0から255の値ですから正の値しかありません。
正の値があるしかないということは確実に直流成分が存在するという。
したがってこの真ん中の直流成分をなくして逆風利変換すると
こんな感じで色がドーンと落ちてしまいます。
直流成分がないから値が下がってこういう画像になります。
だけど指紋自身のこのリング状の周波数成分が残っていますから
この画像を見てわかるように指紋のこの凹凸の成分は
ちゃんと見て取れることがわかります。
ではカットオフ周波数を少し上げましょう。
少しこれぐらい上げました。
これを上げてもまだ指紋の領域のリング状の分布が残っていますので
逆変換した場合はこんな感じで指紋のパターンがちゃんと見て読み取れます。
さらにカットオフ周波数を上げます。半分切れちゃいました。
そうするとだんだんちょっとパターンがこのように崩れてきます。
指紋のパターンが見づらくなってきました。
さらにもっとカットオフを上げるとほとんど指紋のリング状の領域がなくなってしまいましたので
それで逆風里変換するともちろん指紋のパターンが見えなくなるというわけです。
このように風里変換、二次元風里変換して
その周波数成分がどういうふうに含まれているかだけでなく
その周波数成分で必要なところ、不必要なところをカットして戻してあげることによって
逆風里変換することによっていろんな効果を得ることができるわけです。
ではこの二次元画像風里変換を試してみましょう。
ここにURLのところにデモンストレーションが公開されています。
非常に面白いデモになっていますのでぜひ試してみてください。
まず画像を選択します。選択した画像からクリックをして
クリックをした周波数成分がこのようなパターンで
これらの全てのこの白の領域のみの周波数成分を使って戻したパターンがこの画像になります。
すなわちこの顔の周辺の領域はこの辺の周波数に含まれているということがわかります。
もう一度今度は違うことをやってみましょう。
今度は高周波の成分だけをパターンを超えて作って
その逆風里変換した数を作ってみます。
この場合いつまで経っても顔画像がここに出てきませんね。
これなぜかというとこの復元している周りのところの高周波成分には
そもそもこの顔の周波成分が含まれていないから
復元した逆風里変換した画像の中に顔が現れないというわけです。
ただし細かいパターンが復元できているのがわかるかと思います。
ぜひこのサイトに行って試してみてください。
では今日はこれで終わります。それではまた。

皆さんこんにちは。では今日は日課について説明をします。
まず日課を説明する前に、これまで扱ってきたグレースケール画像を
そもそもカラー画像からグレースケールにどうやって変換するかという例を紹介します。
そしてその後、そのグレースケールの画像をどのように日の値に変換する話を説明します。
まずRGBカラーなんですけども、そもそもRGBカラーというものは赤とグリーンとブルーで表現する
RGBの3つの空間で表現されたカラー空間ということになります。
当然この元のところが0、0、0で黒を表して、この一番端のところが255、255、255で白を表しています。
このような空間をRGBカラー空間というふうに呼びます。
このRGBカラー空間はこの値が小さいほど白で、上に行くほど白になっているわけなんですけども、
例えば赤み全体的に色の感じを変える、好ましい色の雰囲気に変えるということがなかなか難しい値です。
それに対してHSIという変換方法があります。
このHSIのHは色相であるヒュー、Sはサチュレーションであるサイド、Iはインテンシティである明度を表します。
このHSI変換をRGBカラーからHSIに一旦変換します。
まずわかりやすいIから説明します。
Iはインテンシティで、このHSIの中ではこの縦軸を表します。
一番下が黒で、上に行くほど白になります。
真ん中のあたりはグレーになるわけです。
そこである例えばグレーのところで区切った断面がこのような断面になります。
この断面はIイコール0.5、すなわちグレーの時の断面になります。
この断面の中はこの断面はこのように円周状になっています。
ということは0から始まって2π周期、ぐるぐるぐるぐる回るような色になっています。
これをこのシーターをHUEと言います。
HUEは色相です。
例えばHが0の時は赤、90度の時は紫というふうにシーターに対応する値、Hが色に対応します。
さあもう一つサイドです。
このサイドという方法は何かというと、色の鮮やかさみたいなのを、色がどれくらいついているかということを表します。
例えばこのIイコール0.5における断面の真ん中はグレーですから色がありません。
この真ん中から外に行けば行くほど色が鮮やかについていくということがわかります。
これが中心からの距離がサチュレーションであるSになるわけです。
なので例えば今画像のカラーの値がこの中心に寄っているような値しかない場合は
Sの値を全体的に外に出してあげれば、大きくしてあげれば色鮮やかなカラー画像に変換することができます。
RGBカラー空間では鮮やかにするということがなかなかできません。
なんとなくRもGもBも値をですね大きくしてあげればいいんじゃないか。
RGBカラー空間にすれば大きくすればいいんじゃないかというふうに思うかもしれませんが、
値を大きくすればそれはここから白の方向に向かうわけですから、全体的に画像が白っぽくなっちゃうんです。
なので一度RGBカラーをHSIに変換して、例えばサイドのSを外に大きくなるように広げて元の画像を見ると、
より前より色彩豊かな鮮やかな画像に変わったりすることができます。
このような式、こちらに書いている式を使ってRGBの値からそれぞれHSIに変形します。
ここではちょっと細かい説明は省略します。
このHSIのIに注目すると、Iは黒から白のグレーですから、
まさにカラー画像をグレースケールの画像に変換した値と変換したものとして扱うことができるわけです。
これがHSI変換におけるIntensity Iを扱うアプローチです。
他にはどういう方法があるかと言いますと、テレビの企画で使われているPAL企画というものでは、
軌道信号と色差信号にRGBの値を変換しています。
軌道信号のことをY、色差信号をUとVと言います。
これはRGBのカラーの値をまず軌道であるYに変換するときに、
Rには0.299という係数を、グリーンには0.587、ブルーには0.114という係数をかけます。
この係数の値はどうやって決められたものなのでしょうか。
皆さん、色を見たときに、その色が持つ明るさという印象があると思います。
例えば、緑と青を比べてみましょう。
緑と青を比べた場合、なんとなくやっぱり青の方が暗いイメージがあると思います。
緑の方が明るいんじゃないかというのがわかると思います。
例えば赤と緑でも比べてみると、やっぱり赤の方が少し暗い印象になると思います。
なので、我々人間がカラーの明るさをどのように感じているか、その度合いをこのように数値で表現した。
結果、明るさに表すためにはグリーンの成分が大きく、
次にレッド、そしてブルーがこういう係数になったというわけです。
ちなみに、このUとVは色差信号と言いまして、色と色の差をとります。
差をとって表現しているものはUVという方法です。
このYUVを使ってカラーを表現しているというのは、テレビで使われているパル規格というものです。
ちなみに、このUとVはなぜ差をとるかということなんですが、
RGBで表現したときは、それぞれRGBが最大の値までとります。
YUVに変換するとYは最大の値をとりますが、UとVは差をとっている分、値が少し小さくなりますので、
そういった効果があるというわけです。
グレースケール画像にはこのYUVのYが使われたりします。
では実際に画像の例を見てみましょう。
これはカラー画像です。
このカラー画像をHSI変換のIだけにして、要は彩度を0、S、サチュレーションを0にして表示すると、このような画像になります。
結構グレースケールになっていると思いますが、ちょっとこの辺は違和感が感じるようなところもあるかもしれません。
では続いてRGBの成分を単純に足して3で割った平均を見てみましょう。
そうするとこのような画像になります。
割りかしRGBを足して3で割っても、比較的良さそうなグレースケール画像になっていることがわかると思います。
では続いて最後に紹介したYUVの軌道信号であるYです。
Yを採用して軌道信号をグレースケールで表現するとこのようなグレースケール画像になります。
このYUVを見るとかなりカラースケール、カラー画像の印象を残したままグレースケールにうまく変換できていることが確認できると思います。
ではそれぞれの方法がどういうふうに変換しているのかもう少し詳しく見てみましょう。
HSI空間における彩度を0にした場合はこのように色が変わっていたとしてもすべて同じ値になってしまいます。
なので色の違いによる明るさを彩度を0にした方法ではうまく表現できていないということがわかります。
次にRGBの値を足して3で割ったという平均を計算した方法です。
この場合は色が変わっていくにつれて明るさが変わっているので上の方法よりもいい方法であるというのが見て取れます。
しかし例えば赤色のところ、緑のところ、ブルーのところを見るとRとGとBの平均をとっていますので
RとGとBがそれぞれ同じ強い値であるところはここを見てもらってわかるように同じ値になってしまう。
すなわち赤、緑、青の色の明るさの差がこの平均値の計算方法では表現できていないということがわかります。
続いてYUVです。YUVのYはRに0.299、グリーンに0.587、ブルーに0.114という係数をかけていますので、
同じようにR、G、Bのところを見るとグレース系の色が変わっていることが確認できると思います。
このようにYUVのYを利用することでより綺麗なグレースケールに変換できることがわかります。
では3つの方法を並べて比較してみましょう。
このカラー画像に対してHSIのS、サチュレーションを0にしてインテンシティを表示するとこのようなグレースケール画像。
RGBの平均をとったグレースケール画像。そして最後にYUVに変換したYの軌道信号を表示したものです。
こうやって並べてみてみると、1番、2番、3番、3つのうちどれが元のカラースケールの画像をよりグレースケールに変換しているかということが比較できると思います。
見て通りこのYUVのYがより綺麗な元のカラー画像の明るさの印象を残したような形でグレースケールに変換できていると言えるでしょう。
これでグレースケールの画像に変換することができました。
続いては日課を行います。
日課というのはグレースケールの画像を白と黒の2種類の値にすることを日課というふうに呼びます。
ではこの日課をどういったところに使うかということなんですが、
皆さんよく知っている文字認識だとかQRコードの読み取りの前処理としてまず日課を行ったりして、
例えば買い物したときのレシートをカメラで撮影してそこから買ったものをテキストで起こしたりすることができるようなアプリがあります。
あとは名詞をカメラで撮影すると名詞上の文字だとか名前とか住所をテキストで起こしてくれるようなアプリがあります。
そういったアプリはまずグレースケールで得られた画像から日課を行って文字のところと背景を白と黒で日にして表現しているわけです。
そうするとこの文字のところだけを取り出してきて、
アルファベットB、O、T、Hというのを認識することができるようになるわけです。
なのでこの日課という処理は文字認識だとかQRコードも同じようにカメラで撮影した後はこういうグレースケールもしくはカラー画像です。
それをグレースケールに変換した後、QRコードの四角形がありますのでその四角形を認識するというところには日課が使われているというわけです。
さあ日課というものは今ではバイナライゼーションというふうに呼ばれます。
これはグレースケールの画像0から255の値を0と1の2つの値に変換をします。
式で書くとこのような式になります。
これはまずfxyが入力画像になります。
なのでこちらがF、こちらがGになります。
xとyで指定した座標の画素値fを入力したときにこれが式値t。
式値tよりも大きい画素値はすべて255にしますよというわけです。
入力した画素値がこの式値tよりも大きいところはすべて255の値にしますよというわけです。
それ以外のところはこのotherwiseというもので表現します。
それ以外のところは式値tよりも低いところはすべて0ですよというわけです。
さあこの日課を行うためにあらかじめ決めないといけないのがこの式値tになるわけです。
この式値をどのように設定するのかという方法にPタイル法、モード法、判別分析法という手法が提案あります。
では順番にこの方法を説明します。
まずPタイル法です。Pタイル法のPはパーセントです。
この名前を聞いてなんとなく想像できるかもしれませんが、どういう方法かというと、まずノータンヒストグラムを計算します。
ノータンヒストグラムのこの黒の面積は画像の面積サイズと一致しますね。
そこでこの画像値の低いところから頻度の累積を計算していって、もともとその画像中にどれぐらい何割のパーセンテージの物体が含まれているかということを決めておくことによって、
そのパーセンテージに対応する画像値を超えたときにその画像値を式値としましょうという方法です。
そうすると物体の画像値に含まれる割合をある0、それ以外を1とかというふうに設定しましょうというわけです。
例えば全画素数が300×150という画素数であれば、そこにおいてあらかじめその画像の中にどれぐらい物体の画素数が含まれているかということを予測しておきます。
180×100ぐらいだっていうことであれば比率は0.4になるので、0.4という値に対応した式値を採用しましょうというのがPタイル法になります。
続いてモード法です。モード法という方法は、そもそも日誌化をしたい画像は明るいところ、
例えば背景の紙の明るいところと文字の黒いところの2つに分かりやすいわけです。
すなわちこういう2つの山を持つような濃淡ヒストグラムの分布が得られることになります。
であれば山と山の谷を式値として採用すればいいんじゃないかという方法がこのモード法という方法になります。
この方法は概念としては非常にわかりやすいんですが、
実際に谷をどのように定義するかによってなかなか決定方法が実は難しかったりします。
きれいな谷ではなくてガタガタしたりしてますので、どこを谷として採用するかということが問題となったりします。
まずPタイル法における割合を変えたときに、どのように日誌化画像が変化するかを見てみましょう。
全体の画像の中の20%を物体領域として日誌化すると、式値が53というふうに計算されてこのような画像になります。
40%だと式値は171、60%だと198、80%だと式値が202となって、それぞれの日誌化した画像がこのように求めることができます。
この画像、そもそも元々の原画像が原画像中に何パーセント物体を含まれているか、なかなかわかりづらい画像になっています。
この4つを比較すると、なんとなくこの40%とした式値が一番好ましいのかなというふうに見ることがわかります。
Pタイル法においては、あらかじめ何パーセントであるということをしっかり理解し、わかっていないとうまく適用できないということになります。
そこで、事前知識を必要とせず、自動的に式値を決定するという方法が判別分析法という方法になります。
ちなみにこの判別分析法は、別の名をオーツの日誌化法とも呼ばれます。
これはオーツ先生が提案したアルゴリズムですので、オーツの日誌化法というふうに世界的に呼ばれたりして、
ぜひ皆さんもですね、新しいより良い画像処理のアルゴリズムを考えて、それを論文として発表して、皆さんの名前のついた手法を提案できるといいんじゃないかと思います。
さあ、この判別分析法はどういう方法かというわけなんですけども、このようなヒストグラムから式値によって2つのクラスに分割したわけですね。
その2つのクラス間の分離度が最大となるような画素値を式値として決定するという方法です。
では、1つ順番にその処理を説明します。
まず、ある式値tにより2つに分けます。
式値tよりも小さい画素Fijはクラス1という集合に属しますよということです。
式値tは小さいわけですが、こちらからですね、こちらをクラス1というふうに属します。
ちなみにこのクラス1というものは、集合を今ここでは表しています。
クラス1という名前の集合にFijは属しました。
一方、この式値よりも大きいところはクラス2に属しますよというわけです。
なのでこちら側はクラス2というわけです。
そしてこの2つのクラスに分割できましたので、2つの集合ができましたので、
それぞれの集合であるクラス1と2ごとにそれぞれの平均値のnと平均値のmと分散を止めます。
同じように今度はクラス2のみのこちらだけの画素を使って、その画素数と平均値と分散を計算します。
次にこの計算した各分散平均画素値、画素数を使ってクラス間分散とクラス内分散を求めます。
クラス間分散はどういうものかということですが、
ノータンヒストグラムが2つのこんなような山になっていると思います。
ここで式1tで2つに分けました。
そうするとC1とC2になりました。
その時ここの中心、C1の平均がm1、C2の平均がm2となっていました。
このクラスC1に属する画素数はn1、クラスC2に属する画素数がn2でした。
ちなみにn1たすn2は全体の画素数nになります。
さあクラス間分散を見てみましょう。
クラス間分散の分母はn1プラスn2ですから全体の画素数になるということがわかります。
なのでクラス間分散を決める重要なものはこの分子になります。
この分子のまず最初n1かけるn2、n1の値とn2をかけるわけですけども、
n1とn2の掛け算した値が大きくなるためには両方の値がなるべく同じ値になってないといけないわけですね。
足してnですから半分に分けた時にn1かけるn2は一番大きくなります。
さあ重要なのはこちらのところです。
これはm1からm2を引きます。
m1はクラス1の平均、m2はクラス2の平均ですから
この差を計算するというわけです。
しかも差を計算するとm2の方が大きい値であるとマイナス値になってしまいます。
そこでそれを自乗します。
すなわちここの距離みたいなものを表すわけですから
これ自乗したものですから分散と言います。
ちなみにこのクラス間分散はn1とn2の画素数が同じぐらいで
かつできるだけm1とm2が離れていれば離れているほど
クラス間分散は大きくなるというわけです。
続いてクラス内分散です。
例えばこのクラス内分散を見てみると
この打1項と打1項に分けられています。
分母はn1たすn2がnですからどちらも

変わりません。上がN1、N2、それぞれに分散をかけたものであることがわかります。
じゃあこのクラス内分散が、上はクラス内分散が大きくて、下はクラス内分散が小さい状態になります。
クラス内分散が小さいということはどういうことかというと、N1とN1に分散をかけます。分散はここの広がりを表していることになりますから、
このクラス1の分散が小さければ小さいほど、上は非常に分散が大きい状態ですから、
分散が小さければ小さいほどクラス1のクラス内分散、クラス2の分散も小さくなればなるほど、このクラス内分散は小さくなるということがわかります。
そしてその後に分離度を計算します。分離度はクラス内分散とクラス間分散の比です。
すなわち、クラス内分散分のクラス間分散をします。分離度が最大となるような式位置を自動的に見つけますので、分離度が大きくなるためにはどうなればいいかというと、
分母は小さくなってほしいし、分子は大きくなってほしいわけです。
SB、クラス間分散が大きいというのはどういう状態かというと、平均がなるべく離れていて、かつ分母であるクラス内分散が小さくなる状態は各クラスの分散が小さくなっている。
すなわち、それぞれのクラスが、クラス内分散が小さくなって平均が離れているような式位置を決定するといいですよというのが判別分析法です。
実際には画素の値は0から255ですから、式位置の取り得る範囲というのはここにあるように1から254になります。
なので、まず式位置を1として、クラス間分散とクラス内分散を計算して分離度を計算します。
次に式位置を2として、クラス間分散、クラス内分散、分離度を計算します。
というように、式位置を1から254まで変化させたとき、それぞれ分離度を計算し、その分離度が最大となる式位置を最終的な一番好ましい式位置として決定するというわけです。
さあ、この式はアーグマックスと呼ばれる式です。これは何を計算するかというと、s分離度を計算します。
この分離度を計算するときに式位置を1から254まで変えて、それぞれ分離度sを計算して、そのマックスを取りましょうと言っています。
このargというところがない状態だと、sの最大値をthに代入するということになりますが、このアーグマックスという方法は最大値を代入するのではなくて、
sの最大となるときのtの値をthに代入するというわけです。こうやって数式で表現することができますので覚えておいてください。
さあ、では判別分析法の処理過程を見ていきましょう。この入力画像に対してノータンヒストグラムを計算し、式位置を1から254まで変えてきます。
式位置を1としたときのクラス間分散とクラス内分散を計算し、分離度を計算します。これを式位置1から254まで変えていって、クラス間分散、クラス内分散と分離度を計算するというわけです。
さあ、クラス間分散、分離度が高くなるところはクラス間分散が大きくて、クラス内分散が小さいところです。
一番高いところはここですかね。なのでこの辺りになって、結果式位置は132となりました。その132という式位置を使って日誌化するとこのような画像になります。
どうでしょう?このグレースケールの画像が入力されたら自動的にこの132という値を計算して判別分析法によって計算し、日誌化すると綺麗な日誌化画像を得ることができるようになりました。
では、これまでのPタイル法と判別分析法の結果を比較してみましょう。判別分析法は自動的に132という値を計算してくれます。
一方、Pタイル法はパーセントを決めないといけません。細かく65.2とか決めれませんので、60とか70とか大予想の値を設定していくわけです。
そうするとそれに合わせた式位置171だとか100といった値が計算できるわけなんですけども、どうでしょう?この3つを比べたときに元の画像をより良く日誌化できているのはというふうに見ると、こちらはやや黒画素が多すぎるような気もしますし、こちらは白の画素が少なすぎる、
足りないところがちょっと削られすぎているということもわかります。判別分析法は元の画像の情報をより残したような形でうまく日誌化できていると言えるということがわかると思います。これが各手法における比較になって判別分析法はより好ましい日誌化方法であるということがわかると思います。

それでは続けて、日誌化した画像の処理について説明します。日誌化によって背景に属する画像と
前景に属する画像に分離することができました。 では、前景に分離した画像は物体に属していることになりますから
その画素の何らかの特徴を計測することで、この画像中に含まれている物体の特徴・性質を知ることができます。
その一つの調べる特徴領として、連結性というものがあります。 まず、四連結というものは、四近法に対して注目画素の連結を定義したものです。
例えば注目画素に対して、前景が上、左、右、下、2のどれかに物体が、黒画素が存在すれば、これは四連結で繋がっていますよということになります。
この緑とは上下左右では繋がっていませんので、この赤の画素は繋がっているということになります。
一方、八連結の場合は、注目画素に対して、1、2、3、4、5、6、7、8の周りの八画素を見て連結しているかということを定義しますので、
緑は八連結としては繋がっているということになるわけです。 このように連結しているこの画素の集合を連結成分と呼びますし、
対象連結成分の中にあって、背景と連結していない白画素の集合を穴というふうに呼びます。
ではまず最初に、四連結で定義した場合どうなるかというのを見てみましょう。
四連結を使ってこの物体のところ連結性を求めると、緑の連結成分と青の連結成分と2つの連結成分、
すなわち2つの物体がこの画像に含まれているということがわかります。 ここは繋がっているように見えますが、先ほど紹介したように四連結は上下左右だけなので、
この斜めとは繋がっていることになりません。 したがってこの場合は2つの連結成分があるというわけです。
四連結で連結成分を見た場合、この中にある穴は八連結として捉えます。
八連結として捉えると穴はここ繋がったことになりますので、ここは穴でないです。 一方こちらの2つは八連結でこの穴のところが繋がって外とは繋がっていないので、
八連結の穴が1個あるということになるわけです。 続いて今度は8連結で見てみましょう。
8連結で見るとここが繋がっていることになりますので、全体でこの青の1つの対象が含まれているということになります。
8連結で連結成分を見たときは4連結で穴を確認しますので、ここはここと背景とは繋がっていないということになりますから、独立していますから、これが一つの穴になります。
こことここも背景とは4連結で繋がっていないので、ちゃんと穴になります。 さらにこのオレンジとこのオレンジは4連結では繋がっていませんね。
なのでこの場合は1個、2個、3個の3つの穴があるということになります。 このような日課画像から4連結の場合はこのような性質が、8連結で定義するとこのような性質をですね
計算することができるようになるというわけです。 続いて今度は輪郭線追跡です。輪郭線追跡も日課画像処理の中でよく行う処理の一つです。
これは連結成分、一つの連結成分の境界を求めるという方法です。 じゃあどのように求めていくか順番に見ていきます。まず1番ラスタスキャンにより白画素から黒画素に変わる画素を探索します。
これラスタスキャンというのはどういうことかと言いますと、画像の左上から順番にこうやってスキャンしていくことをラスタスキャンと言います。
これが右まで来ると1個下のところをまた順番にこうやって見ていきます。 今白から黒画素に変わるところを見つけますので、上から順番に見ていくと白から黒に変わるところはありませんでした。
次の行を行って白から黒行くと、ここですね。白から黒に今変わりましたというわけです。 白から黒に変わる画素を見つけたので、そうするとここをですね
まず輪郭線のスタートとして、最初はこのような領域に対して、ここは真ん中ですね。領域に対して2番です。
進入方向、起点に番号順に右回りに黒画素を探索します。 今左から右に進入しました。左から右に進入しました。
ここのピンクが、ここに今対応しているという状態です。 なのでこの1番、ここから順番に番号順に右側に黒画素を探索しますから、
1番、2番、3番、4番という風に探索します。 ここにおいて1番はここですね。1番、2番、3番、4番と行くと黒画素が見つかりました。
なので黒画素が見つかったところに移動します。 これがこっちに移動するわけです。
隣の画素にこうやって移動した。今度は隣の画素に移動しましたので、また同じように、
今ここから見るとここが1番ですから、ここから見るとここが1番ですが、 1番、2番、3番、4番、5番と見ると5番のところに黒画素が出てきましたので、今度はここから黒画素に移動するというわけです。
なので今度は注目画素はここになって、この領域で確認をします。 この時に斜めに進行しました。
斜めに進入したので、斜めに進入した時は進入方向に対するここが1番になります。 こっちからこういうふうに進入しましたから、こういう形に進入しましたので、1番はここになりますね。
なので1番、2番、3番、4番、次5番のところで黒画素が発見できましたので、なのでこの場合は今度は黒画素がこちらに移動して、また同じように進行方向が超えてきましたから、ぐるっと探索をするというわけです。
これをどんどんどんどん繰り返していくことで、こうやって輪郭線を追跡することができるようになります。 さあこの輪郭線追跡をどのように実際に使うかということですけれども、
今日最初にレシートの数字とか文字を読んだりするという話をしました。まさに文字認識をするためにこの輪郭線追跡が使われています。 じゃあどういうふうに使えているかというと、まずこのように日化をした画像があります。日化した画像から輪郭線を取り出します。
この輪郭線から特徴を各小領域ごとに取り出します。 例えばこの小領域を注目すると、このような輪郭線とどちらに移動したかというものを方向コードで表したものがあります。
例えば横に移動した場合は0番、斜め上に移動した場合は7番ということがわかるわけです。 この小領域からこの方向コードによるヒストグラムを計算します。どういうことかというと、1番、2番、3番、4番、5番、6番、7番という方向コードがあります。
この中に各方向コードが何個あるか数えるわけです。 0番は右に移動したものですから、1個、2個、3個、4個ありました。なので0番は4つありましたよというわけです。
続いて1番下ですね。斜め下はありませんね。なので0。次2番下です。下に移動もこの中には含まれていません。次3番斜め下です。斜め下はここに1つありますね。なので3番は斜め下が1つありましたということで1個です。
次4番。4番は左に移動ですけれども、1、2、3、4、5、6個ありますね。結構たくさんありました。なのでこれは6個ありましたよということで6個ありました。続いて5番行ってみましょう。5番は斜め上ですけれども斜め上はここで1、2、3、3つありましたので5番は3つありました。
そして7番は斜め上1個、1個だけですね。なので1つ1個ありました。最後6番はありませんでしたので6番はこれなしです。
次に7番が1個ありますので7番が終わったということで、この方向各黒画素の輪郭線追跡したときの方向コードがどれぐらい何個方向コードごとにあるかというのをヒストグラムで表現した。
すなわちここの同じ領域のヒストグラムがここの領域の特徴領になるわけです。この処理をすべての小領域に対して計算したものがこのチェーンコードの方向ヒストグラムという特徴になります。
そうするとこのアーという画像だったものがどっちの方向の成分が多いかによってア、イ、ウ、エ、オで文字ごとによってこの特徴パターンが変わってくるわけです。それを具体的には主成分分析という方法を使って特徴のベクトル次元数が大きいので小さく次元圧縮して、
いらない冗長な情報も含まれているのでそういうのをなくしたような形で次元圧縮をして、そして次元圧縮した特徴ベクトルをあらかじめ登録したアの特徴ベクトルと距離計算をして距離が小さければアという文字ですよというふうに識別するというものです。
このようにして先ほどの輪郭線追跡した結果を使って文字認識が実際に使われているというわけです。では他の日課画像処理としてノイズ除去について説明します。
ノイズ除去、日課画像のノイズ除去手法として収縮と同調という処理があります。収縮という処理は背景または穴に接する対象画像の一回りを剥ぎ取る処理です。
どういうことかというと、このような日課画像に対して収縮をすると一回り分剥ぎ取っちゃいますから、痩せたような文字に変わります。小さいこういう点の値はなくなっちゃうというわけです。一方、膨張は背景または穴に接する対象画像の一回りを加えるという処理です。これは一回り太らせるような処理です。
膨張処理をこの画像に施すと一回り増えますから、太く線が変わりますし、小さな点も大きくなるというわけです。この収縮と膨張を行うわけですけども、クロージングと呼ばれる処理では、原画像に対してまず膨張を2回します。
そうすると、太ったような日課画像になって、2回膨張したので、その後2回収縮を行います。そうすると、2回膨張したときに穴が埋まってますから、この収縮したものは穴が埋まった状態になっています。
続いて、オープニングという処理です。オープニングは、まず収縮を2回行います。この画像に2回収縮をすると、この小さい細かなノイズがなくなります。だけど、穴が非常に大きくなっちゃいます。そこで2回収縮したので、次に2回膨張をします。
そうすると、穴の大きさは元と同じだけど、この小さなノイズがなくなった状態になっています。では、最初に原画像に対してクロージングをしてから、その後クロージングした画像にオープニングをすると、穴が埋まって小さなノイズがなくなるようなノイズ処理、ノイズ除去をすることができるようになります。
これが、日誌化画像におけるノイズ除去の手法であるクロージングとオープニングです。このクロージングオープニングは、それぞれ収縮と膨張という処理からなります。
続いて、ラベリングです。ラベリングは、このような日誌化画像に対して番号であるラベルを振っていきます。
同一ラベル、例えば1というラベルを持つものは1つの連結成分、2というラベルを持つものは2という物体というふうに属しているということが分離することができるようになります。
さあ、どのように行うかということなんですが、まず左上からラッサスキャンしていきます。そして、まずラベルがついていない画像を見つけます。
最初はラベルがついていない状態ですから、ラベルがついていないので、ここにまずラベルを最初につけます。
初めて見つかったので1番としました。また続けて見ていきます。そうすると、ここも最初はラベルがついていない状態ですから、新しく、さっきも1番はラベルがついていましたので、
1番というラベルが存在していますから、今度は2番というラベルをつきます。そして、またどんどんどんどん探していくと、今度また新しいのが見つかりました。
この時、上を見ます。注目画像の上の画像がラベルを持つか見ていますので、この場合まだラベルを持ちませんから、新たなラベル3をつけます。
さあ、それで次です。ここを見たときに、上の画像がラベルを持っていることがわかります。なので、その上の画像が持っているラベルをここに採用します。
そうすると1番になります。ただし、この時に注目画像の左の画像がラベルを持ち、注目画像のラベルと異なるとき、
Lookup tableにそれらのラベルが同一連携成分であるよということを記録しておきます。
すなわち、ここの注目画像は上に1番というラベルがついていて、その1番をこの注目画像に採用しました。
左を見ると異なるラベルがついています。これは本来同じラベルになるべきですので、
Lookup tableに1番と3番は同じものですよというふうに控えておく。
これを次にどんどんどんどん繰り返して新しいラベルをつけていくんですが、ここをつけたときに上に2番がついていますから、
ここは2、隣見ると4ですから、この4は2である可能性があるということですから、
Lookup tableに4と2は同一ですよというふうにラベルをどんどん付けていく。
これを左上から下まで繰り返したときに、Lookup tableを参照して、それぞれ1番と3番は1番にして、
5番と1番は1番ですよ、4番は2番ですよ、6番は2番ですよというふうにまとめてあげることによって、
1番と2番の2つのラベルのついたラベリング処理ができるということになるわけです。
このラベリング処理した画素に対して、形状特徴というパラメータを計算することができます。
例えば、重心、外設長方形、面積、周囲長、円形度、オイラー数、モーメント特徴というのがあります。
重心は図形全体の重さの中心です。
外設長方形はこの囲む外設長方形を表します。
面積は全体の画素数になります。
周囲長は輪郭線追跡したときのその長さになります。
面白いのは円形度ですね。円形度はこのような式で計算できます。
この物体が円にどれくらい近いかを表す尺度になっています。
他にはオイラー数。オイラー数は先ほど求めた連結数、連結成分の数から穴の数を引いたものです。
もう一つはモーメント特徴です。
モーメント特徴はこの式で計算できます。
pとqに0と1の値を代入したときにこの式を計算します。
pはiというxの座標にかかりますし、qはjというyの座標にかかります。
もし0と0という風に代入すると、x座標の値を0乗、yの座標を9乗も0乗しますから、それぞれ1になりますから、
Fijは2値化した画像ですから、0もしくは1になっています。
Fij自体は0か1の値を持っている状態ですから、この場合は1の値が相和されます。
すなわち図形の面積を求めることができます。
ちなみにpを1、qを0とすると、x軸の一時モーメントを求めることができ、
m10からm00、すなわちx軸の一時モーメントから図形の面積を割ってあげると、
重心のx座標を出すことができます。
このように0と1の組み合わせによっていろんな数値を計算することができるのを、
モーメント特徴という風に呼びます。
今日は2値化した手法と、その2値化した画像のいろんな処理について説明をしました。
次回はテンプレートマッチングについて紹介したいと思います。
今日はこれで終わります。それではまた。

みなさんこんにちは。今日はテンプレートマッチングです。
ここまでこの講義では画像をどのように標本化、量子化して濃淡を変化させるか、
そして平滑化、エッジチュースのような空間フィルタリング、
さらには画像を変形する気化変換、直線の検出をする波幅変換、
テクスチャーを解析するフリエ変換、
そして日課処理と日課画像処理について紹介しました。
今日はこの処理から次は認識に近いところでテンプレートマッチングという手法について説明をします。
テンプレートマッチングはどういったところで使うかというと、
このような入力画像の中からこのテンプレートと同じものがどこにあるかといったことを探す技術になります。
こちらの画像をテンプレートと言います。
具体的には外観検査、例えば工場の中で商品、いろんな部品がこう、
例えばロボットからすると部品を取り出したい時にピックアップ、グラスピング、
掴む部品をどこにあるか、画像中の中のどこにあるかということを検出する際に
このようなテンプレートマッチングを使ったりします。
このテンプレートマッチングはどのようにできるかというと、
画素同士の比較によって類似度が最大となる位置を見つけるというわけです。
そのためにはテンプレート画像とテンプレート画像と同じ大きさの入力画像の領域との類似度計算を行います。
今度はこの探索するウィンドウと言いますけど、
これを横にちょっとずらして、また類似度計算をします。
隣にずらして、ここと類似度計算をするというのをずっとラスタスキャンしていって、
それぞれの領域に、それぞれの位置において類似度を計算します。
この右の図は各点をスキャンした時の縦軸が類似度で表したものです。
類似度が高いければ高いほど似ているということを表現します。
この中で一番類似度が高いところは色々ピークが超えてあるわけですけども、
一番高いところはここのわけです。
なのでこの座標のところにテンプレートが存在しますよということを検出することができるようになります。
すなわちこの類似度が最大となる座標を結果として出力するということになります。
このテンプレートマッチングではどのように類似度を計算するかということが重要になります。
この類似度の尺度にはまず SSD という方法と SAD という方法があります。
SSD は sum of squared difference リファレンスは差、スクエアードは事情ですから、差の事情の相和を計算したものが SSD ということになります。
一方 SAD は sum of absolute difference になっています。
なので差の絶対値の相和が SAD と言われるものです。
この両者ともにこの尺度は似ているほど小さな値になるので相異度とも呼ばれます。
i が画像の座標、Tij がテンプレートの座標です。
なので IJ で指定した座標同士の画素値の差を計算して事情したものが SSD、差の絶対値をとったものが SAD です。
これがテンプレート画像だとすると N × M のサイズになっていますので、
画像と同じサイズのところの画素の各画素の差の事情をとってその相和をします。
このように計算するのが SSD と SAD になります。
もう一つよく使われる方法に正規化相互相関という方法があります。
これは Normalized Cross Correlation というふうに呼ばれます。
計算式はこのような式になります。
ちょっと複雑な式になっていますが、これは何を表しているかと言いますと、
2つのベクトル、i という画像と T という画像がありますけれども、
その i,T,i と T をそれぞれベクトルで表現すると、そのベクトル間のナス角を実は計算していることになります。
例えばこのテンプレート画像実際は 10 × 10 の画像とします。
そうするとこの場合は100次元のデータということになります。
入力画像の方も同じように10 × 10 とするとこちらも100次元のデータとなります。
今ここの原点は本当はですね、画像絵で描く場合は2次元、3次元しか描けても3次元しか描けませんが、
ここが100次元空間を表していると思ってください。
その100次元空間の中にこの1個1個の値がありますから、1つ T というベクトルが決定されるわけです。
ここちょっとわかりづらいのでもう1回説明します。
画像が10 × 10 であれば全てで100個の値を持ちます。これが100次元ということになるわけです。
100次元の空間を軸で表現することはちょっと可視化ができませんが、ここに100次元の空間を表現していると。
その1個目の値、2個目の値、3個目の値を使うと1つのベクトルが決定できるわけです。
これがこの T を表していることになります。100次元空間におけるベクトルです。
同じように I も10 × 10 の100次元ですから、同じ100次元の空間において I はこのようなベクトルとして表現できますよというわけで、
この正規化相互相関は何を計算するかというと、ベクトル T とベクトル I のナス、このシーター、
すなわち正確にはコサインシーターを求めることがこの式によってできるわけです。
ちなみに SSD は、この SSD した後をルートすれば平方根の定理と同じでここの距離になるわけです。
すなわち SSD はベクトル間のここの距離を計算していることになりますし、
SAD はこことここを足した距離が SAD となっています。このように SAD のことを市街地距離、
SSD のことをユークリット距離の事情、そして NCC はベクトルのナス角のコサインということになります。
このように類似度を計算するためにも、1種類、2種類、3種類とあり、それぞれ意味が異なるというわけです。
さらにこの距離尺度、類似度を計算する方法のアプローチとして、相互相関係数というのがあります。
英語で言うと Zero Means Normalized Cross Correlation です。先ほどの NCC と正規化相互相関と何が違うかというと、ここがありませんね。
ゼロ、すなわち新しくこの Zero Means というところが入っています。これは何かというと、先ほどの式と違うところだけを書きますと、
ここ、ここ、ここ、ここが違います。これ何をやっているかというと、i のバーと t のバーで、それぞれの i からは i のバーを、t からは t のバーを引いているということがわかります。
じゃあこの i のバーと t のバーは何かということなんですが、i のバーは、これは入力画像の領域の平均ですね。
なのでこちらは平均入力画像となっているわけです。入力画像の平均値を計算しています。
一方こちらはテンプレート画像の平均値を計算しています。なのでそれぞれ平均値を引いているということになります。
そうすると、先ほどはこのベクトルのナース角を、こことこのベクトルのナース角で見たわけです。
なので、例えば i と t がそれぞれこのようなベクトルだとすると、この間のナース角、コサインシーターを求めていたわけなんですけども、この時にこの角度は、この t のベクトルが長くても短くても基本的には影響を受けないんですが、
できるだけこの長さをちゃんと揃えて相関を求めましょうという。なのでそれぞれ i に対して、i の平均値で引き算をして、t に対しても t の平均値で引き算することによって、
この正規化相互相関のゼロミーン、すなわち平均で引くといったことを実現している。平均値を引くので、2つの画像領域の平均値が異なっていても、ちゃんと類似度が1となる、いわゆる平均的な明るさの変動を吸収することができるわけです。
どういうことかもう一度確認をすると、この図が平均で正規化した i と正規化した t において、いわゆるコサインシーカー、類似度、相互相関係数を計算することができます。
では実際に具体的に、これらの距離尺度を使って、類似度尺度を使って、どのような結果になるか見てみます。ここではこのような ic チップをテンプレート画像として、このテンプレート画像がこの入力画像の中のどこにあるかということを探しています。
皆さんどこにあるかすぐわかりますか?ここですね。これはこのテンプレート画像と、この入力画像と似たような環境下、いわゆる照明環境下で撮影されているので、すぐパッとわかると思います。
これはテンプレートが撮影した時と同じような環境で入力画像を撮影していれば、比較的簡単に見つけることができます。
しかし、例えば入力画像を撮影するときに、テンプレートを撮影したときと異なる照明環境で撮影したりすると、例えば窓辺で撮影したりすると、太陽の光が外から入ってきますので、それによってこのようにちょっとハレーションが起きたような画像になってしまうことがあります。
そうすると、これと同じものがどこにあるかというのを探すのが少し難しくなるというのがわかると思います。では、今から実際にそれぞれの画像でどのような結果になるのか見ていきましょう。
まず、通常のこちらの通常の画像に対するテンプレートマッチングの結果です。これがSAD、縦軸が類似度を表します。
SAD、SSD、NCC、ZNCCです。このSADとSSDは距離になりますから、距離が小さいほどテンプレートと同じものがあるというわけです。
従って値が小さいところを見てみると、一番小さいところはこの例ですとここですね。ここもちょっとわかりづらいかもしれませんが、ここになっています。すなわちここの座標、これですね。ここが一番近いですよというふうに出てきたところです。ちゃんと同じところが検出できているのがわかると思います。
一方、今度はNCCとZNCCを見てみましょう。NCCとZNCCはコサインシーターを計算しますので、値が1になればなるほど近いことになります。ちゃんと書いておきましょう。こちらは距離を計算して、こちらは類似度になっていますので、距離は小さければ小さいほど似たものがありますよということになります。
類似度は似ているものだと最大が1になりますけど、1に近ければ近いほど似たものがありますよという。さあNCC見てみましょう。これよく見ないとなかなかわかりづらいんですけど、ここにちょっとピークが出ているのがわかります。なのでここの位置座標のところにちゃんとテンプレートと同じ画像が存在していることがわかります。
ZNCCはもう見た感じすぐわかりますね。ここがピークになってますから、ここの位置のところに対象のテンプレートと同じものが映っているということがこれでわかるわけです。続いて今度は照明変動が起こった画像です。こちらの画像ですね。こちらの画像においてどう変わるかというのを見てみましょう。
同じようにSSADとSSDは距離を計算します。縦軸は距離です。なので似ているほど距離は小さくなる。一方下のNCCとZNCCは類似度ですけど高くなるほど似てますよということを表します。
さあこれをラストスキャンしてみると一番距離が小さかったというのはSSADはここになっています。SSDも間違ってしまってこんなところになってしまいます。それはなぜかというと軌道値の変動が起こってますよね。テンプレート画像と入力画像で軌道値が変わってしまいますから、例えば同じこちらがテンプレート画像としましょう。
入力画像も同じものであれば同じような方向を向きます。しかし暗くなってしまうと入力画像が本来であれば入力画像も同じようなこういうベクトルになるので距離を計算すると小さくなるわけです。
しかし今これは照明変動で全体的に、この場合は明るくなっちゃってますね。明るくなったってのはどういうことかというと、このiが明るくなったので持った値がすべての画像において大きくなっちゃったわけです。このとき例えばSSDは何だったかというと、このベクトルのここの距離を計算しますよね。これがSSDでした。
なので照明変動が起こったことによってこの距離は大きくなるというのがこのSSD、SSADにおけるこの例におけるうまくいかない例です。
さあじゃあ一方類似度を見てみましょう。類似度はこれよく見ると一応正解のところに出てます。
わかりづらいですけどちょっとピークが出ています。一方ZNCCはもうはっきりとピークが出ています。じゃあなぜ今度NCCとZNCCではうまくちゃんと正解の位置が見つけられるかということなんですが、先ほども話をしたように
テンプレートのベクトルTとIが照明変動があったとしてもそのナス角シーターは変わらないのでちゃんと求めることができますよってことです。
だけどより正確に求めるためには一旦平均で引いて計算すると周りと大きく差が離れるような形でテンプレートマッチングをすることができるようになるというわけです。
このように手法によってどういう入力画像に対してうまく適応するかできるかできないかということがあるわけです。
ここでは照明変動が起こるとこのナス角を計算するNCC、ZNCCは問題ありません。うまくいきますよ。
一方、ベクトル間の距離をそのまま計算するようなSADとかSSDは照明変動が起こるとうまくいかないという。
例えば入力画像が必ず限定された環境下で撮影される場合であれば照明変動は必ず起こらないということであればNCCやZNCCを使う必要なく
SADとかSSDを使えばいいわけです。実際こちらの方がこちらの方法よりも計算量が少ないので早く計算できるというメリットがあるわけです。
まず距離計算方法ルーチとの尺度によって得意不得意があるといったことを理解しておいてください。

続いてこのテンプレートマッチングを 少しでも早く計算しようという方法が
粗密探索と言われる方法です どういうことかと言いますと
この一番下の画像が 入力画像の原画像だと思ってください
ここを左上からこうやってラスタスキャンをしていくと
解像度が非常に高い画像だと テンプレートマッチングする回数が
かなり増えますので 計算量がどんどん時間がかかってしまうわけです
そこでなるべくこのテンプレートマッチングを 高速に行う手法として
粗密探索という方法があります これは英語ではCourse-to-Findと呼ばれたりします
さあどうするかというと まずこの入力画像をダウンサンプリングして
低解像度の画像を段階的に作っておき まず最初はこの低解像度の画像に対して
テンプレートマッチングをします この場合は4×4の領域に対して
テンプレートマッチングをして 16回するわけですけども
その中でここの位置が一番距離が 小さかったですよとなります
そうするとこの以外のところにはテンプレートは おそらくもう存在しないであろうというわけですから
この辺を他の周りのこの辺りですね こういった周りのところを
もう探索しなくていいよというわけです その代わり実際にここは今低解像度の
4×4のところは低解像度の状態ですから 今度はここの辺にあるってことがわかったので
1段階上の解像度の画像の同じ対象とする領域内で またテンプレートマッチングを順番にします
その中で一番似ている距離が 類似度が一番高いところを探して
その高いところがここというふうに分かりますので
今度はここに対応した領域の原画像の解像度で またテンプレートマッチングをして
最終的にここですよということを見つけるというわけです このように低解像度で全探索して
その見つけたところの領域に対応する 一つ上の高解像度のところでまたテンプレートマッチングをして
さらにその見つけたところに対応する さらに高解像度のところをまたテンプレートマッチングをする
そうすると全体を細かくテンプレートマッチングするより 計算回数を大幅に減少させることができるというわけです
では続いてアクティブ探索という テンプレートマッチング法について説明します
このアクティブ探索法は 相当り計算を間引いて効率的に探索します
例えばこのようなテンプレートが この入力画像のどこにあるかということを探します
その時に通常のテンプレートマッチングであれば 全てのところをガーッとラスタスキャンしていったわけなんですが
このアクティブ探索法を用いると テンプレートと似ていないところは荒く探索します
一方テンプレートと似ているところは細かく探索します
この黒い一個一個の点はそこの座標において 実際に類似度計算をしたというところです
本来であれば細かく全てのところを 類似度計算するわけなんですけども
この点の数を見てわかるように かなり計算回数を減らすことができるようになっているわけです
じゃあなぜこれができるかということなんですが
そこには1つこのテンプレート画像から カラーヒストグラムというものを求めます
このカラーヒストグラム ヒストグラム化した性質を使って
カラーヒストグラムの類似度が低ければ 周辺の重なりを持つ領域の類似度も低いと考えて
周辺の類似度計算を大幅に省略するというアプローチです
ではもう少し詳しく説明しましょう
まずAという領域とBという領域があったとします
Aという領域からカラーヒストグラムを計算しました
入力画像がカラーであればカラーヒストグラムになりますし
入力画像がグレースケールの画像であれば
これはこれまでやってきたノータンヒストグラムと一緒です
横軸は0から255という値を持つヒストグラムになります
Bの方に対しても このBという領域のカラーヒストグラム
ノータンヒストグラムを計算します
この時Aという領域とBという領域には
ここの重なり領域を持ちます
このAキャップBの領域です
ここはどういうことかというと
ここの共通領域のヒストグラムは
この緑のヒストグラムになる
この緑は上のヒストグラムにも下のヒストグラムにも
当然ながら含まれているというわけです
この含まれている共通領域とAの領によって
Bを計算した後にAを計算しなくても
どれぐらい類似度の上限値がいくつになるか
ということを知ることができます
ではもう少し詳しく説明します
まずテンプレート画像Mがあります
入力画像のあるBという領域と
距離計算をした時の類似度があります
これが既に計算した類似度がSBMです
これは既に計算しているわけなんですけども
次このテンプレートをAという領域と
類似度計算をするときに
本当にAという領域とM
テンプレートMと類似度を計算するべきかどうか
それをこのBとの類似度と
重なり度合いによって
Aとの類似度の上限値を
あらかじめ計算しましょうというわけです
その上限値によっては
Aはもしかしたらテンプレートの可能性があれば
類似度を計算しますし
上限値がそもそも低ければ
そこにはテンプレートは存在しないわけですから
その場合は類似の計算をやめればいいですよというわけです
どのように上限値を計算するかというと
これがその式です
類似度Bとの類似度SBMに
Bの面積をかけたものと
AとPの共通領域のミニマムをとって
そこにA-Bをしたものを計算します
これが設定してある上限値と比べて低ければ
そもそもこのAという領域に似ているものはないので
SAMの計算は計算を省略しますよということができるわけです
もう少しわかりやすく説明します
今事前に計算できているものは
この類似度のSBMです
この類似度のSBMが低い場合
もう一回書いておきましょう
こちらがA、こちらがBと別にMというテンプレートがあるわけですね
今計算できているのは
AとBの距離、MとBの距離、SBMになります
類似度が結論に計算できて
そのここの類似度が0.2だった場合ですね
ちなみにここがA-Bの重なり率になります
なのでまず類似度を計算して
この類似度が0.2と小さい時は
そもそもBとの共通領域の重なり率が
大きいとここですね
上限値は0.2が最大になるというわけです
ということは0.2ということはほとんど
もうMとAは似てないよということになります
したがって類似度が低くて
重なり率、ここのA-Bが重なり率が高い時は
上限値が小さくなるので
MとAは類似していません
すなわちMとAの類似度を計算する必要はありません
では逆にMとAの類似度を計算しないといけない時は
どういう時かというと
まず重なり率が低い時は
これ見ていただくと分かるように1ばっかりになっています
ということはテンプレートMとAの類似度の上限値は1なので
テンプレートのある可能性がありますよ
それはなぜかというと
BとAの重なりが低いので
あまり重なってないから
Bにある可能性はあるかもしれません
ということを言っているわけです
その時はちゃんとMとBの距離
SAMを計算しましょうということになります
もう一つ類似度が高い時も横に見て分かるように
当然SBMの類似度が高ければ
このBの周辺にMがある可能性があるわけですけど
高くなりますよというわけですね
この方法のいいところは
この辺が省略できるわけです
この辺の類似度計算の省略ができることによって
テンプレートマッチングの高速化
探索の効率化ができるわけです
くどいですがこの方法は画像と画像ではなくて
画像それぞれを一旦ヒストグラムで表現します
ヒストグラムで表現するから
重なり領域というものが共通情報を持つから
このような方法で上限値を求めて
その上限値がそもそも低ければ
実際の類似度計算をしなくて
次に飛ばしていくことができるようになるという
アクティブ探索の流れをもう一度説明します
アクティブ探索は類似度の上限値を用いた
アクティブ探索による探索の枝刈りというものを行っています
まずステップ1です
目標類似度の初期値を設定します
最初初期値は入力画像全体と
テンプレートの類似度を初期値として入れておきましょう
そしてテンプレートと探索領域
左上の局所領域との類似度をまず計算します
この上でステップ2で求めた類似度と
周辺領域の上限値を計算して
上限値が目標類似度よりも小さい場合は
類似度の計算が省略続いて
上限値が目標類似度より大きい場合は
これは似たものが周辺にあるだろうということで
類似度を計算します
この時、類似度が
そもそもの目標類似度よりも大きい場合は
目標類似度を更新します
そしてこの処理をぐるぐる回していくと
最終的に類似度が最大値となる位置を
テンプレートの位置として出力することができます
この方法のいいところは
テンプレートに類似した金棒では細かく探索して
テンプレートに類似していない領域では
粗く探索するという手法になっています
実際にアクティブ探索による
処理時間のグラフを作ってみました
こちらは一般的な総当たり法全探索です
一枚の画像に見たところ
テンプレートを探し出すのに
約4.5秒かかっているというわけで
それに対して粗密探索を導入したようなものですと
1.6秒になります
一方アクティブ探索法は0.21秒ですから
かなりこの上限値を用いた枝刈りによって
高速化できているということがわかると思います
今日はテンプレートマッチングとして
テンプレートマッチングにおける
距離計算方法について紹介しました
そして粗密探索
そしてアクティブ探索の
アルゴリズムについて紹介しました
次回は特徴点検出Q2の手法である
SHIFTというアルゴリズムの説明をします
今日はこれで終わります
それではまた

みなさんこんにちは 今日は特徴点検出 記述ということで
SHIFTというアルゴリズムについて説明をします
まずSHIFTというアルゴリズムを
まず僕自身がいつ使ったかということを
そのきっかけを紹介したいと思います
よくですね こういったいろんなアルゴリズムというものは
まず最初に本で知るのではなくて
われわれ研究者は論文で知ります
論文でこういう方法があるんだということを知るわけです
一般的にです 一般的に
これ良さそうな論文だと思って読んで
それを実際にプログラムを作って
自分のデータで試してみるわけです
最近はジットハブというサイトがあって
いろんな方がですね
論文で提案した手法のプログラムを公開していますので
そういうのをダウンロードして使ったりするわけです
そういった時にですね
よくあることはどういうことかというと
論文に書いてあるよりも
思ったより性能が出ないということが
よくあったりしたわけです
だったんですが このSHIFTに関しては
僕個人的に意見としては
論文を読んで実際に試してみると
思ってた以上にできたというわけです
その最初にですね
衝撃を受けた画像がまさにこの画像だった
これは皆さん何かわからないと思いますけども
これは僕が以前働いていた
カーネギーメロン大学のキャンパスの画像です
これは2回目カーネギーメロン大学にですね
行ってた時に撮影した画像なんですけども
カーネギーメロン大学の
ロボット工学研究所というところは
白紙工記課程から
いろんな世界中から入ってくるわけです
すごい倍率が高いところです
そこで研究をやってたんですけども
その時に向こうの僕の先生
ボスの先生から
ドクターの学生を1人面倒を見てくれ
というふうに言われたわけです
それで20倍ぐらいの倍率を勝ち抜いてきた学生と
僕は仕事ができるわけで楽しみにしてました
その時に彼の研究としては
2枚の画像からシーンの構造を認識するということで
まず人間の目と同じように
2枚の画像を撮影しようといって
その彼が撮影してきた画像なんです
本来はステレオ視、ステレオビジョン
これまた次回のところでも紹介しますけども
ステレオビジョンをするためには
右のカメラと左のカメラがどこに置いてあって
どこを向いているのかということを
ちゃんと知らないとどこにあるかということを
いわゆるステレオ視
三角測量で求めることができません
この学生が撮ってきた画像は
そういったキャリブレーションと言いますけども
キャリブレーションするためのデータが一切なく
撮ってきたのは本当にこの2枚だけだった
そこでじゃあそういうカメラの情報を使わずに
どうやってこの中の画像の理解をするか
シーンの理解をするかということで
その時にそういえばシフトという論文があったので
それを試してみようかと言って試してみたのです
そしたらですね
これも何の工夫もせずに実際に試した結果です
何を表しているかというと
この線の始点と終点が同じ位置
同じところですよ
これをですね
対応点というふうに呼びます
対応点を事前知識なしに
これぐらい複数のですね
対応点を求めることができました
よく見てみるとこちらの画像とこちらの画像と
色合いが違いますよね
しかも実際にカメラ感がですね
結構離れています
分かりづらいかもしれませんが
ここの同じところを見ると
かなり離れているということが分かると思います
この辺ですここですね
こことここですから
x 座標の差を見るとこれぐらい幅ありますよね
一方ではこちらは
ごめんなさいこれぐらいですね
なので差がこれぐらいずれているというわけです
この辺ですよね
なので対応点見つけるの結構大変なんですけども
Shiftを使うと何の事前知識もなくここまでできた
それで僕はですねすごく感動しました
こんなによくできるなんてすごい
それまでは結構実際に論文を読んで
実装してみて動かしてみると
もしくはソフトコードを持ってきて動かしてみると
思ったよりできなかったことは多かったんですけども
逆だったんですね
この論文は思った
僕は期待していた以上に良い結果だったということで
それでじゃあこのShiftというアルゴリズムが
どうやってできているんだろうということで
詳しく勉強して
そういったShiftを使った研究を
いろいろやるようになったというわけです
これが2004年ぐらいですね
その後日本の学会とかでも
このShiftに関するチュートリアルを
僕がやったりしたという流れです
さあではですねShiftの内容に入っていきましょう
まずShiftは
スケールインバリアントフィーチャートランスフォームと言います
スケールというのは
これ拡大縮小という意味ですよね
それに対してインバリアントという言葉が入っています
インバリアントというのは何かというと
普遍という意味です
なのでスケール変化に不変な特徴変換
特徴抽出ということになります
さあこのアルゴリズムは
ユニバーシティオブブリティッシュコロンビアの
デイビッド・ロー先生
この方が提案した方法です
実は元はもう1999年ですから
かなり古いものになるわけなんですが
今現在もなおですね
この方法は良い方法として使われています
画像処理のライブラリに
OpenCVというライブラリがあります
その中にもこのシフトを使用することが
できるように今はもうなっています
さあこのシフトのアイデアなんですけども
2つのプロセスからなります
まず1番特徴点
シフトのアルゴリズムの中では
特徴点のことをキーポイントと呼びます
この特徴点であるキーポイントの検出と
特徴量給出するということが1つ
もう1つは回転スケールに不変で
照明変化に頑堅な特徴量を算出してくれる
この2つの大きな特徴があります
では実際にこのシフトのアルゴリズムを
1つずつ見ていきましょう
まずシフトのアルゴリズムは
1、キーポイント検出と
2、特徴記述の2つに分かれます
キーポイント検出においては
スケールとキーポイント検出
これをReference of Gaussian
略してDOGという処理で行います
続けてキーポイントのローカライズとして
より良いキーポイント候補の中から
好ましいキーポイントを絞り込んで
選択をしてそしてより精度良くするために
サブピクセル位置推定を行います
これがキーポイント検出の流れです
続いて特徴記述です
特徴記述はまず最初に
オリエンテーションを算出します
そして特徴量を算出したオリエンテーションで
正規化した後、特徴量を記述します
こうすることによって
勾配方法ヒストグラムを作って
これを特徴量として記述していくわけです
さあでは順番にですね
このあるシフトの処理のアルゴリズムを
説明していきます
まずシフトのアルゴリズムに入る前に
ラプラシアン・オブ・ガオシアン
ログオペレーターについて見ていきます
ラプラシアン・オブ・ガオシアン
これはどういったアプローチだったかというと
ガオシアンのラプラシアン
2次微分を取るようなオペレーターです
このような式で表現されるものが
ログオペレーターなんですけども
このログオペレーターはガオシアンを含んでいますので
シグマというパラメータが存在します
このシグマというパラメータは
何を決めるものかというと
スケールです
スケールというのは
このログオペレーターのこの形状を
シグマによって変えることができます
例えばこのシグマの値を大きくしていくと
こんな感じに広くなっていくというもので
ここである画像に
このシグマを変化させた
このようなログオペレーターを
畳み込んでいくのです
積分計算していくのです
そうするとそれぞれのシグマに対応した
何らかの値がこうやって出てくるわけです
この時この値の変化を見てみると
極大値となる点が出てくるわけです
この極大値となるシグマの値が
ここを中心に畳み込んだ画像の周辺領域の
より特徴量を多く含むであろう
スケールということになるのです
例えばシグマ9の時に一番大きくなったとします
そうするとこの範囲に
ここからこの範囲に
より多くの情報が含まれているということが
言えるというわけです
このようにログオペレーターを使うと
そのある点に対して
どれぐらいの範囲により特徴量が
情報量が多く含まれているかということを
知ることができるというわけです
これをシフトのアルゴリズムでは
ログオペレーターの処理ではなく
DOGという処理によって実現しています
ログオペレーターは各点に対して
そのスケールを探すことができます
ただしその点が特徴的なキーポイントであるかどうか
といったことは残念ながら見つけてくれません
そこでどこに特徴点があって
かつどれぐらいのスケールで見ると
一番よりその点に対して情報量が多いかということを
同時に調べるアルゴリズムとして
処理としてこのDOGという方法を提案しています
ではDOGがどうやってできていくのか
どうやって計算するのか説明をしていきます
まずDOGはどういうことかというと
DOGはDifference of Gaussianですから
ガウシアンの差分、差をとるというわけです
ガウシアン、いわゆる平滑化画像は
ガウシアンフィルター、このGはXとYとシグマ
これガウス分布ですよね
このガウス分布を画像にかけたものが
平滑化画像になりますね
この平滑化画像の度合いを決めるのがシグマでした
例えばシグマ0だとこれぐらいの平滑化度合いの画像
そしてシグマを軽倍大きくして平滑化をすると
このような平滑化画像を計算することができます
そしてシグマの大きい、軽シグマで平滑化した
こちらの画像からこっちのシグマ0で
ここは今こっちはシグマ0ですね
シグマ0で平滑化した画像の差分を計算する
これがDOGとなります
実際にこの画像からこれを引き算するわけですから
結果このようなDOG画像を計算することになります
実際にLogオペレーター、ログフィルターをかけると
この画像にログオペレーターをかけると
このような結果になります
非常にこれらが同じような結果になっている
すなわちログと同じような効果を
DOGで得ることができるということがわかると思います
このDOGという処理を1回するだけではなく
何度も処理を行います
どのように行うかというと
平滑化するためのパラメーター、シグマか初期値をシグマ0とし
これに少し軽倍した大きなスケールで平滑化した画像があります
さらに軽を軽倍した軽次乗シグマというもので平滑化した画像
さらに軽の三乗シグマで平滑化した画像
なのでこの縦軸方向にはスケールの値をどんどん大きくしていって
それぞれの平滑化画像をこのようにまず作ります
そして DOGは平滑化した画像間の大きい方から小さい方の
大きい方から平滑化した画像間の大きい方から小さい方の差分をとりますので
この画像とこの画像の差分をとる
次はこの画像とこの画像の差分をとり
続いてこの画像とこの画像の差分をとります
連続して変化するスケール感で
DOG画像をこのように作成します
このようなガオシアンフィルターの平滑化画像の計算をするときに
気をつけなくちゃいけないことは
より大きなシグマ
こういうシグマから
だんだんこういうシグマになっていくわけですけども
大きなシグマのときは
そもそもフィルターが大きくなるので
画像全体に平滑化処理をするとかなり時間を要します
さらに画像の周辺には
このフィルターが大きくなると
処理できないという領域も出てきたりするわけです
そこでこのDOGの処理では
シグマの連続性を保持した平滑化処理を
効率よく計算するために
ダウンサンプリングと平滑化処理を
うまく使って実現します
まずどうするかというと
入力画像と同じ解像度において
まずシグマゼロで平滑化します
そして続いて
軽倍したスケールのシグマで平滑化をします
続いてもっと実はここもっと細かくやるわけですけども
2シグマまでなんとか計算できるということで
2シグマまでシグマゼロから2倍したところまで
平滑化した画像を作ります
実際はここにも間がまだ
画像がいっぱいあると思ってください
ここは元の画像と同じ解像度で作っていますから
何ら問題ありません
問題はこの2シグマより
より大きなガウシャンフィルターで
平滑化をしようとすると
計算時間と問題が出てくるというわけです
そこでどうするかというと
シグマゼロから2シグマゼロまで
平滑化画像を作成したら
入力画像を半分にダウンサンプリングするわけです
このダウンサンプリングした画像に
シグマゼロで平滑化を行います
ここと同じパラメータですね
このダウンサンプリングした画像に
シグマゼロで平滑化するということは
平滑化度合いの効果としては
どういうことになるかというと
入力画像の同じ解像度において
2シグマで平滑化したものと
ダウンサンプリングして
シグマゼロで平滑化したものは
もちろん画像の大きさは違いますけども
平滑化の度合いとしては同じになります
フィルターを大きくした場合
2倍に大きくした場合と
フィルターをそのままで
画像を半分にしたわけですから
平滑化の効果は同じですよ
ということになるわけです
なのでこのダウンサンプリングした
画像に対して
シグマゼロから軽シグマゼロで
いろいろ計算して
2シグマゼロまでここで計算をするわけです
これを2オクターブ目という風に呼びます
続いてもっとさらに
さらに大きなシグマでの平滑化を
計算したいので
今度は入力画像を
4分の1に以降ダウンサンプリングして
そしてまた同じように
シグマゼロで平滑化してあげます
そうするとこれは
3オクターブ目で
シグマゼロで平滑化したものは
2オクターブ目では
2シグマで平滑化したものと同等で
かつそれは元の画像では
4シグマで平滑化したものと
同じ効果ですよということになります
この3オクターブ目も
同じようにシグマゼロから
2シグマゼロまで平滑化をすることで
結果シグマゼロから2シグマ
ここ同じ効果ですから
2シグマから4シグマ
そして同じ効果ですから
4シグマから8シグマということで
非常に大きな平滑化度合いを得られるような
シグマのパラメータ
4シグマのパラメータによる
より平滑度合いの高い画像までを
このように効率的に計算することができるというわけです
実際にその平滑化した画像を見てみましょう
元の画像に対して
2シグマで平滑化しました
そしてダウンサンプリングしたものを
また2シグマで平滑化すると
これは原画像の解像度において
4シグマで平滑化したことと
同じ効果があるわけです
さらにダウンサンプリングして計算すると
8シグマで平滑化すると
かなりボケているのがわかると思います
このようなボケを入力画像と
同じ解像度で計算しようとすると
どうしても時間がかかってしまう
このようにシフトにおける
DOG処理においては
ダウンサンプリングを併用して
シグマ0から8シグマ0という
非常に大きなスケールまでの
平滑化画像を作成しています
では続いて特徴点である
キーポイントを検出する
処理について説明します
今何ができたかというと
シグマ0から2シグマ
元の解像度における
シグマ0から2シグマ0までの
効果画像と
同じ値での2オクターブ目における
2シグマから4シグマまでの効果画像と
さらにまた4シグマから8シグマというのがあります
ちょっとスペースの都合上
そこは省略しています
ということで平滑化画像が
シグマ0から8シグマ0まで
たくさんの平滑化画像が
用意されたというわけです
これをスケールスペースという風に呼びます
スケールですね
それぞれの連続する平滑化画像間で
DOG画像を
それぞれこのように計算をします
そしてこのDOG画像から
ここの画像がこの画像を表します
このDOG画像のスケールスペースを使って
その画素が特徴点
いわゆるキーポイントであるかないか
ということを確認します
前ログフィルターにおいて
シグマを変化すると変わってたところの
この極大値をスケールとして取り出しますよ
ということをしたと思います
これと同じことを
画像空間とスケール方向にします
こちらはスケール方向がこっちでしたね
なのでそれがDOGの板
こちらの軸になります
スケール方向だけではなくて
空間的にそこの点が
他よりも大きいか小さいかということで
特徴的なキーポイントであるかどうか
ということを判定します
どうするかというと
この青色を注目画素とします
この青色の注目画素の
DOGの値と周辺の赤色の画素と
1つ上の9つの画素
さらに1つスケールの下の9つの画像と
比較をします
青のDOGの値と周辺の値との
比較をするわけで
極大値となるという
このようなスケールとなる点は
一番大きな点になっていますので
青の点が赤の金棒
これはスケール方向にも合わせた
26金棒となるわけですけども
26金棒と比較して
そこが極大値であるか
もしくは極小値であるかといったこと
極値であるというところを検出します
この操作を左上から順番に
ラスタスキャンしていきます
順番にやっていくということをします
このときもし
ここの画素に対しては
ここのスケールで
極大値となった場合は
同じ画素のスケール方向には
もう探索はしません
ここで一旦極値となれば
さらにその上のところは
計算はしません
何が言いたいかというと
下の方がスケールが小さいわけですから
ここではより点に対して
より小さなスケールで
より情報量の多いところを見つけたら
もし画像に対して
より良いスケールを考えると
画像全体を含むと
一番良くなってしまうわけですよね
なので小さいスケールから探していって
一旦極大値が見つかったら
そこでストップします
それ以上はここが極大値となった場合は
上には探さないという
このような操作をすることで
DOG画像から極値
まずどこにその極値があって
そのときのスケールも覚えておくわけです

このDOGの処理を2次元に考えると分かりづらいので、少し1次元で考えてみたいと思います。
今、縦軸が画素値、明るさです。横があるXという座標です。
今画像が白くなって、黒くなって、白くなって、また黒くなってというようなプロファイルを持つ画像が得られたとします。
その時に、まず初期値であるSigmaで平滑化を行います。
ここの点を中心にスケールを探索しますので、ここを中心となるこのような平滑化、ガオシャンフィルターを畳み込むわけですが、
これどういった処理をするかというと、結果、ここの点から見た時、
この画素関数の間に入るところの、この元々の持つ画素値の情報を通過させていることになるわけです。
ここでは実際に2という量が計算されたとしてください。
では、続いてSigmaを大きくします。
Sigmaを大きくすると、この元の濃淡の情報がより含まれるようになりますから、
当然ながら平滑化したガオシャンフィルターの値スロックは大きくなるわけで、
DOGは、よりSigma6で平滑化した値からSigma4で平滑化した値の引き算でしたから、
引き算をすると、DOGの値は10になるわけです。
続いてSigmaをもっと大きくします。
そうすると、ここが広くなったので、その分元の情報がよりスロックされるようになるわけで、
DOGの差分を取ります。
そうすると41になりました。
これをどんどんどんどん続けます。
さらにSigmaを16にすると、全体がより含まれていますから、一番大きな値に変わりました。
そこで差分を計算すると、47になります。
もっと大きくしましょう。
もっと大きくすると、全体が十分に含まれていますから、
103という値になります。
そこで差分をとると、3になるというわけです。
今度はこのDOGのスケールスペースから、
この値が極大値であるかどうかということをチェックしていきます。
そうすると、当然ここの時は3と41と比較すると、
47ですから一番大きな値、極大値となっていますので、
この時のスケール、すなわちSigma10、この点に関しては、
Sigma10の時によりその範囲、Sigma10ですとこういう形状のガウシアンですから、
だいたいこれぐらいの範囲により多くの情報を含まれていますよということで、
このスケールの値を求めることができるというわけです。
これがDOGにおけるスケール探索の仕組みとなります。
これを二次元的にいろんなところを操作していって、
極大値のところを探していくということをします。
実際に画像でこのDOGの処理をしてみました。
これAという文字の画像です。
そこでDOG処理をするといろんな点が出てきますが、
ここでは一つの点に注目します。
この真ん中の×の点です。
この×の点を中心にSigmaを変えたときのDOGの値をプロットすると、
このような変化となりました。
このときの極大値はここですから、Sigmaが5ということになります。
このSigmaが5である範囲を元の画像で表現すると、
ちょうどこの赤色の範囲に対応しています。
すなわち、Aという文字の上の領域のよりパターン、特徴を含まれているようなところを
ちゃんとSigmaが捉えているということがわかります。
ではですね、画像を2倍に大きくしました。
その画像に、2倍に大きくした画像に対して、
DOG処理を行います。
このDOG処理を行うと、Sigmaを変えていくと、
このようなDOGの値が計算され、極大値は10というのが得られます。
当然ながら、画像を2倍に大きくしていますから、
このSigmaの値もちゃんと2倍のところで極大値が得られたという、
この画像におけるSigma10に対応する範囲は、
この赤色で囲った円の領域です。
すなわち、小さな画像でも大きな画像でも、
同じ範囲を捉えることができているというのがわかると思います。
なので、であればどうすればいいかというと、
この円の大きさを一定の大きさにして、
こちらもこの円の大きさをこちらの同じ大きさにして、
それでこの2枚同士を比較すれば、
大きさの変化、いわゆるスケール変化に関係ない状態で、
同じものであるかどうかという対応点を求めるときに
使うことができるわけです。
これがスケールに対する不変性を得る、
このDOGによる効果になります。
よって、シフト特徴量はスケールに対して不変な特徴量を
計算することができるようになるというわけです。
続いて、ここまでの処理、DOGによってキーポイントの点と
そのスケールを求めることができました。
これは今、いろんな画像のいろんな場所で、
このような特徴点、キーポイントの候補が出てきます。
残念ながら、キーポイントとして向かない点も今検出されていますので、
次の処理でよりキーポイントとして好ましい点だけを
取り出すようなことをしています。
まず、先ほどのDOGの処理によって出てきたキーポイント候補点を
画像に表示するとこれぐらいになります。
1枚の画像から1895点のキーポイント点というものが
候補として、DOGからDOG処理によって計算できます。
この中から見てもらうと、
例えば、対応点を計算するためには、
その特徴点、キーポイントが非常にユニークな特徴を
持っていないといけない。
すなわち、似たようなものがあるようなキーポイント、特徴点は
対応点を求めるためにあまり向かないということになるわけです。
例えば、どういう点が向かないかというと、
こういうエッジの上に乗った点ですね。
エッジの上に乗った点は、
例えば黒白の領域でこういう点が出たとしますね。
こういう点は、例えばここの点でこういう風に出たとした場合、
どちらも同じようなパターンですね。
なので、似たようなものがたくさん1枚の画像の中に含まれているようなものは
ユニークな対応点には向かないわけです。
なので、このようなエッジ状の点は対応点に向かないので削除したい。
それが最初にやることです。
すなわち、エッジ状のキーポイント候補点は
対応点の計算に向かないので削除してあげます。
そうすると、この1895点が1197点まで減ります。
どのように削除するかはこの後説明します。
そしてもう一つです。
DOGのキーポイント探索をするときには、
周辺と比べて27キンボウですね。
実際は27キンボウと注目画素を比較して
注目画素が極大値か極小値であるかということで
キーポイントがそうでないかという判定をしました。
その周辺領域だけを見ると極大値、極小値が
かなりたくさんのところで出てしまいます。
よりはっきりしたところで出る場合はいいんですけども、
このようにテーブルが真っ白ですよね。
テーブルが真っ白だけどこういう特徴点が出てしまう。
このテーブル上の点は非常にコントラストが低い点ですね。
明暗さがはっきりしていません。
だけどキンボウ、本当に周りの三画素、スケール上と下の
3×3画素の領域を見た場合は、
やっぱり真っ白とはいえ同じ値を持っているわけではありません。
多少微小な変動があります。
従ってその微小な変動を見て
このキーポイント候補が出てしまうことがあるので、
それでコントラストが低いような点というのは
そもそも対応点に向かない点ですから、
ローコントラストのキーポイントはノイズの影響を受けやすいので
このように削除しましょうということをするわけです。
ではこの2つの処理によって
候補点の絞り込みを行います。
ではまず事情の点を削除します。
事情の点を削除するために
ここではヘッセ行列というものを使います。
ヘッセ行列というものは何かというと、
例えば先ほどの方法で、DOGで
推定したスケールのDOG画像があります。
そのDOG画像のある点を中心としたときに
X方向の2回微分、Xと方向で1回微分して、
次にY方向で微分したDXYと、
あとY方向の2回微分を計算して
このように表現する行列をヘッセ行列、
ヘッシアマトリックスというふうに呼びます。
このヘッセ行列の第1個誘致をα、
第2個誘致をβというふうにします。
このときαの値もしくはβの値を見ることによって、
このヘッセ行列から計算した第1個誘致であるαと
第2個誘致であるβの関係で、
そこの点がどういう点であるかという
3種類に分類することができます。
αとβがともに小さいところは、
X方向にもY方向にも
変化がほとんどないということを表しますので、
フラットの領域ですよという、
すなわち濃淡変動が、明暗の変動が
ほとんどないところですということが分かります。
さらにαとβがともに大きいところ、
ともに大きいところはコーナー点と言えるわけです。
これはどういうことかというと、
X方向にも変化が勾配が大きくて、
Y方向にも変化が大きいわけですから、
コーナー点みたいなところである可能性が高いと言われます。
ではエッジの点はどういう性質があるかというと、
エッジはある方向に対しては勾配が大きいですよね。
縦エッジであれば横方向に対する変化、
勾配が大きいわけです。
だけど縦に見ると、縦エッジの場合は変化がないわけです。
なので片一方の勾配が大きくて、
片一方の勾配が小さいような領域がエッジとなるわけです。
もちろんこれ逆回りです。
βが大きくて第1勾配が小さい領域、ここですよね。
こことここがエッジの性質を持つ点であるということになるわけです。
そこでそれをこのような式を使って表現しています。
こちらのヘッセ行列の対角成分の和、いわゆるトレースですね。
トレースはαプラスβで計算できます。
同様にヘッセ行列のこの行列式、ディターミナントはαβで計算できます。
この値を使ってこのような式を考えます。
この式はトレースはαプラスβ、ディターミナントはαβですからこのような式になります。
この式を見てみますと、ここではエッジであるかどうかということを判定したいので、
エッジであるとよりこの値が大きくなるように表現したもの。
例えばフラットの点もしくはコーナー点ですと、αとβの値が同じ値ですよね。
ほとんど同じような値を持ちます。
ということはαとβがアルファと似ていれば、下は2αになりますよね。
同じようにαプラスここがαになりますから、2αですから同じ値になります。
なので要は全体的にこの、自乗して割りますが、
全体的にαとβの値が同じような値を持つときは、この比率の値は小さくなるというわけです。
ここでエッジの点を考えてみましょう。
エッジの点はαもしくはβのどちらかが一方的に大きいわけです。
大きい値と小さい値です。
その掛け算をすると値は小さくなります。
小さい方に引っ張られますよね。
分母は小さくなる。
一方分子はαプラスβですから、片一方が大きければその事情を取りますからどんどん大きくなります。
従って分子は大きくなって分母は小さくなるので、
この式はもしその点がエッジであればあるほど、
この比率の計算した結果は大きくなるように計算されるというわけです。
実際に、これはハリスのコーナー検出という方法で使われている方法の
先ほど紹介した勾配を実際に計算したものです。
例えばこのような縦エッジの場合ですと、
横方向に勾配を計算するとここにエッジ成分が出ます。
Y方向の勾配は縦方向にはほとんど変化がありませんから、
ノイズが強調されたような結果が出ます。
フラットのところはX方向にもY方向にもともにエッジが出ないことがわかります。
勾配が小さいということがわかります。
コーナー点は同じようにX方向とY方向にそれぞれ勾配をとったときに
大きな勾配が計算されるということがこの結果を見てわかると思います。
なので今とらえたいのはこのような点ですね。
ここを捨てたいのでこのようなところを見つけてきている。
それをこことここから見つけてきたというわけです。
実際にこのX方向とY方向の勾配の分布を調べてみます。
そうするとフラットの領域はX方向の勾配もY方向の勾配も小さく分布していることがわかります。
一方このようなコーナー点はX方向の勾配もY方向の勾配も
より大きく両者にそれぞれ広く分布しているということがわかります。
同様に続いてエッジの点はこの場合はX方向だけに対してこういう分布が大きいということがわかると思います。
ちなみにこの分布の楕円形状がそれぞれ
ラムダ1、ラムダ2、これはどんな感じでしょうか。
こちらがラムダ1、ラムダ2となります。
これもこうなると広い方がラムダ1、こっちがラムダ2になるわけです。
これを見てわかるようにこの分布の場合は片一方の固有値が長形がかなり長くて単形との差があるというわけですよね。
これが先ほどの片一方が大きくて片一方が小さいという分布を表しているわけです。
はいでは続いて今度はこのようにして比率を計算したのでそこに式値を導入します。
式値を導入してその比率が式値よりも大きい場合はこれはエッジの点ですからそれは捨てちゃいます。
一方比率が式値より小さい点はエッジの点でない可能性が高いのでキーポイント候補点として採用するわけです。
もともと1895点あったこのようなキーポイントの候補点がこの処理を施すことによって
こういったエッジの点がなくなることによって1197点までこの場合は絞り込むことができたというわけです。
では続いてキーポイントのローカライズを行います。
DOGを作成した時にダウンサンプリングを併用していますので空間的な解像度が落ちているわけです。
そこで元の原画像の位置においてどこになるかということをサブピクセル推定と言いますけども
サブピクセル推定を行います。
どうやってやるかというと、DOGの関数がありますので、DOGの関数をテーラー展開して
そしてこの周りのDOGの観測した赤色の点があるのでそこから0となるようなこの極大点ですよね。
この極大点を求めてその極大点の座標XとYと
実際はこれシグマというスケール方向に対しても内装というか一番好ましい0となる点を求めます。
そうすることによってその各点ごとにキーポイントのXとYとシグマというものを計算し直すということをします。
その時計算し直したXとYとシグマにおけるDOGの値を求め、そのDOGの値が式値0.03よりも小さいときは
そこはノイズの影響を受けやすい点であるということになりますので削除するわけです。
そうすると1897点の点が421点に絞り込むことができるわけです。
この残った421点がシフトのキーポイント候補点として使われるということになります。
ここまでがキーポイント検出のアルゴリズムでした。

続いてシフトアルゴリズムの特徴記述について説明します
特徴記述においてはまず最初にオリエンテーションの算出を行います
オリエンテーションの算出とは
このような点とスケールで表現された中の領域を使って
代表的な方向、方位というものを求めます
さあではなぜこのようなオリエンテーションを算出するのかということなんですが
画像が回転していたときに
その回転していても必ず同じ方向を取り出すことができれば
その方向を正規化してあげれば
回転に不変な特徴量を算出することができるようになるわけです
そのためにはその局所領域における代表的なオリエンテーション
これでここでオリエンテーションと呼んでいますが
例えば方位、方向を算出することをします
さあではどのようにオリエンテーションを算出するか説明します
まずキーポイントの点とスケールが検出されました
そのスケールが検出されたときの平滑化画像の領域を拡大してみます
この緑色の四角形が一つ一つが画素だと思ってください
まず何を計算するかというと
この画素ごとに、この一つ一つの緑色の画素ごとに
勾配強度とオリエンテーションを算出します
勾配は隣り合う画素同士の差分で
x 方向の勾配と y 方向の勾配を算出することができました
したがって u プラス 1 から u マイナス 1 を差をとって
ここでは強度にするので自乗をとります
同様に v プラス 1 から v マイナス 1 縦方向ですね
縦方向 y 方向の差分をとって
それを強度にするため自乗して
それぞれ自乗したものを足してルートをとるというわけです
これがこの各座標ごとに、各画素ごとに
勾配の強度を求めます
そしてここでは上はこれは縦方向ですから
デルタ y で下はデルタ x を表します
すなわち x 方向の勾配分の y 方向の勾配を表していますので
これはその勾配の方向であるオリエンテーションを
アークタンジェントによって計算することができるわけです
これはどういうことかというと
デルタ x デルタ y ともに大きな場合は
こちらの方向ですよというシーターを計算することができるというわけです
この計算、勾配強度とこの方向であるシーターを各画素ごとに求めます
この矢印があるかと思いますが
矢印の長さがこの勾配強度で
矢印の向きがこちらのシーター、オリエンテーションを表しているわけです
続けて今度はこの各画素で計算した勾配強度とオリエンテーションを使って
勾配方向ヒストグラムを算出します
勾配方向ヒストグラムは何を表すものかというと
横軸が勾配方向を表します
オリエンテーションは0から360度まであります
ここではこのオリエンテーションを360度を10度間隔に区切ります
それ一つ一つをビンと呼びます
一つのこの枠をビンと言いますけど
ここでは360度を10度ずつ36方向に理算化しますので
36個のビン、受け皿があるという状態です
そこにこの計算した勾配強度にさらに重み付けしたもの
重み付けした勾配強度を該当するオリエンテーション方向に投票していきます
例えばここの画素に対して勾配強度と勾配方向を求めます
その勾配強度に関しては
勾配強度をここを中心としたガウス窓の値で
重み付けして勾配強度を計算します
これはなぜこのようなガウス窓を使うのかというと
この端の方の特徴と真ん中の特徴ではどっちが重要かというと
やはり真ん中の方がより重要であるということが言えます
この端の方に大きな勾配強度が発生すると
その端の方の勾配強度がこの勾配方向ヒストグラムに加算されますので
その影響を受けてしまいます
そこでより真ん中の情報を使いましょうということで
各座標の勾配強度にガウス窓をかけて
重み付けした勾配強度をここの画素の持つシーターに対応する便に投票するという
これをすべての画素について行うと
このような勾配方向ヒストグラムを算出することができるようになります
続いてこの算出した勾配方向ヒストグラムから
代表的なオリエンテーションを算出します
どのようにするかと言いますと
今、横方向が勾配方向で現在は36個の便に分けられています
縦軸は先ほどの重み付けした勾配強度を各画素に加算したものです
この中からどのように算出するかというと
まず最大となるところを見つけます
この最大値の値より80%以上上にあるところをですね
正確には80%以上となるピークですね
ここがピークここですね一つだけですから
ここをここのキーポイントのオリエンテーションとして割り当てます
ここの便は何の方向かということがわかりますので
こちらの方向ですよというふうに割り当てを行います
例えばこのようなコーナー点がキーポイントとして検出された場合
その勾配方向ヒストグラムはどのようになるか考えてみましょう
当然x方向とy方向の勾配が2つ強く出ます
なのでこのx方向とy方向の勾配のところに対応する2つのピークが出てくるわけです
実際にどのように計算するかというと
まず最大値を求めます最大値はここだとします
この最大値の80%以上のところを取り出すと
緑のところも含まれてしまうわけですが
先ほど紹介したようにピークのところだけを取り出します
この緑のところはピークではありません
従ってこの場合はこのこことここの2つのオリエンテーションが割り当てられます
ということはどういうことになるかというと
この局所領域によるオリエンテーションはこういうふうに2つの方向があるという
ゆくゆくはこのオリエンテーションを使って正規化をして特徴領を算出します
従ってShiftの場合は面白いことに
同一点で2つ以上のオリエンテーションが検出されたら
オリエンテーションごとに正規化をして特徴領を記述することになるわけです
従ってこの点においては2種類の特徴領が記述されることになるわけです
では続いて特徴領の記述について説明します
このようにここまでの処理でキーポイントの座標とスケール
そして勾配方向ヒストグラムからオリエンテーションが算出されました
続いてこれらの値を正規化して特徴領を計算します
この128次元の特徴ベクトルがShiftの特徴領となるわけです
ではまずこの算出したキーポイント点とスケール
そしてオリエンテーションに対してどのように特徴を記述していくか説明します
まず記述するためのウィンドウを作ります
この記述するウィンドウはスケールの大きさに合わせた大きさとなっています
そしてさらにこのオリエンテーションの向きに合うように
特徴記述するウィンドウを回転するわけです
そしてこの特徴記述ウィンドウを順番に特徴領記述することで
スケール変化と回転に対して
普遍な特徴領を記述することができるようになるわけです
ではここではどのように具体的に特徴領を記述していくか説明します
まず先ほどのこの向きのオリエンテーションを上向きにした領域をこのように表示しました
ここから特徴領の記述をします
まずこの緑の四角形一つ一つが画素に対応します
この画素はこの場合は1,2,3,4,5,6,7,8掛け8画素の領域になっています
この8掛け8という大きさはスケールによって決定されます
なのでより大きなスケールが選ばれた場合は
例えば64x64だとかという風にサイズ大きさが
画像の切り出されたウィンドウの画素数が異なるわけです
そこでこの領域を4x4の16個の1個1個のこのようなブロックに分割をします
そしてこの分割した領域この場合は
ここの2x2の領域がここに対応しますね
したがってこの一つのブロックに対応するこの局所領域の勾配情報から
また新たに8方向の勾配方向ヒストグラムを計算します
これは具体的にはこのように8方向の先ほどの横軸が勾配方向ですね
これが8方向ですからすなわち45度間隔のここに対応する勾配情報から
勾配方向ヒストグラムを算出します
そしてこの一つ一つがその特徴領になるわけです
これがそれぞれの方向の勾配強度を表しているわけです
これを次のブロックに対応する領域から計算して
それぞれすべてで16ブロックごとに8方向の勾配方向ヒストグラムを計算する
そして順番にまず左上の1つ目2つ目3つ目という風に値を記述して
次2つ目のブロックの順番に特徴値を記述していくわけです
そうすることによって4×4ですから16ブロック
そして一つのブロックには8方向の値を持ちますから
結果シフトの特徴量は128次元の特徴量となるというわけです
ただしこの値は勾配の強度を使っていますので
明るいくらいによって勾配の大きさが変わります
そこで最終的にはこの128次元の各特徴ベクトルの長さは
そのベクトルの相和で正規化をします
これがシフトの特徴量になるわけです
ではここまでシフトアルゴリズムのまとめです
シフトの処理を大きく分けてキーポイント検出と特徴量給付に分けられました
キーポイント検出の中ではまずDOGを使ってスケールとキーポイントを検出します
この処理によってスケールに対する不変性を獲得します
続いてエッジだとかローコントラストな候補点を削除しました
それがキーポイントのローカライズでした
これによって対応点に向かない点をなくなるので
ノイズに対する断片性を獲得することができるようになったというわけです
2つ目の処理である特徴給付ではまず最初にオリエンテーションを算出しました
特徴量を給付する際にオリエンテーションに合わせて
給付する空間を回転させますので
これによって回転に対する不変性を得ることができるわけです
最後に特徴量を積化してあげます
これによって軌道変化に対する断片性を獲得したというわけです
それぞれの処理によってこのような効果を獲得したというわけです
シフトアルゴリズムはこれまでの画像処理のアルゴリズムに比べて
やや難しかったかもしれませんが
それぞれの処理でどういったことを目的に
そのアルゴリズムができているかということを理解できるといいと思います
では続いてこの128次元の特徴量
これを特徴ベクトルと呼びますが
この特徴ベクトルをどのように使うか
実際に対応点を求める処理について説明します
これをキーポイントマッチングと呼びます
今ある画像においてこのようなシフトの特徴点キーポイントが得られたとします
画像2においては1,2,3,4,5個点のシフトのキーポイントが得られたとします
それぞれのキーポイントごとに128次元の特徴ベクトルを持つわけです
そのシフトの特徴量をVで表していますね
Kは何番目のキーポイントであるかということを表します
さあではあるキーポイントとあるキーポイントの228次元の特徴ベクトルに対して
128次元ですからそれぞれの値の差をとって
自乗したものをサンメンション、相和をします
そしてルートをとっているわけですから
これは128次元の特徴量間のユークリット距離を計算していることになります
したがってまずある画像1のキーポイントと
画像2のすべてのキーポイントとこのユークリット距離を計算します
そしてユークリット距離の値がこのように求まりました
ここで対応点としては距離が小さければ
その特徴ベクトルが似ているということになりますので
距離を見て一番小さなこの点、すなわちこことここが対応点ですよというふうに判定しようと思います
しかし実はそうではなくて
もう一つ対応点を求めるために
一番小さい点と2番目に小さい点を使って決定します
これはなぜかというと
小さいのが12で2番目に小さいのが13だったとします
そうすると値は小さくても
こちらの画像の中に似たような点が複数あるということになってしまう
似たような点が複数あるときは
これはユニークな対応としてはあまり好ましくありません
そこで先ほど計算した距離をまず
小さい順にソーティングします
ソーティングした後に2番目に小さい点の距離の値を持ってきて
この式の条件に一番小さい値が成り立つかどうかということを確認します
ということかというと一番小さいのは12ですから
ここに12が入ります
続いて2番目に小さいのが47になるわけです
47に0.6をかけた値よりも
一番目に小さい距離が小さければ
この場合はこの一番目に小さいこの点とこの点は
多様点ですよという風にしましょう
もしこの条件が成り立たなければ
すなわち一番目に小さい点と2番目の小さい点の距離が
あまり変化がない値が近いというわけですよね
その場合はそこは似たような点で
ご対応である可能性があるので
対応点として採用しないというわけです
ちなみにではこの値を比率、この係数ですね
この係数をどのように決定するのかというわけです
例えば係数を0.6と小さく設定をすると
このような対応点が得られます
もちろん0.6と小さくすると
一番目の距離と2番目の距離が離れていないと
一番目の距離の点同士を多様点として求めることができませんので
対応点の数は当然ながら少なくなります
ただし、一つ一つの対応点を見ていただくと
より確からしい対応点になっていますので
すなわち誤った対応点が少ないという効果を得ることができます
これは一般に画像認識とかに使うときは
このシフトの特徴量を画像認識に使う場合に
このようなより小さな値で厳しく対応点をチェックして
もっと使うことがあります
じゃあ先ほどの係数の値を0.9と大きくしました
0.9とすると
1番目に小さい距離と2番目の距離が
あまり差がなくてもいいですよということになりますので
このように結果、対応点がたくさん増えます
こちらと比較すると対応点がたくさん増えているのがわかると思います
しかし、中には残念ながらこの点を見てわかるように
誤った対応点も含まれるということになるわけです
このような値をなるべく大きくして
より多くの対応点数を得るということは
どういったところで使うかというと
画像間のレジストレーションに使います
例えばイメージモザイクをやりました
イメージモザイクはある画像とある画像の
特徴点を対応付けして
変換行列を求めるというものでした
この変換行列を求めるときに
画像全体からこの対応が取れていないと
画像全体を表す変換行列を
うまく求めることができません
もし厳しくしてしまうと
部分的なところしか対応点が得られないので
より好ましい変換行列が求められないわけです
そこで画像のレジストレーションを
使うような用途の場合であれば
このように値を大きくして
より多くの画像全体から
対応点が求められるようにしておくというわけです
このように用途によって
この係数の値を設定します
では続いて実際にシフトが
どれくらい回転、スケールの大きさ、軌道変化、
遮影変化に効果があるか
といったことを調べてみましょう
今この画像に対して
それぞれの画像と対応点マッチングを行います
まず回転変化に対する
シフト特徴量の変化を見てみましょう
この画像でシフトのキーポイントを算出すると
この点でこの大きさで
このオリエンテーションのキーポイントが得られました
他にもたくさん出るわけなんですけども
この一つに注目をします
そしてこの画像が45度回転した画像に対しても
同様にシフトを算出します
そうすると同じ点で
だいたい同じような大きさで
ちょっと異なるオリエンテーションが
算出されたというわけです
この場合、上の元の画像においては
このようなスケールでこのオリエンテーション
下の45度回転した画像からは
このようなスケールでこのようなオリエンテーション
これを見てもらう上と下を比較してみると
ほとんど同じ大きさ
すなわちスケールがちゃんと得られているのは
わかると思います
ただしこのオリエンテーションの方向が
ちょっと異なるというのが
わかるんじゃないかなと思います
上の方向を下で描くとこんな感じですかね
なのでオリエンテーションの方向が
少し変わったということがわかると思います
実際にそれぞれで算出したスケールと
オリエンテーションで正規化して
特徴量を記述すると
このような特徴量になります
このような特徴量間の
ゆっくりと距離を計算したら
この場合は42.6という距離になりました
これは後で比較に使います
続いてスケール変化における距離です
こちらのシフト特徴量に対して
画像を2倍大きくしたところから
シフトキーポイントを算出すると
やはり同じような点が出ました
ただしそこの領域を見ると
こちらに対してやはり2倍大きくしているので
2倍ぐらい大きいスケールが算出されています
オリエンテーションは回転が起こっていませんので
大体同じような方向を向いていることがわかります
それぞれの領域からシフト特徴量を算出しました
形状が非常に似ているのがわかると思います
実際にゆっくりと距離を計算すると
27.3という値になります
続いて軌道変化です
これは画像が全体的に暗くなったときです
この場合は同じスケール
同じ向きのオリエンテーションが算出され
それぞれシフト特徴量がこのような形を持ちます
ほぼ同じような形状を持っていることがわかります
したがってゆっくりと距離は7.8と小さくなります
続いて遮影変化です
遮影変化というのはどういう変化かといいますと
皆さんのカメラに対して
こういうのは拡大縮小ですね
こちらが回転になりますね
遮影変化はこのような変化が起こることを
遮影変化というふうに呼びます
したがってこのような遮影変化が起こると
似たような点が検出されているのですが
このスケールの中に遮影変化によって
異なる上とは異なる領域が含まれることになります
したがって異なる領域が含まれるため

その領域の特徴量は当然、上とは異なる特徴量になってしまいます。
したがって距離は149ということで大きくなるというわけです。
ではこれまでの回転変化、スケール変化、軌道変化、遮影変化における
全体で対応点を求めた結果を表示しています。
回転はたくさんの対応点が得られています。
スケールもうまく対応点が求まれていることがわかると思います。
軌道変化も問題ありません。
遮影変化です。
遮影変化は先ほど説明したように、
遮影変化するとスケール領域に異なる領域が含まれてしまうので、
結果、対応点がうまく求めることができません。
シフトができるのは回転変化、スケール変化、軌道変化が生じても
似たような特徴量をちゃんと計算してくれるわけです。
遮影変化に対してはうまくいきません。
ゆっくりと距離を比較しましょう。
ゆっくりと距離を見ると、
軌道変化、スケール変化、回転変化は小さな値が出ていますし、
より多くの対応点を求めることができていることがわかります。
残念ながら、遮影変化においては距離が大きく、対応点が少ないことがわかります。
何度も言いますが、シフトは軌道変化、スケール変化、回転変化に
元件な特徴量を算出するアルゴリズムです。
遮影変化に対して元件な特徴量を算出するわけではありません。
その違いを理解しておきましょう。
ちなみに、遮影変化とアフィン変化の違いを説明します。
アフィン変化は、この3点で変形を表現できるものをアフィン変化と言います。
一方、遮影変化は、この4点の変化によって表現されるものを遮影変化と呼んでいます。
シフトでは残念ながら、この遮影変化に対応することはできていないというわけです。
では、シフトがどのように実際に画像認識に使われるか見てみましょう。
これは我々が作ったデモンストレーションの一つです。
大きな画像に対して、こちら小さな画像を入力しています。
小さな画像、パッチ画像ですけども、これが大きな画像のどこにあるかといったことを
多様点マッチングした結果から認識しています。
ジグゾーパズルではないんですけど、ジグゾーパズルみたいなことをしていると思ってみてください。
これをですね、見ていただくとよくわかるように回転してますね。
入力が回転していても一度も間違いません。
また、この画像が出てきてわかるようにミッキーマウスがたくさん映っています。
このような画像においても、どこであるかということを一切間違えることなく
シフト特徴領によって対応点を求め、それがどこにあるかということを見つけ出すことができるわけです。
これはこれまでこの講義でやったテンプレートマッチングを考えてみましょう。
これをテンプレートとしてだーっと探すということを考えると、
似たようなものがかなり中にいっぱい含まれていますから難しい問題ですね。
しかも回転も含まれています。
ですが、このシフトを使うとこれぐらい眼鏡に入力した画像が
この大きな画像のどこであるかということを正確に認識することができるというわけです。
その例になります。
このように今度は具体的にシフトを使った画像認識として
特定物体認識について紹介します。
特定物体認識というものは何かというと、テンプレートです。
ここではこのようなテンプレートがあって、
これらのテンプレートが入力画像のどこに写っているかということを知るということが特定物体認識です。
まず何をするかというと、
シフトの特徴点をそれぞれ計算して多様点マッチングを行います。
多様点マッチングをした後に、その多様点の関係からどのように変化しているか、
このパターンが多様点マッチングの多様点の関係から
どのように変形しているかといったことを求めます。
これはアフィン・パラメータになります。
そうすることによって、それぞれのテンプレートがこの画像中のどこに写っているかということを知ることができるわけです。
ここでこの方法の良い点は、このテンプレート画像は一部分が隠れていますよね。
隠れています。隠れちゃっているわけです。
隠れていてもこのようにちゃんとこの物体を認識することができるというわけです。
ではもう少し詳しく説明をします。
入力画像があると、まずテンプレート画像が一つ目のテンプレートと多様点マッチングを行います。
そして多様点マッチングした結果からこのアフィン・パラメータ、変換するアフィン・パラメータを求めることで、
この領域画像がどこにあるかということを知ることができる。
同様に二つ目のテンプレートに対しても多様点マッチングをしてアフィン・パラメータを求め、どこにあるかを認識します。
三つ目のテンプレートについても同様です。
このような処理を行うことで特定物体認識を実現することができます。
これは我々が共同研究である企業さんと共同研究でやった成果です。
このシフトを使って交通道路標識の認識を行いました。
これ、線がたくさん引っ張ってあると思いますけれども、これがシフトの多様点を表します。
いろんな標識と入力画像のシフトの特徴点の多様点を求め、
そしてその多様点が多く見つけられたものがその標識というふうに認識しているというわけです。
ただし標識は画像の中で小さくしか映りません。
なので先ほどの特定画像の特定物体の認識のように大きく映っていないので、
多様点の数が少ないという問題があります。
そこで我々が考えたアプローチはシフトの多様点を求めます。
シフトの多様点を求めた後に今度は1点の多様点の関係から
このテンプレート画像の基準点がどこにあるかということを投票したわけです。
この時スケールと回転というオリエンテーションがありますから、
1点の多様点が求められればこのスケールとオリエンテーションの関係から
基準点が画像中のどこにあるかを投票することができる。
1点1点の多様点から投票することができるので、
この画像中に多様点が少なくてもより投票数を稼ぐことができるわけです。
これによって先ほどのように物体が小さく映っていても
ちゃんと認識ができるようになったというわけです。
他にはどういったところで使われているかというと
類似画像の検索に使われています。
皆さんAmazonのアプリを使ったことありますか?
Amazonのアプリではカメラを使って本を検索することができます。
このような形です。
まずその本をカメラで写すと多様点を求め、
そしてその多様点からその本は何であるかということを
このように表示してくれます。
したがってその本がここの下に出てきているように
Amazonではいくらで売ってますよという情報が得られますので
そこでAmazonで注文することができるというアプリです。
こういったところにも画像の認識、
しかもこのようなシフトみたいなアルゴリズムが
実際に使われているというわけです。
他の例は前回紹介したようにイメージモザイクです。
このように画像をそれぞれバラバラに撮影します。
このバラバラに撮影した画像間で多様点を求めます。
そしてその変換行列を求め、画像を変換してつなげることで
1枚の大きな画像になるわけです。
これは実際に黒板を使って授業をやっていたときに撮影した例です。
このように1枚目、2枚目、3枚目の画像を撮影します。
当然そのときに部分的に重なりを持つように撮影してください。
そしてこのシフトを使って多様点を求めつなぎ合わせることによって
このような1枚のモザイク画像を作ることができます。
それぞれに僕が写っていますからこのように
ちょっと薄くなっていますけども
1枚の画像に同一人物が3人写ったような形で
1枚の画像として表現することができます。
これはぜひここのサイトに
AutoStitchというソフトウェアがダウンロードできます。
Windows、Macそれぞれで動きますので
ぜひ皆さん実際にやってみてください。
では今日はここまでです。
これまでシフトの特徴点検出と記述について説明をしました。
これまでの画像処理のアルゴリズムに比べて
かなり難しかったかもしれません。
しかしそれぞれでどういったことをしているのかということを
理解するようにしてください。
このシフトのアルゴリズムが
どういったことをするためにどういった処理があるのかということの
関係性が理解できれば
画像処理全般をですね
ちゃんと理解できたと思っていただいていいと思います。
それではまた。

みなさんこんにちは 今日はコンピュータービジョンについて紹介します
この講義の最初にロボットビジョン、コンピュータービジョンを実現するためには
まず画像を取得して処理をして認識をしてそのシーンを理解することで
ロボットビジョン、コンピュータービジョンが実現できるという話をしました
今日はこのここまで学んできた画像処理の次であるコンピュータービジョンという世界が
どういうものであるのかということを紹介したいと思います
まずコンピュータービジョンにおいてまず画像とは何かということを考えましょう
画像は何らかのこのような3次元のシーンを撮影し
人が見る色明るさを保存したものでした
これはこれまでの講義の中でやってきました
そこで画像からこの画像からこのシーンである風景の情報を獲得
もしくは推定する技術をコンピュータービジョンと呼びます
逆に何らかのシーンを想定した画像を作成、生成する技術をコンピューターグラフィックスというふうに呼びます
したがってコンピュータービジョンとコンピュータグラフィックスは逆のことをやっている関係になるというわけです
コンピュータービジョンの起源はいわゆる人工知能AIやロボットの視覚部分、目の部分ですね
この機能を扱う問題として研究が行われてきました
このコンピュータービジョンもいろんな研究があります
例えば認識という関係では、そこで映っているシーンがどういうものであるかというシーン認識、文字認識、パターン認識
もしくは動いているものを認識する移動体検出、物体認識などいろいろあります
認識とは異なって、そもそもそのシーンの3次元がどうなっているかということを知る
3次元計測、モデリング、キャリブレーション、スラム、ストラクチャーフロンモーションといった分野の研究もあります
その他には観測対象の物体の表面の反射特性を推定するという問題
例えばこういうスマートフォンだと硬いですよね
光が共鳴反射するようなテクスチャーだとか材質だとか
もしくはこのようにちょっと柔らかいもの、この材質を求めるためにこういった反射特性を推定するという研究
同じように環境の光源、テクスチャー解析とか、いろんなコンピュータービジョンという分野の中には研究が行われています
技術的、理論的には信号処理、コンピュータグラフィックス、VR、AR、AI、ロボット工学などと一部重複して関連しており
基本的に画像に関する技術はすべてこのコンピュータービジョンという分野に含まれるということになります
ロボットを見てもロボットが入力するセンサーはいろいろあるわけですけれども
やはり画像から得られる情報は非常に多いので
このコンピュータービジョン、すなわちロボットビジョンというものがロボットにおいても重要とされているわけです
このコンピュータービジョンのまず3次元の計測ということを紹介します
3次元を計測する技術自体もコンピュータービジョンの研究になります
その中にも能動的な方式と自動的な方式があります
能動的な方式の中にはタイムオブフライト、光切断法、モーションキャプチャーシステム、構造加工投影法、
回れパターン法、フォトメトリックステレオ法などいろんな方式が提案されています
最近、とても多くの方が注目しているものがタイムオブフライトと言われる方法です
このiPhoneのカメラのところに実はこの能動的なタイムオブフライト形式の3次元計測が実は使われたりしています
一方で、能動的でない自動的な方式がありまして、それらをShapeFromXと呼ばれます
このXにはシェーディング、フォーカス、リフォーカスといったいろんなものが当てはまります
後で紹介します
あとはステレオ法、ストラクチャーフローモーション、動きから形状を獲得するといったような
自動的なこういった方法が研究されているわけです
ではまずステレオ法について説明します
我々人間も左目右目を持っています
この時に左目と右目があることによって
左目と右目で見たシーン、画像が少し異なるわけです
この異なる、どれくらいずれているかといったことを
視差という言葉で表現します
その視差を計算することで
例えば右と左の同じところが視差が大きいとずれていると手前にあって
視差があまり少ないと遠くにあるということがわかるわけですね
よってこの2枚の画像からこのような結果を出力することを
ステレオ法を使ってこの結果画像を作ります
もう少し詳しく説明しましょう
ある左の画像と右の画像を取得します
この時にこの画像はそれぞれ3次元空間のある点を
こちらに写像したものがこの画像上で観測できるわけです
なのでこの点とこの点が同一の点ですよという対応点が求まれば
カメラがどこに設置してあって
どういう風に姿勢が向いているのかということがわかると
そこからステレオ法を通過して
伸ばした線とこことここを結ぶ直線が交わる点が
すなわちこの対応点の3次元座標という風に求めることができるわけです
この対応点のズレの量というものを
画像上のズレの量を視差という風に呼びますね
この場合2つに見ている視点が異なるので視差が発生し
その視差を使って3次元値を求めることができるわけです
ちなみにこれは2台のカメラで同時に撮影するということもありますし
ある時刻で撮影してそれからカメラを移動してまた撮影しても同じです
カメラを移動させた場合を運動視差と呼びます
このステレオ法を拡張した方法が今多く取り組まれています
ステレオ法は左のカメラと右のカメラで同時に
しかも固定した状態で撮影してそこから対応点を求め
ステレオ視によって3次元形状、3次元情報を獲得しています
一台のカメラを時系列に動かして撮影することをした場合
この2時刻間、ある時刻とある時刻のこの2つの時刻間で
対応点を求めていくことによってこの3次元形状と
このカメラがどのように移動したかを同時に復元することによって
形状とこのカメラの座標がどう動いたかということを知ることができる
こういった研究をストラクチャフログモーションという
動きからストラクチャを求めるという問題です
さらには異なる時刻でいろんな人が撮影したカメラから
その場合これらのカメラがどこでどう撮影したかが分かりません
だけど複数の人が同一対象を異なる時刻で異なる場所で撮影したものから
この3次元の情報を獲得する方法をバンドル調整と言われています
ただしこの問題はこちらのストラクチャフログモーションの
時系列がバラバラになった状態と同じですので
これも一種のストラクチャフログモーションというふうにも呼ばれています
このストラクチャフログモーションはいろんな原理理論
基本的にはこのストラクチャフログモーションは同じ原理理論なんですけども
分野とか目的によって実際にそのアプローチ
少し呼び方が異なったりします
例えばコンピュータービジョンという分野ではストラクチャフログモーションと呼ばれますし
我々のロボット分野であればビジュアルスラムというふうに呼ばれたりします
他にはAR、オーグメンティッドリアリティの分野ではマーカレスAIだとか
コンピュータービジョンの分野でもマッチムーブという技術として使われているのが
このストラクチャフログモーションという技術になります
一般的にはVスラム、ビジュアルスラムという呼び方が最近はですね
よく呼ばれたりしています
ではこのスラムをちょっと紹介しましょう
スラムというものはサイマルテイニアスローカライゼーション&マッピングです
サイマルテイニアスというのは同時にという意味です
すなわちローカライゼーションとマッピングを同時にします
すなわち自分がどこにいるかということと合わせて
そのマップを作っていくということを同時にしてくれます
ちょっと見てみましょう
これは今ロボットが観測している領域が見えています
その観測している領域では全体の地図は分かりません
だけどそのロボットが移動していくと見える範囲が変わっていくわけです
その時に以前のところで獲得した地図と今の観測した結果をですね
つなぎ合わせていくことで
自分の自己位置がどこにあるかということを知ることができる
なのでロボットがこうやって移動していくことによって
新たな場所で情報を獲得して
その情報を今までの地図情報につなぎ合わせていくことによって
広い地図を作ることができます
これをスラムというふうに呼びます
ちなみにこのスラムでは大きな問題があります
今観測した情報を前作った地図情報につなぎ合わせていくわけですね
なのでつなぎ合わせる時に誤差が生じると
どんどんそれに合わせていきますから
誤差がどんどんどんどん蓄積するという問題があります
そこでこういったスラムというものでは
ループクロージャーという技術を使います
どういうことかというと
つなぎ合わせていってまた同じところに戻ります
同じところに戻った時に本来であればズレがあるわけです
ここを見ていただくと
こことここは同じ廊下なんですけども
ずれているのがわかります
ただしずれているんですが
画像上で対応点を求めた時に
これは以前求めた廊下だということがわかれば
そこで全体的にもう一度最適化を行います
そうすることによってより正確な
今ですねこのようにより正確な
このような地図を求めることができるようになるというわけです
これもスラムというコンピュータビジョンの技術になります
このスラムを画像だけでいろんなセンサーを使うんですけども
例えば車の場合であればどういうものを使うかというと
こういったIMUというセンサーから得られる
車の速度とか加速度等を使って
自己位置がどのように動いているのかということをですね
使って使うこともあります
さらに画像のみからスラムをしましょうというのが
ビジュアルスラムになります
このビジュアルスラムをやるときに重要となるのが
前回やったShiftの対応点マッチングですね
Shiftの対応点マッチングをすることによって
異なる画像間の対応点を求めることができるわけです
この対応点を複数の画像間で求めることによって
画像を使って自分の位置と地図ですね
この場合地図というのは
3次元形状を求めていくことができるようになります
では実際にビジュアルスラム
画像によるスラムの例を見てみましょう
この例では車載カメラ
車に積んだカメラ映像画像のみから
このように特徴点を出して
時系列間で対応点を求めることによって
この下のように地図をどんどん更新していきます
したがって車にカメラを設置するだけで
フレーム間、時系列間の対応点から
車がどのように移動したのか
そしてその周りの対応点の
3次元情報を取得することができるわけです
当然1フレーム前と次のフレームの位置が分かれば
そこからステレオ使用することによって
このように自分の動きと
さらにその周りの情報を地図として
3次元地図として獲得することができる
このような技術は最近ですと
こういったHMDの中でも使われています
この例をちょっと見てみましょう
これはマイクロソフトが出している
HoloLensというヘッドマウントディスプレイです
ここには、見てもらったら分かるように
いろんなカメラとかセンサーが付いています
このセンサーから見えている
3次元形状をこのように獲得することによって
その3次元情報に今、手の形状を当てはめることができる
手がどのような姿勢になっているのか
どういう操作をしたいのかということを
このヘッドマウントディスプレイで認識することができる
それに合わせてこの周辺に
こういった3次元のものを
オーグメンティットリアリティとして
CGで見せることもできるし
擬似的に触って操作しているようなことが
こうやってできるようになってきた
ぜひこのHoloLensは
いろんなコンピュータビジョンの機能が詰まった
デバイスになっていますから
いろいろ試してみるといいと思います
面白いです
他には、VisualSlamの例としては
複数の人が撮影した画像から
どのように3次元形状を復元しているかという
研究に取り組んでいる
ワシントン大学の例を紹介します
この人はスティーブン・サイツと言いまして
今はワシントン大学の先生なんですけど
その前にカーニギメロン大学で研究者をやっていました
その当時、僕も知り合い
その時に知り合った研究者です
彼らは何をやっているかというと
こういう有名な観光地があります
そこでいろんな人が撮影した画像が
アップロードされています
なので、インターネットにアップロードされている画像を
集めてきて、その画像から
多様点マッチングをして
そしてそれぞれのカメラが
どこから撮影したものか
かつ、合わせてその3次元形状を復元するというものです
そうすると、このように
複数の画像をいろんな人が撮影しました
そして、その対応点から
それぞれの画像がどこで撮影したのか
かつ、その対応点が
3次元的にどこに位置するものか
ということを求めることによって
このように、それぞれのカメラ視点から見た
位置情報が分かるので
それぞれの位置から見た画像と
かつ、このような大規模な3次元の形状のモデリングができるわけです
不動意ですけども、この3次元は
そこ行って誰かがセンサーを使って計測したのではなくて
観光した人が撮影してアップロードした画像から
こういう広い範囲の3次元形状を
復元したというわけです
非常に面白い研究だと思います
さあ、もう少し具体的に役に立つ例も紹介しましょう
最近のGoogleマップの
ARナビゲーションというのがあります
これはGoogleストリートビューと
うまく連動したアプリ、サービスになっています
皆さんも例えば、どこか知らない土地に行った時に
地下鉄を使って
地下鉄から出たとします
その瞬間、ナビは地図上で
こっちに行けってあるんですけども
そもそも自分自身が地下鉄から出た時に
どっちの東西南北を抜いているかというのが分からないので
地図上でどっちに行けば分かっても
自分のいる、立っている、向きから
どっちに行くのかというのを
知ることがよくできないですよね
よく分からなくなることがあると思います
そういった時にGoogleは
Googleストリートビューと言いまして
世界中のストリートの画像を持っていますね
ですかつ、このカメラ、いわゆるスマートフォンで
今の自己位置はGPSで撮りますけど
どっちを向いているのかということで
自分の位置だけでなく、そのカメラを向けている
この向き、姿勢をカメラ画像から
Googleストリートビューのデータと
マッチングすることで位置情報を獲得しています
ちょっとこれも見てみましょう
このように地下鉄から出てきた時ってよく
あれ、右左どっち行くんだろうってのは分からないですよね
これは東西南北になっていますから大丈夫なんです
こういった時にこのGoogleの
GoogleマップAIナビゲーションではカメラを使うわけです
カメラを使うことでそのカメラ情報と
Googleストリートビューのデータと
マッチングすることでこっちに行ってくださいっていうのを
教えてくれる、これ結構便利な機能だと思います
わかりますか、こうやってカメラの向きを変えると
地図の向きも連動して変わっています
このようにコンピュータービジョンは
皆さんの生活の中でいろんなところで
今使われようとしている技術になるわけです
今日はですね、画像処理の次に行われる処理である
コンピュータービジョンという分野の
研究をザーッと紹介しました
コンピュータービジョンの分野の研究は
まだいろいろいっぱいなされています
ぜひこういったコンピュータービジョンの技術を
高めることによって
ロボットにとってより良い視覚機能を
実現していきましょう
それが僕の研究でもありますし
その研究をですね
今一生懸命取り組んでいるというわけです
今日はコンピュータービジョンについて紹介しました
それではまた

みなさんこんにちは。今日は我々の研究グループマシンパーセプション&ロボティックスグループの研究紹介ということで
アフィン不変なキーポイントマッチングについて紹介します。
今日の内容はNANAのところでやった特徴点検出・記述におけるシフトの問題点を解決するような研究です。
キーポイントマッチングにおけるそれぞれの画像間の変化においてどういったことを考えないといけないかといったことをまず振り返りましょう。
まず画像間に回転変化が生じた場合は特徴点として点を求めれば良いわけです。
続けて画像間に拡大縮小が起こった場合はこの場合は点だけではなく領域を求める必要があります。
シフトはこの回転変化とスケール変化に不変な特徴量を検出・記述することができます。
しかし遮影変化についてはアルゴリズム上対応できていませんでした。
そこでこの遮影変化に対応するにはどうすればいいかといったことを今日は扱います。
この遮影変化に対応するためにはこのように楕円領域を求め、その楕円領域を一旦深淵に正規化してから特徴量を記述するということが重要となります。
回転変化に対してはキーポイント検出、スケール変化に対してはキーポイントの座標と東方性のスケール推定を行っています。
そして遮影変化に対応するためにはキーポイント検出とさらにアフィン領域、ここでいう楕円領域を推定する必要があるというわけです。
この表はこれまでのいろんな研究がキーポイント検出に関わるいろんな研究を年表という形でまとめたものです。
古くはLOG、そしてSHIFTが1990年代の後半に提案されてからいろんな手法が出てきたというわけです。
まず一番下は回転変化に対応する特徴点のみを検出するアプローチです。
続いて真ん中の段は回転とスケール変化に対応するための東方性にスケールをどのように推定するか、
すなわちスケールをどのように求めるかというアプローチがこのようにいろんな研究がなされてきたというわけです。
そして遮影変化に対応するためにアフィン領域の推定としては2002年にMSER、2004年にヘシアンアフィンという2つの方法が提案されたのみだけであって、
残念ながら2004年以降にこのアフィン領域を推定するための手法という新しい手法が提案されていないということがわかります。
これが逆に言えばいかにここが難しい問題設定であるかということがわかると思います。
ではまず従来法であるこのヘシアンアフィンがどうやってアフィン領域を推定するかについて紹介をしたいと思います。
ヘシアンアフィンは1つのアフィン領域を探索します。
そのキーポイント周辺の二次モーメント行列より固有値を計算します。
この固有値を使って領域を深淵に、楕円を求めます。
その楕円を深淵となるように繰り返し処理していくことで楕円領域を推定していきます。
まずこのキーポイント点に対する東方星のスケール領域の中から二次モーメント行列より固有値を計算します。
そして楕円の領域を推定していくことをどんどん繰り返していくことで、最初は深淵だったのですが、
繰り返し処理によってこのような楕円領域の当てはめを行うことができます。
これはこれで非常に良い方法ですが、実際にこういったことが起こります。
問いプログラムという問題設定で、実際の画像ではなくて分かりやすい簡単な例で、
このヘシアン・アフィンの良いところ、悪いところを確認します。
このような二つの楕円の形状が重なったこのような画像が入力されたとしてください。
この時、この画像からアフィン領域を推定すると、このようなアフィン領域が求められます。
上と下は同じ画像ですが、X方向に1画素、Y方向に2画素ずれている画像、
すなわち最初に探索する初期値の深淵の位置が少しずれているというわけです。
見た目はほとんど変わりませんが、実はこの中心値がずれているわけです。
中心値がずれたものからスタートしてアフィン領域を推定すると、
残念ながら上と異なるパターンに収束してしまうということがわかります。
このペシアンアフィンでは、この当てはめを行った楕円領域から深淵にして、
そしてそこの領域からシフト特徴量を記述します。
このシフト特徴量間の距離を計算すると、残念ながら当然違う領域を捉えていますので、
距離は大きくなってしまう。すなわちこのパターンとこのパターンはほとんど見た目一緒なんですが、
このようなずれを生じているだけで別物ということになってしまう。
これが従来のアフィン領域推定であるペシアンアフィンの問題点になるわけです。
我々はこの問題点を見つけました。
そこでこの問題点を解決するようなアプローチというものを研究で取り組んだわけです。
これを複数のアフィン領域推定、Multiple Hypothesis Affine Region Detectorということで、
ICCV、2015年に開催されたInternational Conference on Computer Vision、
コンピュータービジョンの国際会議、これの中でもトップカンファレンスといって非常に難しい国際会議なんですけれども、
そこで採択されまして発表した内容です。
この方法はこれらの入力画像に対して複数のアフィン領域を推定するというアプローチになっています。
したがってそれぞれの推定したアフィン領域ごとにシフト特徴量を計算し、
これらの距離計算をすることで部分的に一致するものがあるので、
この上のパターンと下のパターンは距離が小さい、すなわち同一ですよというふうに判定することができるという考え方です。
では実際にどのようにこの複数のアフィン領域を探索するか紹介します。
ここで非東方性のログフィルターというのを使います。
皆さんログフィルターはX方向もY方向も同じ大きさ、それを東方性のログフィルターというふうに言います。
ここではXとYで大きさが違うような、しかも回転するような非東方性のログフィルターというものの畳み込みを行います。
このような入力画像が与えられたら、ここでいろんなパラメータにおける非東方性ログフィルターを作り、
この非東方性ログフィルターをこのキーポイント画像に畳み込むわけです。
そしてその時の応答値を計算し、応答値が最大となる非東方性ログフィルターのパラメータがアフィン領域になるというアプローチです。
もちろん他のところでもピークが出れば、これが2つ目のアフィン領域になるというわけです。
さあ、ここで問題は、この非東方性ログフィルターを作るためには、
sigmaXというX方向の分散と、sigmaYというY方向の分散、そして回転を表すθというパラメータがあります。
これらに対してそれぞれ非東方性ログフィルターを作りますので、実際はこのようにたくさんのログフィルターとの畳み込み処理を行う必要があります。
実際このようなパラメータで非東方性ログフィルターを作ると、全部で4913個のフィルターができます。
ということは、画像のある点に対して、この4913個のフィルターとの畳み込み処理を行わないといけないというわけです。
これは計算量が膨大になってしまう。
それではですね、非効率なので、そこで我々はどうしたかというと、特異値分解という方法を使って、
この4913個のログフィルターを、1、2、3、4、5、6、7、8、9、10、11、12、13、14という、
14個の主成分フィルターというものと係数で近似するといったことを考えたわけです。
特異値分解についてはですね、説明を省略しますが、
これらの情報を、この少ない14個の主成分フィルターというものに分解してくれるアプローチというわけです。
ではどのように分解するかを簡単にだけ紹介します。
まず、非東方性ログフィルターをたくさん集めた行列を表現します。
この行列の横には、1つの非東方性ログフィルターの左上から右下までを並べた値が入っていると思っている。
したがって、1つ目、2つ目ということで、全部で4913個のログフィルターを
1次元ベクトルに展開して、それを並べたものが、スタックしたものが、このダーブルという行列です。
このダーブルという行列に特異値分解を適用すると、
U、S、Vの点値という行列に分解することができます。
このとき、このSという行列の対角成分は、このように固有値を持っています。
この固有値のどこまであるかが、この元々の行列のランクというものを表しています。
実際に調べてみると、14個の特異値を使うことで、全体の96%を表現できていることがわかります。
すなわち、ここを全部使う必要がなくて、14個に制限することができますよというわけです。
したがって、14個だけを採用して、そして行列をもう一度元に戻してあげると、
U、Sという行列とVの点値という行列になるわけです。
こちらのVというものの、ここの一つが縦に14個あるわけですけども、
一つ一つが固有フィルターと言われるものになります。
そして、こちらのUとSの行列が固有関数というもので、
この固有フィルター、ここでいうV1、V2、V14というのは固有フィルターで、
この固有フィルターにかける係数をロー1、ロー2というふうに言いますが、
この係数がこちらで表現される固有関数として求めることができるようになるわけです。
まず、固有フィルターを見てみましょう。
非統合性ログフィルターの行列を特一分解してできたときの固有フィルターの14個は、
このようないろんな面白いパターンを持つ固有フィルターが生成されています。
そして、それぞれの固有フィルターに対応した固有関数はこのような形状を持ちます。
シーターを変えていきます。
シーターによってこの固有関数の形状が変わります。
特にこのような点対称のものは当然シーターを変えても変わりません。
しかし、このようにシーター方向に対してパターンが異なるような固有フィルターにおいては、
固有関数もシーターに合わせて変化するということがわかります。
少し式を追って説明をします。
今ここまでできたことは何かというと、
シグマX、シグマY、シーターというパラメータを持つ非等法性ログフィルターがありました。
我々はこれを特異値分解を適用することで、
14個の固有フィルターと固有関数で表現される係数の掛け算でこれらを表現する、
近似するといったことを実現しました。
ここをよく見ると、この固有関数は最初にシグマXとシグマYとシーターを理算的にパラメタイズしていますので、
それに合わせた値を出力することができるだけです。
もしシグマXが1.62とかという、
ここで設定していない値におけるログフィルターを求めようとすると、
この場合はパラメータは理算的なので求めることができない、
すなわち近似することができないということになります。
そこで我々はこの理算関数をこのような連続関数に使ってフィッティングを行っています。
連続関数をフィッティングすることで、
この理算的な点以外の場所におけるパラメータを近似することができるようになります。
このような連続関数でこちらの理算的なものを連続関数で表現し直すということです。
これで連続関数のフィッティングにより、
2の連続パラメータのログフィルター、最初41900個と言いましたけど、
それ以上のログフィルターの畳み込んだ応答値を計算することができるようになるわけです。
ではもう少し細かく見ましょう。
ログフィルターに対して、実際にログフィルターに対して画像を畳み込む処理を考えます。
iが画像です。
vが固有フィルター。
そしてこのρが固有関数でした。
この計算は、まず固有関数から出てくる係数と固有フィルターを掛け算して、
そして画像に畳み込むという処理をします。
よくここの中身を見てみると、この式は順番を、ここは画像ですね。
なのでこの順番を入れ替えることができる。
iを後ろに持ってくることでどのようにできるかというと、
まず最初に固有フィルターと画像を畳み込んでから、後で係数を掛ければいいということになるわけです。
したがって、このあらかじめキーポイント画像が与えられたら、
このパラメータのσx、σy、σθの係数は置いておいて、
まず固有フィルターとパッチ画像の掛け算をして取っておき、
そしてこの2のσx、σy、σθを与えて、
この固有関数に与えて、その係数を掛けていけばいいという。
こうすることによって計算の効率化を行うことができるわけです。
さあでは実際に近似計算の流れを見てみましょう。
キーポイント画像が入力されました。
そしてそこにまず14個の固有フィルターを畳み込んでおきます。
そしてその後に固有関数にパラメータを入力して、
それに該当する係数を求め、
それを総合することで近似計算値が出ます。
この上は実際に連続的な非等方正ログフィルターを計算したときの真値で、
下が14個の固有フィルターを使って近似計算したときの結果です。
ほぼ値が一緒であるということがわかります。
では実際にこのσx、σy、σθのパラメータの値を入れ替えてみましょう。
このようにパラメータを変えたときは、
この係数、固有関数が出てくる係数だけが変わるだけなので、
こちらの計算はもう事前に終わっています。
したがって高速にこのように近似計算を行うことができるというわけです。
ではこの方法によって我々はσx、σy、σθという空間における
すべての応答値を効率的に計算することができた。
では今度この中からどこにそのピークがあるかということを調べます。
σx、σy、σθという3つがパラメータがあります。
このある回転した同じ同一σにおいて複数のアフィン領域が出るということは、
ちょっとこれはシフトにおける小さなスケールから探索して
そこで出たらストップするという考え方からも1つで十分だということがわかります。
したがって各シーターごとにまずピークの点をこうやって見つけ出します。
これを並べてみるとこのような形状になります。
この時に最大値となるところから80%以上となるピークのみを取り出します。
まず1つ目のピークのパラメータはσxが6.4、σyが12.8、σθが15というこのようなアフィン領域を表しています。
もう1つのピークはσxが6.4、σyが12.0、σθが156度というこのようなアフィン領域を示しているわけです。
したがってここでは2つのアフィン領域が検出されたということになるわけです。
では従来法とそのアフィン領域推定結果を見てみましょう。
従来法のヘシアンアフィン、そしてMSERという方法に対してアフィン領域の推定結果ですが、
このように複数の団員が含まれるような形状ですと、毎回初期値によって異なる領域が求められてしまっているというのがヘシアンアフィンでした。
一方MSERは軌道値の変動を見ますので、全体を含むような領域を取り出します。
なのでこれらのパターンに対しては同じ領域を求めることができていますが、
グラデーションがあるようなものですと色の軌道値の変化を見ているので、
このようにグラデーションがかかったパターンにおいては領域が変わってしまいます。
一方我々の提案手法はすべての4913種類の非統合性のログフィルターと同じように探索した結果を効率的に求めることができています。
さらに複数の団員が含まれるパターンにおいては、複数のアフィン領域がこのように求められていることができていることがわかります。
実際に画像にこのアフィン領域検出をしてみました。
このような色んなアフィン領域が計算され、視点Bの画像においても同じように色んなアフィン領域が推定されています。
少しわかりづらいので一箇所拡大しましょう。
この領域、2つの領域のアフィン領域が推定されています。
同じように同じ中心点に対して2種類のアフィン領域が推定できています。
当然見え方が変わっていますが、それに合わせたアフィン領域が推定できているというわけでは、
この我々の方法がどれくらい効果的であるかといったことをデータセットを使って評価しました。
まず、Oxford Matching Datasetの中にある遮影変化を含むGraftyというデータです。
横が1と2という画像間でのマッチング精度、次が1と3、1と4、1と5、1と6です。
当然1から6になればなるほど遮影変化がきつくなっていますから、
当然性能が落ちていくということがわかります。
その時にまず従来手法であるヘッシアンアフィンを見てみましょう。
ヘッシアンアフィン、ちなみにDOGはシフトの方法ですね。
まずシフトのDOGを見てみると、当然遮影変化が少ないときは通常のスケール推定で求まるので、
60%くらいできているんですが、遮影変化がきつくなっていくとどんどんできなくなってしまうというのがわかります。
同様にヘッシアンアフィンも最初はうまくいくんですけども、ある程度のところからうまくいかなくなってしまいます。
MSERはその点非常によくできるんですが、我々の提案手法はそのMSERを上回るような性能を獲得することができています。
続けて遮影変化だけではなく、回転スケール、ブラー、軌道変化、JPEG圧縮が生じた画像感におけるアフィン領域推定結果を見ましょう。
それぞれの結果をグラフで表します。赤色が提案手法です。
赤色のグラフが一番カーブが上にあります。
すなわち、我々の提案手法が回転スケール、ブラー、軌道変化、JPEG圧縮においても効果的であるということがこの実験結果がわかります。
では実際に今度はどれだけ計算効率が良くなっているか、実際に求める計算時間の比較をしてみました。
まず従来のシアンアフィンですと1枚の画像に対して大体4秒ぐらいの時間を必要とします。
オリジナルの非統合性のログフィルター4913種類を畳み込んで、すべて畳み込んでしらみつぶしに探索した場合は当然時間が198秒かかるというわけで、
我々はシアンアフィンより性能が良く、かつオリジナルログフィルターと同じ精度を保ったまま計算速度を2秒ということですから、
かなり早くすることができたというのがこれを見てわかると思います。
実際にこのシアンアフィンからは大体2倍ぐらい、さらにオリジナルのログからは87.2倍の高速化を実現したというわけです。
これがキーポイント検出におけるアフィン領域推定として、我々はマルチプルアフィンリージョンディテクターという方法を考えました。

考えて提案しました。では続けてキーポイントマッチングの特徴記述についても見ていきましょう。特徴記述においても年表でそれぞれのアプローチをまとめていました。
まず軌道ベースの回転スケール変化に対応するような特徴量というのは2010年頃からいろんな方法が出てきました。
勾配ベース、シフトでは勾配を計算して勾配方向筆グラムを出しました。最初に提案されたのが1999年でした。それからもいろんな方法が提案されています。
一方、アフィン領域、アフィン変形に対応した記述方法というのは少なく、これを視点合成ベースというふうに呼びますが、
Aシフト、ASRという方法が提案されているだけで、それ以外にはなかなか研究が取り組まれていない、すなわち非常に難しい問題であるということがわかります。
特にこの視点合成に基づく特徴量記述を効率的に求めるということは、研究が少ないということからも困難であるということがわかります。
ではまずシフトと従来法であるAシフトという方法がどれくらい違うかということを見てみましょう。
この2画像間においてシフトの特徴量を使ってマッチングしたのですが、マッチングが一つもできていない、対応点が取れていないということがわかります。
一方、Aシフトはこれぐらい遮蔽変化が起こっても、これだけの対応点を求めることができていることがわかります。
いかにAシフトがシフトよりも、当然ながら遮蔽変化に対応した方法ですから、より良くできているということがわかります。
ではこのAシフト、どうやって実現しているか想像してみましょう。
このAシフトは画像の視点合成に基づく特徴量記述を行います。
実は検出したパッチ画像に対して、いろんなパラメータ、フィンパラメータでそれぞれ変換して、
いろんなパラメータに対応する視点合成をした画像を作ります。
そしてそれぞれから、この1個1個からシフトの特徴量を記述して、これらをマッチングするということをしているわけです。
よく考えれば、素直なアプローチではあるんですが、計算量が大幅に多いということが想像できると思います。
そうですよね。一つのパッチ画像に対して、数百種類というフィン変換をして、それぞれで特徴量、128次元の特徴量を出して、
それをすべてしらみつぶしに距離計算をして対応点を求めるというわけです。
これを式で表現すると、どういったことかというと、
i というパッチ画像が入力されたら、あるアフィン変換パラメータ p で変換したものから特徴量記述をしたものということになります。
まずここの視点合成は、i という画像が与えられたら p というパラメータで変換をしました。
その変換した画像に対して特徴記述を行い、そして特徴量を使ってマッチングするというわけです。
問題はここですね。この一つの一枚のパッチ画像に対して、すべてのアフィンパラメータで視点合成をしないといけないわけです。
これを入力画像を与えられて、オンラインでアフィン変換をしようとすると、明らかに計算コストが高いということがわかります。
そこで我々はどういったことを考えたかというと、画像をアフィン変換するのではなくて、特徴量を記述するためにフィルターを設計し、
そのフィルターを事前にアフィン変換しておけばいいんじゃないかということを考えたわけです。
このように画像をアフィン変換するのではなくて、下にあるようにフィルターをアフィン変換してから画像を畳み込みましょうというわけです。
そこで従来の特徴記述書を記述する方法に ORB という方法があります。
ORB を見てみると、1枚のパッチ画像の赤の点から青の点の差分を計算し、その差分の値を特徴量にするという方法です。
これが1次元目。2次元目は異なるペアにおける差分を計算します。
これが例えば 256 個あることで 256 次元の特徴量になるというわけです。
この差分計算をフィルターで表すことを考えてみると、この領域にはプラスの値、この領域にはマイナスの値、それ以外はゼロという値を持つフィルターをこの画像に畳み込むことは、この計算をすることと同じであることがわかります。
我々はこのような特徴記述を、こういうフィルターをそれぞれ設計し、このフィルターの畳み込み処理によって特徴記述をするということにしたわけです。
そしてこの特徴記述するためのフィルター、これを視点合成、すなわちアフィン・パラメータによって変形させましょうというわけです。
この時に実際にこのパターンをいろんな視点から見ます。
ここではこのtというパラメータとphiというパラメータでいろんな視点からのこのフィルターを作るということをするわけです。
実際にどうすればいいかというと、この特徴記述をするためのフィルターをまずあらかじめアフィン・パラメータで合成をします。
それぞれ合成をします。そしてこれを使って多視点の特徴量をAシフトと同様に、だけどAシフトよりも効率的に算出しようというわけです。
しかしこのアフィン変換済みの特徴記述フィルターというものはこれも大量にあります。
大量の特徴記述フィルターの計算が必要になるわけなんですが、これは先ほど実際にこの記述フィルターは256種類あって、
それを576視点になりますので、合計でなんと14万7456枚の特徴記述フィルターになります。
1枚のパッチ画像に14万7456回のフィルター処理をしないといけないということになります。大変です。
そこでこれも前のアフィン領域の検出と同じように特一分解を使って、この大きな特徴記述フィルター群をより少ない固有フィルターで禁止しようというわけです。
この後アプローチは全く前回と一緒です。
まずダブルという行列、すなわちアフィン変換した特徴記述フィルターを並べスタックした行列を求め、これを実際は縦の方向に対しては14万7456、
こちらの横軸は一つのフィルターのサイズですから、60×65の4225というふうになります。
そしてこのダブルという行列を特一分解により、UとSとVの点値に分解し、同じように特一を見ています。
ここでは上位250個の特一のみを使います。
そうして求めると、USという行列とVの点値という行列になり、こちらは固有フィルター、そしてUSは固有関数を表すことになります。
この固有関数は前回と同様に連続関数でフィッティングをしておきます。
そしてこのように行列計算をしていけば、各1次元目の特徴量、2次元目の特徴量という形で、各次元の特徴量を行列計算によって求めることができるようになります。
さらにマッチングをする際に、視点Aの特徴点と視点Bの特徴点をマッチングする際は、
本来はこのAの赤の点に対して、緑のいろんなアフィン・パラメータを変えたときの特徴量と一番近いものを探すという方法。
これは一体、片一方はアフィン・パラメータで変えたので、他のユークリット距離を算出します。
さらに叩いたという形で、こちらのキーポイントに対しても各アフィン・パラメータの特徴量、
そしてもう一方の画像における各キーポイント点に対しても、それぞれアフィン・パラメータに対する特徴量を記述して、叩いたでこの中で一番近くなるところを探すということもできます。
この場合、我々の方法は連続関数でフィッティングしていますから、パラメータを細かくこのようにすることができるわけです。
そうすると、より良いところでのマッチングができるようになります。
さらには、この特徴給付空間を遮蔽して部分空間に遮蔽することで、ここの全体同士での距離計算もすることになるわけです。
こうすることによって、より高精度化することができます。
実際に実験結果です。青色がAシフトです。
Aシフトに対して、我々の赤色、オレンジ色と赤色が我々の提案手法になりますけれども、
この従来手法を上回る精度を達成することができていることがわかります。
実際にキーポイントマッチング例ですが、このように非常に難しいパターンにおいても、
提案手法は従来法と比べ、青色がご対応ですから、ご対応が少なくマッチング率精度が高いことがわかります。
これは別の例です。
実際にこちらも計算時間を比較してみましょう。
Aシフトは特徴量を記述するために、画像をいっぱいアフィン変換しないといけません。
なので、計算時間がかなりかかっているということがわかります。
このAシフトの計算時間を100%としたときに、提案手法がどれくらいか見てみると、
この結果を見てわかるように、Aシフトと比べて約6.6倍高速化を実現することができました。
この方法は、この特徴点、キーポイント、画像間のマッチングだけではなく、
ロボットの物体を掴むというところにも展開することができます。
例えば、ロボットがこのような物体を挟んで掴むときにはどうすればいいかというと、
ハンドの形状であるこういったテンプレートと画像の畳み込みをします。
ただし、細かくハンドのパラメータを回転角、開き幅を変えると、
当然ハンドテンプレートが324枚という大量の画像になってしまいますから、
早くどこを掴めばいいかということはわかりません。
そこで同様に特位値分解をして、ハンドテンプレートを効率よく表現します。
こちらは従来法の計算。
一方、その特位値分解を使った我々の提案手法では、
より早く舵位置がこちらよりも検出することができているということがわかると思います。
このように画像処理のテクニックを提案したんですけども、
こういったロボットへの適用もできるアプローチになっています。
提案手法はもう終了しています。
いまだに従来法は探索をしているということがわかると思います。
今日はですね、アフィン不変なキーポイントマッチング、
非常にシフトでは解決できていない、難しいとされてきた問題に対して、
我々が研究として取り組んだアプローチを紹介しました。
キーポイントにおける複数のアフィン領域を推定する方法と、
視点合成による特徴量給付の効率的な算数方法というものを、
両者ともに特位値分解という方法をうまく使って、
法制度化とかつ法律化を同時に実現したアプローチになっています。
最後に、僕の師匠であるカーネギメロン大学の金谷先生の言葉を紹介したいと思います。
僕はカーネギメロン大学に2回行っています。
これは2回目に2005年から2006年の1年間行ってた時にですね、
最後帰る時に、また日本に戻ってくる時に、
金谷先生からいただいた言葉を紹介します。
金谷先生からいただいた言葉としては、
尺眼対極、着手消極、大きな視点を持って捉えながら、
一つ一つ小さなことから手をつけてやっていきましょうということだと思います。
そしてもう一つが、素人発想、苦労と実行です。
これ言葉で言うと非常に絶やすいんですが、
実は研究者になると、どんどんどんどんいろんなことを知っていますので、
なかなかこういった柔軟な素人的な発想ができません。
なので、素人的な発想で、だけど素人の実行ではなく苦労との実行しましょうというのが、
より良い研究になるというわけです。
そういうアドバイスをいただきました。
非常に良い言葉なんですけれども、
この言葉に合うような研究をするということは、
並大抵できるもの、その簡単にはできるものではありません。
これ実際いただいたのが2006年です。
2006年からこの言葉をいただいて、
いつかこういう研究をしたいと思って頑張ってやってきたわけなんですけれども、
ようやく今回の先ほど紹介した研究、
10年ぐらい経ってでしょうか、
できるようになってきたかなというふうにちょっと思えるようになったというわけです。
先ほどの着眼対極、着手消極という観点では、
従来のヘシアンアフィンはこのような領域に対して、
局所領域だけを見てフィッティングをしてしまいますので、
良いところを探せずに終わってしまう。
すなわち、スインクローカリー、アクトローカリー、
局所領域だけを見ていろいろ処理をしちゃっているので問題があったんです。
一方、我々はスインクグローバリー、
全アフィンパラメータ、アフィン領域における応答値を計算して、
そしてこういったピークを取り出すので、
ちゃんと最適な解を求めて、
全体を見て最適な解をちゃんと求めることができるようになります。
そしてこの素人発想、クロート実装においては、
もし非統合性のログフィルター4913枚を全て畳み込むということを考えるということは、
これすごく素人発想ですよね。素直な考え方だと思います。
だけどこれを実際に4913枚を全て畳み込んでしまったら、
これクロート実装ではなくて素人実装になってしまいます。
そこで我々はクロート実装ということで、
特異値分解という手法をうまく適用して、
効率よくクロート的な実装をした。
これによって精度向上と高速化という効率化を同時に獲得したという意味で、
ようやく先ほど金田先生からいただいた言葉に対応するような研究ができたかなと思っているところです。
最後の今日の話においては研究の話です。
いろいろな研究が提案されていて、
研究というものはいきなりポンとできるものではなくて、
いろいろな従来の研究があって、
その中から切磋琢磨するような形で新しい方法を考えて提案するということです。
その時に先ほどもあったように、
素人発想、クロート実装というような考え方で研究ができると、
より良い研究になるというのが、
僕の金田先生がいただいた言葉であり、
僕の研究モットーでもある。
今日は、アフィン不変に対応するキーポイントマッチングとして、
我々の研究グループの研究について紹介しました。
我々の研究グループ、機械知覚ロボティクス研究グループは、
私と情報工学部の山下先生、
そして特任授業の平川先生の3名の先生と学生たちで、
研究グループという形でこういった研究、
最近では特に人工知能といわれるディープラーニングの研究に取り組んでいます。
今日の内容はちょっと難しかったかもしれませんが、
どういう観点で、どういうふうに研究をしているかということを知っていただければ嬉しいです。
では、今日はこれで終わりです。
それではまた。

