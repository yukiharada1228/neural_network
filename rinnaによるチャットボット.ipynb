{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNec20sbb8W7HJfUR3oma0j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukiharada1228/neural_network/blob/main/rinna%E3%81%AB%E3%82%88%E3%82%8B%E3%83%81%E3%83%A3%E3%83%83%E3%83%88%E3%83%9C%E3%83%83%E3%83%88.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mMHL0Cj3eBn",
        "outputId": "5bb4cb1c-92cb-42c3-a5fa-7126ed0e7c3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# 必要なライブラリをインストール\n",
        "!pip install -q transformers accelerate sentencepiece bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# rinnaチャットボットを作成\n",
        "class RinnaChatBot:\n",
        "    def __init__(self, max_length=128, k=40):\n",
        "        model_id = \"rinna/japanese-gpt-neox-3.6b-instruction-ppo\"\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16\n",
        "        )\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        self.log = []\n",
        "        self.max_length = max_length\n",
        "        self.k = k\n",
        "\n",
        "    def chat(self):\n",
        "        while \"[exit]\" not in (user_message := self._input()):\n",
        "            self.add_log(\"ユーザー\", user_message)\n",
        "\n",
        "            prompt = (\n",
        "                self.make_prompt()\n",
        "                + \"<NL>\"\n",
        "                + \"システム: \"\n",
        "            )\n",
        "\n",
        "            token_ids = self.tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "            output = \"\"\n",
        "            for word in self.gradually_generate(token_ids):\n",
        "                print(word, end='', flush=True)\n",
        "                output += word\n",
        "            print()\n",
        "\n",
        "            self.add_log(\"システム\", output)\n",
        "\n",
        "    def _input(self):\n",
        "        s = input(\"> \").replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n",
        "        return s\n",
        "\n",
        "    def make_prompt(self):\n",
        "        prompt = [\n",
        "            f\"{uttr['speaker']}: {uttr['text']}\"\n",
        "            for uttr in self.log\n",
        "        ]\n",
        "        prompt = \"<NL>\".join(prompt)\n",
        "        return prompt\n",
        "\n",
        "    def add_log(self, role, text):\n",
        "        self.log.append({\n",
        "            \"speaker\": role,\n",
        "            \"text\": text\n",
        "        })\n",
        "\n",
        "    def gradually_generate(self, token_ids):\n",
        "        token_ids = token_ids.to(self.model.device)\n",
        "        for _ in range(self.max_length):\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(token_ids)\n",
        "\n",
        "            logits = outputs.logits\n",
        "            indices_to_remove = logits < torch.topk(logits, self.k)[0][..., -1, None]\n",
        "            logits[indices_to_remove] = float('-inf')\n",
        "            probs = torch.nn.functional.softmax(logits[..., -1, :], dim=-1)\n",
        "            next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "            token_ids = torch.cat((token_ids, next_token_id), dim=-1)\n",
        "\n",
        "            output_str = self.tokenizer.decode(next_token_id[0])\n",
        "\n",
        "            yield output_str.replace(\"<NL>\", \"\\n\")\n",
        "\n",
        "            if \"</s>\" in output_str:\n",
        "                break"
      ],
      "metadata": {
        "id": "Ws_GzAvq3iYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# チャットを開始します\n",
        "bot = RinnaChatBot()\n",
        "bot.chat()"
      ],
      "metadata": {
        "id": "s1KllsDE3n8s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b532946b-5c67-4fe1-eaf9-850ea8e38eb7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> 今日も暑いですね\n",
            "はい、本当に夏です。本当に暑いです。そして、毎日、日ごとに気温が上昇しています。本当に大変な暑さです。体調管理に気をつけて、熱中症にならないように、水分を十分に補給することが大切です。また、運動や外出の際には、日焼け対策をしっかり行う必要があります。暑くて日差しが強いため、サングラスや帽子、日焼け止めなどの紫外線対策も、健康と美容のために重要です。さらに、熱中症対策や熱中症の進行を防止するために、定期的に水分補給をしたり、涼しい服装をしたりすること\n",
            "> 熱中症には気をつけないといけないですね\n",
            "はい、熱中症は大変な病気で、重症になると命にもかかわることがあります。また、予防するために、定期的に運動や外出をしたり、水分補給をしたりすることが大切です。さらに、外出時には、日差し対策をしっかりと行い、熱中症を防止することが大切です。</s>\n",
            "> 水分補給のほかに気をつけたほうがいいことはありますか\n",
            "はい、カリウムを含む食品を摂取することが重要です。カリウムは、野菜、ナッツ、全粒穀物、豆類、ヨーグルトに豊富に含まれています。カリウムが多く、食感がよい食品には、レタス、ほうれん草、芽キャベツ、芽キャベツ、にんにく、トマトがあります。バナナや柑橘類にもカリウムが豊富に含まれています。カリウムは、筋肉を収縮させる役割も果たし、運動や減量に役立つことがあります。</s>\n",
            "> [exit]\n"
          ]
        }
      ]
    }
  ]
}