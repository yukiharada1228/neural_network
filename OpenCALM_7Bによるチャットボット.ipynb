{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNch9cks8LocNoax7u36dT3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukiharada1228/neural_network/blob/main/OpenCALM_7B%E3%81%AB%E3%82%88%E3%82%8B%E3%83%81%E3%83%A3%E3%83%83%E3%83%88%E3%83%9C%E3%83%83%E3%83%88.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUの確認\n",
        "import torch\n",
        "\n",
        "is_cuda = torch.cuda.is_available()\n",
        "device_name = \"\"\n",
        "if is_cuda:\n",
        "    device_name = torch.cuda.get_device_name()\n",
        "print({\"is_cuda\": is_cuda, \"device_name\": device_name})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNpy9PEsTh5R",
        "outputId": "d7f7b751-95a8-43b7-e267-0eda0de04809"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'is_cuda': True, 'device_name': 'Tesla V100-SXM2-16GB'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-mMHL0Cj3eBn"
      },
      "outputs": [],
      "source": [
        "# ライブラリをインストール\n",
        "%%capture\n",
        "%pip install transformers peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# チャットボットを作成\n",
        "class ChatBot:\n",
        "    def __init__(self, max_length=128, k=40):\n",
        "        hf_peft_repo = \"yukiharada1228/open-calm-7b-instruct-lora-epoch1\"\n",
        "        peft_config = PeftConfig.from_pretrained(hf_peft_repo)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            peft_config.base_model_name_or_path,\n",
        "            return_dict=True,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map='auto'\n",
        "            )\n",
        "        self.model = PeftModel.from_pretrained(model, hf_peft_repo)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)\n",
        "        self.max_length = max_length\n",
        "        self.k = k\n",
        "\n",
        "    def chat(self):\n",
        "        while \"[exit]\" not in (user_message := self._input()):\n",
        "\n",
        "            prompt = self.generate_prompt(user_message)\n",
        "\n",
        "            token_ids = self.tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "            output = \"\"\n",
        "            for word in self.gradually_generate(token_ids):\n",
        "                print(word, end='', flush=True)\n",
        "                output += word\n",
        "            print()\n",
        "\n",
        "    def _input(self):\n",
        "        s = input(\"> \").replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n",
        "        return s\n",
        "\n",
        "    def generate_prompt(self, instruction, input=None, response=None):\n",
        "      def add_escape(text):\n",
        "        return text.replace('### Response', '###  Response')\n",
        "\n",
        "      if input:\n",
        "        prompt = f\"\"\"\n",
        "    Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "    ### Instruction:\n",
        "    {add_escape(instruction.strip())}\n",
        "\n",
        "    ### Input:\n",
        "    {add_escape(input.strip())}\n",
        "    \"\"\".strip()\n",
        "      else:\n",
        "        prompt = f\"\"\"\n",
        "    Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "    ### Instruction:\n",
        "    {add_escape(instruction.strip())}\n",
        "    \"\"\".strip()\n",
        "\n",
        "      if response:\n",
        "        prompt += f\"\\n\\n### Response:\\n{add_escape(response.strip())}<|endoftext|>\"\n",
        "      else:\n",
        "        prompt += f\"\\n\\n### Response:\\n\"\n",
        "\n",
        "      return prompt\n",
        "\n",
        "    def gradually_generate(self, token_ids):\n",
        "        token_ids = token_ids.to(self.model.device)\n",
        "        for _ in range(self.max_length):\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(token_ids)\n",
        "\n",
        "            logits = outputs.logits\n",
        "            indices_to_remove = logits < torch.topk(logits, self.k)[0][..., -1, None]\n",
        "            logits[indices_to_remove] = float('-inf')\n",
        "            probs = torch.nn.functional.softmax(logits[..., -1, :], dim=-1)\n",
        "            next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "            token_ids = torch.cat((token_ids, next_token_id), dim=-1)\n",
        "\n",
        "            output_str = self.tokenizer.decode(next_token_id[0])\n",
        "\n",
        "            yield output_str\n",
        "\n",
        "            if \"<|endoftext|>\" in output_str:\n",
        "                break"
      ],
      "metadata": {
        "id": "Ws_GzAvq3iYe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# チャットを開始します\n",
        "bot = ChatBot()"
      ],
      "metadata": {
        "id": "s1KllsDE3n8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bot.chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2XXPMnQVEF8",
        "outputId": "5bde29e8-d548-487f-b125-ad4096926607"
      },
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> 機械学習と人工知能の違いについて教えてください\n",
            "機械学習は、多くのコンピュータシミュレーションを行いながら特定のタスクを実行するのに役立つコンピュータシステムの能力を利用した研究技術であり、人工知能はそれとはかなり異なります。人工知能の主な研究目標は、人間から自律的に学習できるシステムを開発することです。したがって、人工知能は学習プロセスによって自律的に行動できるように設計されています。機械学習は、タスクの種類に依存し、多くのコンピュータ科学および機械学習研究プロジェクトを通して発展していきます。人工知能は、自律的でより倫理的であることが期待されているため、人間や動物のように振る舞い、より優れた認知を持つことができます。機械学習は、より高度な思考を持つ機械学習システムを開発することが目標であり、多くの研究が行われています。人工知能は、自律した知能を備えた自律システム\n",
            "> [exit]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ajuASnC5W6Bd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}